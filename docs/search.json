[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Bio Data Science",
    "section": "",
    "text": "A tidy approach to wrangling, exploring, visualising and communicating bio data with an emphasis on doing collaborative and reproducible bioinformatics projects \nLeon Eyrich Jessen & the TA team\n\nWelcome to R for Bio Data Science\nSo, you signed up for the 22100/22160 bioinformatics study line course - Congratulations! That was your first step towards getting a set of bio data science skills, which will serve you throughout your future career regardless of your path!\nInspirational quotes can be cliche, however this one hits the nail on the head:\n\n“Think about the readability of your code. Every project your work on is fundamentally collaborative. Even if you are not working with any other person, you are always working with future you and you really do not want to be in a situation where future you has no idea what past you was thinking, because past you will not respond to any emails!” Hadley Wickham\n\nBio Data Science in intrinsically collaborative (even if it’s just you working) and intrinsically interdisciplinary, so collaborative-, reproducibility- and communication- skills are key. In this course, you will learn how to generate value by activating your knowledge on doing modern project oriented collaborative bio data science in tidyverse R - Welcome!",
    "crumbs": [
      "Welcome to R for Bio Data Science"
    ]
  },
  {
    "objectID": "prologue.html",
    "href": "prologue.html",
    "title": "Prologue",
    "section": "",
    "text": "This course was designed to accommodate the need for a modern bio data science in R course as part of the Master of Science (MSc) in Bioinformatics. The pilot course was executed in 2019 and in 2020 the first fully fledged course was launched with ~35 students, since then the course has grown to ~150 students.\nThe course is designed as a semi-flipped classroom, with an emphasis on active learning. This means that during the 4h classes, the first hour will be dedicated to reviewing key points from last week and then a brief introduction to the topic of the day followed by a break. Hereafter, the students will work hands-on in groups on computational exercises using cloud computing infrastructure. The exercises will rely on relevant bioinformatics data from publicly available databases and gradually build the students toolbox with an emphasis on collaborative project work. This part will take up the first 9 labs. Each lab is defined by a set of specified learning objectives (LOs), it is essential that students continuously make sure, that they are on track with these LOs.\nThe fist hour of the 10th lab is dedicated to introducing the project part of the course and the subsequent 3h are dedicated to a mini symposium on “Application of R for Bio Data Science in Industry”. Here students will get a change to get insights into how the course topics are implemented in industry and get a glimpse into what career options are available upon completing their education. This hybrid event typically attracts ~250 participants and have featured talks from major national and international Pharma/biotech companies, such as: Novo Nordisk, Lundbeck, Chr. Hansen, Bristol Meyer Squibb, a.o.\nIn the project part of the course, students will form groups of 4-5 students based on common interests. Hereafter, the students will seek out and select a data set, which will form the foundation for the project work. In the project work, the students will go through the entire data science cycle and produce the code base for a complete bioinformatics project. This entails a complete synthesis of all components of the exercise labs and supports the collaborative aspect. The groups must then condense the project into a presentation, thereby addressing communicative competencies as an essential part of being a modern bio data scientist."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "This section contain essential information for getting up and running for the classes"
  },
  {
    "objectID": "getting_started.html#when-and-where",
    "href": "getting_started.html#when-and-where",
    "title": "Getting Started",
    "section": "When and Where",
    "text": "When and Where\nTeaching sessions will be E3A, Tuesday mornings 8 - 12 in building 358, room 060a (exercises: Also room 045) and the general schedule will be:\n\n08.00 - 08.30 Recap of key points from last weeks exercises\n08.30 - 09.00 Introduction to theme of the day\n09.00 - 12.00 Exercises\n\nPlease note, at DTU we do not use “the academic quarter”, this means that 8 am, means that class will commence at 8 am"
  },
  {
    "objectID": "getting_started.html#class-rules-expectations",
    "href": "getting_started.html#class-rules-expectations",
    "title": "Getting Started",
    "section": "Class Rules & Expectations",
    "text": "Class Rules & Expectations\n\nWhat I Expect From You (the Students):\n\nActive Participation\n\nCome prepared to engage with both coding and biological data science concepts.\nAsk questions, curiosity drives learning.\n\nEffort Over Perfection\n\nMistakes are part of programming and science. Document them, learn from them, and keep iterating.\nShow your reasoning process, not just the final result.\n\nCollaboration & Respect\n\nSupport each other in learning R, teaching your peers is one of the best ways to learn yourself.\nRespect diverse perspectives and backgrounds.\n\nProfessional Conduct\n\nMeet deadlines unless unforeseen circumstances arise (in which case, communicate early).\nUphold academic integrity: your code should be your own work unless group collaboration or AI use is explicitly stated.\n\n\n\n\nWhat You Can Expect From Me (the Course Responsible):\n\nClarity & Structure\n\nI will explain concepts step by step and connect theory to practice.\nI will provide clear instructions and transparent guidelines and expectations.\n\nAccessibility & Support\n\nI will make myself available during class and on Slack for questions.\nI will provide constructive feedback to help you grow.\n\nA Safe Learning Environment\n\nNo question is “too simple”\nI will foster a respectful, inclusive space for all students.\n\nPractical Relevance\n\nI will use real-world biological datasets where possible.\nI will highlight how R skills connect to research and professional opportunities."
  },
  {
    "objectID": "getting_started.html#logging-onto-cloud-server",
    "href": "getting_started.html#logging-onto-cloud-server",
    "title": "Getting Started",
    "section": "Logging onto Cloud Server",
    "text": "Logging onto Cloud Server\nFirst, make sure you have a working DTU account, either as a student id or employee initials (e.g. PhD-students or postdocs) and that your multi factor authentication is functional. In this course we will be using a cloud server infrastructure to perform our work.\nPlease click here to test your access the cloud server: R for Bio Data Science Cloud Server"
  },
  {
    "objectID": "getting_started.html#sec-slack",
    "href": "getting_started.html#sec-slack",
    "title": "Getting Started",
    "section": "Slack as Class Communication",
    "text": "Slack as Class Communication\nTo streamline class communication, we will be using Slack. The aim is to facilitate getting you help fast and efficiently from classmates, the TAs, and myself. Rather than emailing questions to the teaching staff, I encourage you to post your questions on Slack. Each teaching lab will have its own dedicated Slack channel. Note: You can also make group specific Slack channels!\nPlease join the Slack workspace by clicking here"
  },
  {
    "objectID": "getting_started.html#sec-github",
    "href": "getting_started.html#sec-github",
    "title": "Getting Started",
    "section": "Setting up a GitHub account",
    "text": "Setting up a GitHub account\nPrior to class, please go to GitHub and setup and account, shouldn’t take long. During the registration process you can set up a student account, which will give you additional benefits, including GitHub Pro. In order to get it, you need to use your DTU email when setting account and select Apply for your GitHub student benefits when asked during the registration process. You’ll be then asked to apply for GitHub Student Developer Pack, which will require uploading your student id photo. The process of confirming a student account may take up to a few days (however, it can be almost instantaneous). In the meantime you can already use your free account. Free account should be sufficient for this class, so if you don’t want to set up a school account, you don’t need to.\nPlease state your GitHub username in this google sheet.\nWe will then invite you to the Github organisation for this year’s course. This is where you will learn how to do collaborative data science."
  },
  {
    "objectID": "getting_started.html#sec-groups",
    "href": "getting_started.html#sec-groups",
    "title": "Getting Started",
    "section": "Group Formation",
    "text": "Group Formation\nThe backbone of this course is modern collaborative data science and as such group work and active participation herein is mandatory. Furthermore, It is important that course participants prioritise to be present during classes as the course design is based on student-student interaction in an active learning environment. Therefore, students will work in groups of 4-5 students.\nPlease click here and fill in the group formation sheet"
  },
  {
    "objectID": "getting_started.html#sec-questionnaire",
    "href": "getting_started.html#sec-questionnaire",
    "title": "Getting Started",
    "section": "Pre Course Questionnaire",
    "text": "Pre Course Questionnaire\nIt is important for the teaching team to understand the class composition.\nPlease click here and fill in this brief and 100% anonymous questionnaire"
  },
  {
    "objectID": "getting_started.html#sec-midwayeval",
    "href": "getting_started.html#sec-midwayeval",
    "title": "Getting Started",
    "section": "Course Midway Evaluation",
    "text": "Course Midway Evaluation\nIMPORTANT: This is ONLY for teaching lab 6\nAn integral part of the course is for the teaching team to align efforts with the status of the student body.\nSo, please take 2 minutes and fill in this brief questionnaire: Course Midway Evaluation\nThank you 🙏"
  },
  {
    "objectID": "lab00.html",
    "href": "lab00.html",
    "title": "Course Labs",
    "section": "",
    "text": "This chapter provides a complete overview of the course curriculum"
  },
  {
    "objectID": "lab01.html",
    "href": "lab01.html",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "",
    "text": "base"
  },
  {
    "objectID": "lab01.html#schedule",
    "href": "lab01.html#schedule",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Arrival, fill in the pre-course anonymous questionaire and interest based group formation\n08.15 - 08.45: Lecture: Course Introduction\n08.45 - 09.00: Break\n09.00 - 11.15: Exercises\n11.15 - 11.30: Break\n11.30 - 12.00: Lecture: Reproducibility in Modern Bio Data Science"
  },
  {
    "objectID": "lab01.html#sec-learning-materials",
    "href": "lab01.html#sec-learning-materials",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nRead the full course description here: 22100 BSc / 22160 MSc\nIf you have not already done so, please take 2 min. to answer the pre-course anonymous questionaire\nRead course site sections: Welcome to R for Bio Data Science, Prologue and lastly Getting Started, where it is important that you perform any small tasks mentioned\nBook: R4DS2e: Welcome\nBook: R4DS2e: Preface to the second edition\nBook: R4DS2e: Introduction\nBook: R4DS2e: Chapter 2 Workflow: basics\nBook: R4DS2e: Chapter 28 Quarto (Don’t do the exercises)\nVideo: RStudio for the Total Beginner\nPaper: A Quick Guide to Organizing Computational Biology Projects\n\nUnless explicitly stated, do not do the per-chapter exercises in the R4DS2e book"
  },
  {
    "objectID": "lab01.html#learning-objectives",
    "href": "lab01.html#learning-objectives",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nMaster the very basics of R\nNavigate the RStudio IDE\nCreate, edit and run a basic Quarto document\nExplain why reproducible data analysis is important, as well as identify relevant challenges and explain replicability versus reproducibility\nDescribe the components of a reproducible data analysis"
  },
  {
    "objectID": "lab01.html#sec-exercises",
    "href": "lab01.html#sec-exercises",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Exercises",
    "text": "Exercises\nToday, we will focus on getting you started and up and running with the first elements of the course, namely the RStudio IDE (Integrated Developer Environment) and Quarto. If the relationship between R and RStudio is unclear, think of it this way: Consider a car, in that case, R would be the engine and RStudio would be the rest of the car. Neither is particularly useful, but together they form a functioning unit. Before you continue, make sure you in fact did watch the “RStudio for the Total Beginner” video (See the Learning Materials for today’s session).\n\nFirst steps\n\nUp and Running with the Course Cloud Server\nIn the menu on your left, you will find, amongst other things, some guides. Find the Guide for Cloud server and the RStudio IDE, click it and follow the guide.\n\n\nThe Console\nNow, in the console, as you saw in the video, you can type commands like:\n\n2+2\n1:100\n3*3\nsample(1:9)\nX <- matrix(sample(1:9), nrow = 3, ncol = 3)\nX\nsum(X)\nmean(X)\n?sum\nsum\nnucleotides <- c(\"a\", \"c\", \"g\", \"t\")\nnucleotides\nsample(nucleotides, size = 100, replace = TRUE)\ntable(sample(nucleotides, size = 100, replace = TRUE))\npaste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\")\nreplicate(n = 10, expr = paste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\"))\ndf <- data.frame(id = paste0(\"seq\", 1:10), seq = replicate(n = 10, expr = paste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\")))\ndf\nstr(df)\nls()\n\nTake some time and play around with these commands and other things you can come up with. Use the ?function to get help on what that function does. Be sure to discuss what you observe in the console. Do not worry too much about the details for now; we are just getting started. But as you hopefully can see, R is very flexible and basically the message is: “If you can think it, you can build it in R”.\n\nGo to R4DS2e Chapter 2 Workflow: basics in R4DS2e and do the exercises\n\n\n\nThe Terminal\nNotice how in the console pane, you also get a Terminal, click and enter:\n\nls\nmkdir tmp\ntouch tmp/test.txt\nls tmp\nrm tmp/test.txt\nrmdir tmp\nls\necho $SHELL\n\nBasically, here you have access to a full terminal, which can prove immensely useful! Note, you may or may not be familiar with the concept of a terminal. Simply think of it as a way to interact with the computer using text command, rather than clicking on icons etc. Click back to the console.\n\n\nThe Source\nThe source is where you will write scripts. A script is a series of commands to be executed sequentially, i.e. first line 1, then line 2 and so on. Right now, you should have a open script called Untitled1. If not, you can create a new script by clicking white paper with a round green plus sign in the upper left corner.\nTaking inspiration from the examples above, try to write a series of commands and include a print()-statement at the very end. Click File \\(\\rightarrow\\) Save and save the file as e.g. my_first_script.R. Now, go to the console and type in the command source(\"my_first_script.R\"). Congratulations! You have now written your very first reproducible R program!\n\n\n\nThe Whole Shebang\nEnough playing around, let us embark on our modern Bio Data Science in R journey.\n\nIn the Files pane, click New Folder and create a folder called projects\nIn the upper right corner, click where it says Project: (None) and then click New Project...\nClick New Directory and then New Project\nIn the Directory name:, enter e.g. r_for_bio_data_science\nClick the Browse... button and select your newly created projects directory and then click Choose\nClick Create Project and wait a bit for it to get created\n\n\nOn Working in Projects\nProjects allow you to create fully transferable bio data science projects, meaning that the root of the project will be where the .Rproj file is located. You can confirm this by entering getwd() in the console. This means that under no circumstances should you ever not work within a project, nor should you ever use absolute paths. Every single path you state in your project must be relative to the project root.\nBut why? Imagine you have created a project, where you have indeed used absolute paths. Now you want to share that project with a colleague. Said colleague gets your project and tests the reproducibility by running the project end-to-end. But it completely fails because you have hardcoded your paths to be absolute, meaning that all file and project resource locations point to locations on your laptop.\nProjects are a must and allow you to create reproducible encapsulated bio data science projects. Note, the concept of reproducibility is absolutely central to this course and must be considered in all aspects of the life cycle of a project!\nIf projects and paths seem unfamiliar to you, you may want to look in the menu on the left and find the Primers and then find and click Paths & Projects, which will guide you through the details.\n\n\nQuarto\nWhile .R-scripts are a perfectly valid way to write scripts, there is another Skywalker:\n\nIn the upper left corner, again, click the white paper with the round green plus, but this time select Quarto Document\nEnter a Title:, e.g. “Lab 1 Exercises” and enter your name below in the box Author:\nClick Create\nImportant: Save your Quarto document! Click File \\(\\rightarrow\\) Save and name it e.g. lab_01_exercises.qmd\nMinimise the Environment pane\n\nYou should now see something like this:\n\n\n\n\n\nTry clicking the Render button just above the quarto-document. This will create the HTML5 output file. Note! You may get a Connection Refused message. If so, don’t worry, just close the page to return to the cloud server and find the generated .html-file, left-click and select View in Web Browser.\nYou may also get a bit of a weird message, which could look something like this /file_show?path=%2Fnet%2Fpupil1%2Fhome%2Fpeople%2Fztizh%2Ftry.html not found. In that case, click the generated .html-file and choose Open in Editor and then at the top of document, find the button saying Preview and click it. Note, this is a work-around.\nIf you have previously worked with Rmarkdown, then many features of Quarto will be familiar. Think of Quarto as a complete rethinking of Rmarkdown, i.e. based on all the experience gained, what would the optimal way of constructing an open-source scientific and technical publishing system?\nIf you have previously encountered Jupyter notebooks, Quarto is similar. The basic idea is to have one document covering the entire project cycle.\nProceed to R4DS2e Chapter 28 Quarto and do the exercises.\n\n\n\nArtificial Intelligence in modern Bio Data Science\nThis section is new as of 2025, please read carefully and follow along…\nHonestly, if AI is going to solve all coding and analysis tasks moving forth, then why even bother??? 🤷\nThis technology has made its entrance and is here to stay, but in order to really increase your learning yield, it is very important to understand how to use this technology.\nYou can compare these technologies with being in the same group as the absolute smartest students in class, in fact the smartest student who has ever taken this class.\nIt is really important to distinguish the following two scenarios:\n\nGetting the smart student to solve your programming/analysis task\nGetting feedback from the smart student on how you solved your programming/analysis task\n\nIn scenario 1, you are robbed of your thought processes on how to approach the task, divide into to manageable sub tasks and then come up with structured solutions, which combined, will solve the overall task. This process is referred to as “Computational Thinking”.\nIn scenario 2, you go through the above outlined computational thinking process and you train and develop your competencies\nIn this course, the importance of being aware of actively building and training your computational thinking competencies, while also building your technical toolbox cannot be underestimated!!!\nCan an AI solve the majority of tasks you will work with in this course? YES!!! BUT, it is up to you as a student to actively engage the course taking responsibility for building competencies as outlined above.\nBasically, what will you learn, if you let the smartest student do your homework? 🤷\nHere is the long term catch, if you do not possess competencies beyond what an AI can do, then why would a future employer hire you instead of just getting an AI to solve the tasks???\nIt is not just about passing this course, it is about understanding that in this course, you will be building the foundation for your future career.\n\nGetting started\nFor this exercise, you will need access to some\n\nChatGPT by OpenAI\nClaude by Anthropic\nCoPilot by Microsoft\nGemini by Google\nGrok by X.ai\n\nThere are many others and more keep coming, the development in this field is extremely rapid!\nIn the following, please feel free to use whichever of the above you prefer or perhaps try one, which is new to you.\n\nLet us get acquainted\nI am assuming that most students by now have encountered this technology. Nevertheless, let us get this exercise started:\n\nT1: Start a new chat and enter the question: “Explain in simple terms what you are”\nT2: Now enter the question: “Explain in simple terms how you work”\n\nAnd then in your group, compare the output from the AI and answer:\n\nQ1: Were the answers the same, when using different AIs?\nQ2: Were the answers the same, when using the same AI?\nQ3: Discuss in your group why/why not?\nT3: Use your favourite AI and try to get input on your “why/why not”-discussion\nQ4: Discuss in your group if the AI added additional reflections on why/why not and what the consequence of this is in context with solving tasks?\nQ5: Discuss in your group: Will the same person always give the same answer? How about different persons?\nQ6: Discuss in your group if these AIs basically are human? Discuss how they are created, how they generate text and if this limits their capabilities compared to humans. If you do not know how they are created of how the generate text, ask them\n\n\n\n\nUsing AI as an assistant, a use case\nIn this use case, we will work with the following paper:\n\nAccurate prediction of HLA class II antigen presentation across all loci using tailored data acquisition and refined machine learning\n\nYou are working as a researcher, or a consultant or a bioinformatician or a bio data scientist and your boss have given you this paper and asked you to look into it. You decide to use AI:\n\nT4: Download the paper, then upload it to your favourite AI and ask it: “Explain in simple terms what this paper is a about and create a simple brief summary” and read the summary, perhaps if something is unclear, ask the AI to elaborate\n\nYou take interest in the pseudo-sequence distance metric and again you decide to use AI:\n\nT5: Ask your favourite AI: “Please in simple terms explain the pseudo-sequence distance metric used in the paper”, again if something is unclear, simply ask for elaboration\n\nLikely your AI will outline something concerning similarity or distance between these protein molecules. Let us dig a bit further in:\n\nT6: Ask your favourite AI: “Please in simple terms explain exactly how the authors calculate this pseudo-sequence distance metric between two molecules”\n\nThe AI may or may not point you in the right direction. Being a bit unsure of this, you dig into the paper yourself and you find equation 2:\n\nT7: Ask your favourite AI: “As I read the paper, the authors define the pseudo-sequence distance metric using equation 2. Please state the equation and in simple terms explain the terms”\n\nAgain, the AI may or may not point you in the right direction.\n\nT8: Check that the equation the AI returns looks like equation 2, if not (which may or may not happen!):\n\nT8a: Take a moment to ponder: You were quite specific in your instructions and the AI returned a wrong equation with FULL confidence! (This is called AI hallucination and it’s something to be VERY aware of!)\nT8b: Then state: “Looking at the equation you have stated, I see it does not seem to match equation 2 in the paper. Please check and correct”\n\n\nOnce you are confident that the AI has the right equation you now want to impress your boss and decide to create an R-function:\n\nT9: Check that the equation the AI returns looks exactly like equation 2 and then state: “Please propose an R-function, which given two pseudo-sequences computes and returns the pseudo-sequence distance metric. Include an example of running the function and getting a result”\n\nNote, in the proposed code, you can simply in the upper right corner click copy.\n\nT10: Copy the code and paste it into the console in your RStudio Cloud server session and then run it, by hitting return. Run the function with these two examples CASSIRSSYEQYF and CASSLGQGNTLYF and compare with your group, did all of your functions run? Did you all get the same value?\n\n\n\n\nPssst… Don’t know how to do this and need a hint? Try to click here…\n\n\nWell, in this exercise, ask your faourite AI: “How do I run these two examples paste examples through this function paste function\n\n\nT11: Now, return to your favourite AI and enter: “Please propose an R-function for randomly generating a pseudo sequence. Then use that to compute the pseudo-sequence distance metric 10 times”\n\nAgain, run this code and check the 10 computed distances. The exact meaning of these 10 examples is not clear to you, so you decide to again turn to your AI:\n\nT12: Enter: “The exact meaning of these values are not clear to me. Here are my computed values:” and then copy paste the computed values from your R-console\n\nThe AI will now come with some longer explanation, which may or may not be correct.\nOk, so now you want to collect everything, so you enter: “I now want a complete script, please create a script, with a function implementing the pseudo sequence distance calculation, a function, which can randomly generate amino acid sequences and then finally, generate 2 times 10 random amino acid sequences and compute all pseudo sequence distances”\nAnd then let’s vibe-code: Simply copy/paste the script and run it in the console, if it does not run, simply copy/paste the error into your favourite AI and keep doing that until you code runs.\n\nT13: Once your code runs, try to see if you can share your examples with your group and compare if you get the same distances\n\nFinally:\n\nQ7: Assuming your code runs, discuss if you understand the details of the code and whether you would be comfortable sharing the code into an environment, where it is to support a multi-million-dollar drug development pipeline?\nQ8: Knowing that these AI assistants can hallucinate, i.e. give you information, which is completely wrong equally confident as information, which is correct, how do you know, if your function does what it is supposed to?\n\nThat’s it for today and remember moving in this course: DON’T use AI to generate the solution to an exercise, instead think your way through the exercise, generate YOUR solution and then DO get feedback from AI on your solution!\nPut in the time and the effort in this course and you will learn how to create the good solution and often even better than an AI and that’s your angle right there!"
  },
  {
    "objectID": "lab02.html",
    "href": "lab02.html",
    "title": "Lab 2: Data Visualisation I",
    "section": "",
    "text": "Package(s)",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#schedule",
    "href": "lab02.html#schedule",
    "title": "Lab 2: Data Visualisation I",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: pre-course anonymous questionaire Walk-through\n08.15 - 08.30: Recap: RStudio Cloud, RStudio and R - The Very Basics (Live session)\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#learning-materials",
    "href": "lab02.html#learning-materials",
    "title": "Lab 2: Data Visualisation I",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nBook: R4DS2e: Chapter 1 Data Visualisation\nPaper: “A Layered Grammar of Graphics” by Hadley Wickham\nVideo: The best stats you’ve ever seen\nVideo: The SDGs aren’t the same old same old, video from the Gapminder foundation\nVideo: EMBL Keynote Lecture - “Data visualization and data science” by Hadley Wickham\nPrimer: If you are not familiar with working with paths and projects, please read this primer on Paths and Projects\n\nUnless explicitly stated, do not do the per-chapter exercises in the R4DS2e book",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#learning-objectives",
    "href": "lab02.html#learning-objectives",
    "title": "Lab 2: Data Visualisation I",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nExplain the basic theory of data visualisation\nDecipher the components of a simple ggplot\nUse ggplot to do basic data visualisation",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#sec-exercises",
    "href": "lab02.html#sec-exercises",
    "title": "Lab 2: Data Visualisation I",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#prelude",
    "href": "lab02.html#prelude",
    "title": "Lab 2: Data Visualisation I",
    "section": "Prelude",
    "text": "Prelude\nFirst, make sure you are where you supposed to be, i.e. 1. In a group, 2. On Slack, 3. On the RStudio Cloud Server and 4. on Github. If not, see the Getting Started section\nThen, discuss these 4 visualisations with your group members:\n\nWhat is problematic?\nWhat could be done to rectify?\n\n\n\n\nClick here for visualisation 1\n\n\nNote, AMR = Antimicrobial Resistance\n\n\n\n\n\nClick here for visualisation 2\n\n\n\n\n\n\n\nClick here for visualisation 3\n\n\n\n\n\n\n\nClick here for visualisation 4",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#getting-started",
    "href": "lab02.html#getting-started",
    "title": "Lab 2: Data Visualisation I",
    "section": "Getting Started",
    "text": "Getting Started\nFirst of all, make sure to read every line in these exercises carefully!\nIf you get stuck with Quarto, revisit R4DS2e: Chapter 28 Quarto or take a look at the Comprehensive guide to using Quarto\nIf you get stuck working with paths and projects, remember there is a primer to help you, see the preparations materials for this session\n\nGo to the R for Bio Data Science RStudio Cloud Server session from last time and log in and choose the project you created.\nMake sure you are working in the right project, check in the upper right corner, it should say r_for_bio_data_science\nCreate a NEW Quarto Document for today’s exercises, e.g. lab02_exercises.qmd and remember to SAVE it in the same location as your .Rproj-file\n\nRecall the layout of the IDE (Integrated Development Environment)\n\n\n\n\n\nThen, before we start, we need to fetch some data to work on.\n\nSee if you can figure out how to create a new folder called “data”, make sure to place it the same place as your r_for_bio_data_science.Rproj file.\n\nThen, without further ado, run each of the following lines separately in your console:\n\ntarget_url &lt;- \"https://github.com/ramhiser/datamicroarray/raw/master/data/gravier.RData\"\noutput_file &lt;- \"data/gravier.RData\"\ncurl::curl_download(url = target_url, destfile = output_file)\n\n\n\n\nClick here for a hint if you get an error along the lines of Failed to open file...\n\n\nThink about what we are doing here. If you look at the code chunk above, then you can see that we are setting a variable target_url, which defines “a location on the internet”. From here we want to download some data. We need to tell R, where to put this data, so we also define the output_file-variable. Then we call the function curl_download() from the curl-package, hence the notation curl::curl_download(). As arguments to the function-parameters url and destfile, we pass the aforementioned variables. So far so good. Now, look at the target_url- and the output_file-variables, which one of those are we responsible for? If you think about it - We cannot change the target_url, we can only change the output_file, so there is a good chance, that the error you are getting pertains to this variable. But what does the variable contain? It contains the path to where you want to place the file, that is the data/-part and then it contains the filename you want to use, that is the gravier-part and then finally, it contains the filetype you want to use, that is the .RData-part. The filename and filetype is up to you to choose, but the path… Well, R can only find that if it exists… So… Did you remember to create a folder data in the same location as your r_for_bio_data_science.Rproj-file? If this small intermezzo has left things even more unclear, you should go over the primer on paths and projects. Remember, if at first you don’t succeed - Try and try again!\n\n\nUsing the files pane, check the folder you created to see if you managed to retrieve the file.\n\nRecall the syntax for a new code chunk:\n  ```{r}\n  #| echo: true\n  #| eval: true\n  # Here goes the code... Note how this part does not get executed because of the initial hashtag, this is called a code-comment\n  1 + 1\n  my_vector &lt;- c(1, 2, 3)\n  my_mean &lt;- mean(my_vector)\n  print(my_mean)\n\nIMPORTANT! You are mixing code and text in a Quarto Document! Anything within a \"chunk\" as defined above will be evaluated as code, whereas anything outside the chunks is markdown. You can use shortcuts to insert new code chunks:\n\n- Mac: CMD + OPTION + i\n- Windows: CTRL + ALT + i\n\nNote, this might not work, depending on your browser. In that case you can insert a new code chunk using ![](images/insert_new_chunk) or You can change the shortcuts via “Tools” &gt; “Modify Keyboard shortcuts…” &gt; Filter for “Insert Chunk” and then choose the desired shortcut. E.g. change the shortcut for code chunks to Shift+Cmd+i or similar.\n\n- Add a new code chunk and use the `load()` function to load the data you retrieved.\n\n&lt;details&gt;\n&lt;p&gt;&lt;summary&gt;Click here for a hint&lt;/summary&gt;&lt;/p&gt;\nRemember, you can use `?load` to get help on how the function works and remember your project root path is defined by the location of your `.Rproj` file, i.e. the path. A path is simply where `R` can find your file, e.g. `/home/projects/r_for_bio_data_science/` or similar depending on your particular setup.\n&lt;/details&gt;\n\nNow, in the console, run the `ls()` command and confirm, that you did indeed load the `gravier` data.\n\n- Read the information about the `gravier` data [here](https://github.com/ramhiser/datamicroarray/wiki/Gravier-%282010%29)\n\nNow, in your Quarto Document, add a new code chunk like so\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(\"tidyverse\")\n:::\nThis will load our data science toolbox, including ggplot.",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#create-data",
    "href": "lab02.html#create-data",
    "title": "Lab 2: Data Visualisation I",
    "section": "Create data",
    "text": "Create data\nBefore we can visualise the data, we need to wrangle it a bit. Nevermind the details here, we will get to that later. Just create a new chunk, copy/paste the below code and run it:\n\nset.seed(676571)\ncancer_data=mutate(as_tibble(pluck(gravier,\"x\")),y=pluck(gravier,\"y\"),pt_id=1:length(pluck(gravier, \"y\")),age=round(rnorm(length(pluck(gravier,\"y\")),mean=55,sd=10),1))\ncancer_data=rename(cancer_data,event_label=y)\ncancer_data$age_group=cut(cancer_data$age,breaks=seq(10,100,by=10))\ncancer_data=relocate(cancer_data,c(pt_id,age,age_group,pt_id,event_label))\n\nNow we have the data set as a tibble, which is an augmented data frame (we will also get to that later):\n\ncancer_data\n\n# A tibble: 168 × 2,909\n   pt_id   age age_group event_label    g2E09    g7F07    g1A01   g3C09    g3H08\n   &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;\n 1     1  34.2 (30,40]   good        -0.00144 -0.00144 -0.0831  -0.0475  1.58e-2\n 2     2  47   (40,50]   good        -0.0604   0.0129  -0.00144  0.0104  3.16e-2\n 3     3  60.3 (60,70]   good         0.0398   0.0524  -0.0786   0.0635 -3.95e-2\n 4     4  57.8 (50,60]   good         0.0101   0.0314  -0.0218   0.0215  8.68e-2\n 5     5  54.9 (50,60]   good         0.0496   0.0201   0.0370   0.0311  2.07e-2\n 6     6  58.8 (50,60]   good        -0.0664   0.0468   0.00720 -0.370   2.88e-3\n 7     7  52.9 (50,60]   good        -0.00289 -0.0816  -0.0291  -0.0249 -1.74e-2\n 8     8  74.5 (70,80]   good        -0.198   -0.0499  -0.0634  -0.0298  3.00e-2\n 9     9  47.6 (40,50]   good         0.00288  0.0201   0.0272   0.0174 -7.89e-5\n10    10  55.8 (50,60]   good        -0.0574  -0.0574  -0.0831  -0.0897 -1.01e-1\n# ℹ 158 more rows\n# ℹ 2,900 more variables: g1A08 &lt;dbl&gt;, g1B01 &lt;dbl&gt;, g1int1 &lt;dbl&gt;, g1E11 &lt;dbl&gt;,\n#   g8G02 &lt;dbl&gt;, g1H04 &lt;dbl&gt;, g1C01 &lt;dbl&gt;, g1F11 &lt;dbl&gt;, g3F05 &lt;dbl&gt;,\n#   g3B09 &lt;dbl&gt;, g1int2 &lt;dbl&gt;, g2C01 &lt;dbl&gt;, g1A05 &lt;dbl&gt;, g1E01 &lt;dbl&gt;,\n#   g1B05 &lt;dbl&gt;, g3C05 &lt;dbl&gt;, g3A07 &lt;dbl&gt;, g1F01 &lt;dbl&gt;, g2D01 &lt;dbl&gt;,\n#   g1int3 &lt;dbl&gt;, g1int4 &lt;dbl&gt;, g1D05 &lt;dbl&gt;, g1E05 &lt;dbl&gt;, g1G05 &lt;dbl&gt;,\n#   g1C05 &lt;dbl&gt;, g1G11 &lt;dbl&gt;, g2D08 &lt;dbl&gt;, g2E06 &lt;dbl&gt;, g3H09 &lt;dbl&gt;, …\n\n\n\nQ1: What is this data?\n\n\n\n\nClick here for a hint\n\n\nWhere did the data come from?\n\n\nQ2: How many rows and columns are there in the data set in total?\n\n\n\n\nClick here for a hint\n\n\nDo you think you are the first person in the world to try to find out how many rows and columns are in a data set in R?\n\n\nQ3: Which are the variables and which are the observations in relation to rows and columns?",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#ggplot---the-very-basics",
    "href": "lab02.html#ggplot---the-very-basics",
    "title": "Lab 2: Data Visualisation I",
    "section": "ggplot - The Very Basics",
    "text": "ggplot - The Very Basics\n\nGeneral Syntax\nThe general syntax for a basic ggplot is:\n\nggplot(data = my_data,\n       mapping = aes(x = variable_1_name,\n                     y = variable_2_name)) +\n  geom_something() +\n  labs()\n\nNote the + for adding layers to the plot\n\nggplot the plotting function\nmy_data the data you want to plot\naes() the mappings of your data to the plot\nx data for the x-axis\ny data for the y-axis\ngeom_something() the representation of your data\nlabs() the x-/y-labels, title, etc.\n\nNow:\n\nRevisit this illustration and discuss in your group what is what:\n\n\nA very handy ggplot cheat-sheet can be found here\n\n\nBasic Plots\nRemember to write notes in your Quarto document. You will likely revisit these basic plots in future exercises.\nPrimer: Plotting 2 x 20 random normally distributed numbers, can be done like so:\n\nggplot(data = tibble(x = rnorm(20),\n                     y = rnorm(20)),\n       mapping = aes(x = x,\n                     y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\nUsing this small primer, the materials you read for today and the cancer_data you created, in separate code-chunks, create a:\n\nT1: scatter plot of one variable against another\nT2: line graph of one variable against another\nT3: box plot of one variable (Hint: Set x = \"my_gene\" in aes())\nT4: histogram of one variable\nT5: densitogram of one variable\n\nRemember to write notes to yourself, so you know what you did and if there is something in particular you want to remember.\n\nQ4: Do all geoms require both x and y?\n\n\n\nExtending Basic Plots\n\nT6: Pick your favourite gene and create a box plot of expression levels stratified on the variable event_label\nT7: Like T6, but with densitograms GROUP ASSIGNMENT (Important, see: how to)\nT8: Pick your favourite gene and create a box plot of expression levels stratified on the variable age_group\n\nThen, add stratification on event_label\nThen, add transparency to the boxes\nThen, add some labels\n\nT9: Pick your favourite gene and create a scatter plot of expression levels versus age\n\nThen, add stratification on event_label\nThen, add a smoothing line\nThen, add some labels\n\nT10: Pick your favourite two genes and create a scatter plot of their expression levels\n\nThen, add stratification on event_label\nThen, add a smoothing line\nThen, show split into separate panes based on the variable age_group\nThen, add some labels\nChange the event_label title of the legend\n\nT11: Recreate the following plot\n\n\n\n\n\n\n\n\n\n\n\nQ5: Using your biological knowledge, what is your interpretation of the plot?\nT12: Recreate the following plot\n\n\n\n\n\n\n\n\n\n\n\nQ6: Using your biological knowledge, what is your interpretation of the plot?\nT13: If you arrive here and there is still time left for the exercises, you are probably already familiar with ggplot - Use what time is left to challenge yourself to further explore the cancer_data and create some nice data visualisations - Show me what you come up with!",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab02.html#further-resources-for-data-visualisation",
    "href": "lab02.html#further-resources-for-data-visualisation",
    "title": "Lab 2: Data Visualisation I",
    "section": "Further resources for data visualisation",
    "text": "Further resources for data visualisation\n\nA very handy ggplot cheat-sheet can be found here\nSo which plot to choose? Check this handy guide\nExplore ways of plotting here",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "lab03.html",
    "href": "lab03.html",
    "title": "Lab 3: Data Visualisation II",
    "section": "",
    "text": "ggplot2\npatchwork\nscales\nggridges"
  },
  {
    "objectID": "lab03.html#schedule",
    "href": "lab03.html#schedule",
    "title": "Lab 3: Data Visualisation II",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.45: Recap of Lab 2 and Lecture\n08.45 - 09.00: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab03.html#learning-materials",
    "href": "lab03.html#learning-materials",
    "title": "Lab 3: Data Visualisation II",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: R4DS2e: “Visualize”, chapters 9, 10 and 11\nVideo: William Chase | The Glamour of Graphics | RStudio (2020)\nWeb: Patchwork - Getting started\nWeb: Scales - Getting started\n\nUnless explicitly stated, do not do the per-chapter exercises in the R4DS2e book"
  },
  {
    "objectID": "lab03.html#learning-objectives",
    "href": "lab03.html#learning-objectives",
    "title": "Lab 3: Data Visualisation II",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUse more advanced ggplot features\nCustomise the data visualisation\nCombine multiple plots into one pane\nLook at a more advanced ggplot and decipher the components used"
  },
  {
    "objectID": "lab03.html#sec-exercises",
    "href": "lab03.html#sec-exercises",
    "title": "Lab 3: Data Visualisation II",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nRead the steps of this exercises carefully, while completing them\n\nAssignment Feedback\nEveryone who met the deadline for handing in last week’s assignment, should have gotten written feedback - Make sure to read this, so you get feedback on your work and progress!\nIn the unexpected case, that you met the deadline, but did not receive any feedback, please make sure to contact the teaching team\n\n\nIntroduction\nSome authors are kind enough to supply the data they used for their paper, e.g.,\n\n“Assessment of the influence of intrinsic environmental and geographical factors on the bacterial ecology of pit latrines” (Torondel et al. 2016)\n\nWhere the supporting data can be found here:\n\nhttp://userweb.eng.gla.ac.uk/umer.ijaz/bioinformatics/ecological.html\n\n\n\nGetting Started\nAgain, go to the R for Bio Data Science RStudio Cloud Server session from last time and login and make sure you are working in the r_for_bio_data_science project you created previously, then:\n\nCreate a new Quarto Document for today’s exercises, e.g. lab03_exercises.qmd\nNB! The Quarto document MUST be placed together with your .Rproj file (defining, the project root - look in your Files tab) and also there, the data-folder should be placed!\nREMEMBER paths are important! Also, R is case-sensitive, i.e. “data” is not the same as “Data”\n\nSee Paths and Projects\n\n\nThe here package as your friendly neighborhood path helper\nPaths can be tricky and as you have seen, some file formats cough .qmd’s cough, may not really feel that strongly about your project setup, so… The here package to the rescue.\nIf you decided to put your qmd-file in e.g. an exercises-directory, which is a very nice and perfectly understandable idea, then you will find that the following chunk will not work:\n\nlibrary(\"tidyverse\")\nload(file = \"data/gravier.RData\") # Nope!\n\nSo here is the trick, here keeps track of your project root, as defined by the .Rproj-file. So the following will work even if you have your .qmd-files in other directories:\n\nlibrary(\"here\")\nload(file = here(\"data/gravier.RData\")) # Yup!\n\nEnjoy your friendly neighborhood path helper!\n\n\nGetting the data\nAdd a new code chunk and add the following code (Never mind the details, we will get back to this), remember you can use headers to nicely section your quarto Document.\n\nbase_url <- \"http://userweb.eng.gla.ac.uk/umer.ijaz/bioinformatics/ecological/\"\nraw_data_path <- \"data/_raw/\"\n\nSPE <- read_csv(file = str_c(base_url, \"SPE_pitlatrine.csv\"))\nwrite_csv(x = SPE,\n          file = str_c(raw_data_path, \"SPE_pitlatrine.csv\"))\n\nENV <- read_csv(file = str_c(base_url, \"ENV_pitlatrine.csv\"))\nwrite_csv(x = ENV,\n          file = str_c(raw_data_path, \"ENV_pitlatrine.csv\"))\n\nAdd the chunk settings #| echo: true and #| eval: true, then run the block and change the latter to #| eval: false.\n\nDiscuss in your group, what this means and why we do it\n\n\n\n\nClick here for hint\n\n\nWhere do we retrieve the data from, where do we write it to, and what happens if we run the chunk more than one time?\n\n\n\nWrangling the data\n\nWhat is data wrangling?\n\nBefore we continue with plotting, we want to unify the data, so here again you will run some code, where the details are not important right now.\nBut… Make sure, that you have run library(\"tidyverse\") somewhere in your Quarto document - Perhaps under an initial header saying “Load Libraries” or similar?\n\nSPE |> \n  pivot_longer(cols = -Taxa,\n               names_to = \"Samples\",\n               values_to = \"OTU_Count\")  |> \n  full_join(ENV, by = \"Samples\") |> \n  mutate(site = case_when(str_detect(Samples, \"^T\") ~ \"Tanzania\",\n                          str_detect(Samples, \"^V\") ~ \"Vietnam\")) |>  \n  write_tsv(file = \"data/SPE_ENV.tsv\")\n\nChange the chunk settings as before"
  },
  {
    "objectID": "lab03.html#data-visualisation-ii",
    "href": "lab03.html#data-visualisation-ii",
    "title": "Lab 3: Data Visualisation II",
    "section": "Data Visualisation II",
    "text": "Data Visualisation II\n\nRead the data\n\nSPE_ENV <- read_tsv(file = \"data/SPE_ENV.tsv\")\nSPE_ENV\n\n# A tibble: 4,212 × 15\n   Taxa   Samples OTU_Count    pH  Temp    TS    VS   VFA  CODt  CODs perCODsbyt\n   <chr>  <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>      <dbl>\n 1 Acido… T_2_1           0  7.82  25.1  14.5 71.3   71     874   311         36\n 2 Acido… T_2_10          0  9.08  24.2  37.8 31.5    2     102     9          9\n 3 Acido… T_2_12          0  8.84  25.1  71.1  5.94   1      35     4         10\n 4 Acido… T_2_2           0  6.49  29.6  13.9 64.9    3.7   389   180         46\n 5 Acido… T_2_3           0  6.46  27.9  29.4 26.8   27.5   161    35         22\n 6 Acido… T_2_6           0  7.69  28.7  65.5  7.03   1.5    57     3          6\n 7 Acido… T_2_7           0  7.48  29.8  36.0 34.1    1.1   107     9          8\n 8 Acido… T_2_9           0  7.6   25    46.9 19.6    1.1    62     8         13\n 9 Acido… T_3_2           0  7.55  28.8  12.6 51.8   30.9   384    57         15\n10 Acido… T_3_3           0  7.68  28.9  14.6 48.1   24.2   372    57         15\n# ℹ 4,202 more rows\n# ℹ 4 more variables: NH4 <dbl>, Prot <dbl>, Carbo <dbl>, site <chr>\n\n\n\n\nIMPORTANT INSTRUCTIONS - READ!\nFor these exercises, you will have to identify what you see in the plot!\nFor each plot, complete the following steps\n\nLook at this overview of the components of a ggplot (see below)\nLook at the plot you are to recreate and discuss in the group:\n\nWhat is the data? Take a look at it and understand what is in the data\nWhat are the mappings? I.e. what variables are on the x-/y-axis?\nAre there any colour-/fill-mappings?\nWhat are the geoms used?\nAre there any modifications to theme?\n\n\n\n\n\nClick here for hint\n\n\n\nConsult the Data visualization with ggplot2 cheatsheet\nCheck which options you have available\nConsult the chapters in the book you read, see preparation materials for labs 2 and 3\n\n\n\n\n\nTASKS\n\nTask 1 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 2 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 3 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 4 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 5 - Recreate the following plots\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nSame data, but a transformation happened, changing the representation of the data. Look carefully at the axes.\n\n\n\nTask 6 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nSee if you can find something online on geom_smooth()\n\n\n\nTask 7 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nThink about fill and then see if you can find something online on geom_tile(), scale_fill_gradient2 and how to ggplot rotate axis labels\n\n\n\nTask 8 - Recreate the following plot\nStart by running this code in a new chunk (ignore details for now)\n\ntargets <- c(\"Methanobacteria\", \"Clostridia\", \"Actinobacteria\",\n            \"Sphingobacteria\", \"Anaerolineae\")\nSPE_ENV_targets <- SPE_ENV |>\n  filter(Taxa %in% targets)\n\nand then use the created dataset SPE_ENV_targets to recreate this plot:\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nHere we need to use geom_density_ridges(), but which package contains this? Also, we are using a colour scale called viridis, but how do we add this? Also, perhaps there are more themes we can use than just theme_classic()?\n\n\n\nTask 9 - GROUP ASSIGNMENT (Important, see: how to)\nFor this assignment you and your group are to apply what you have learned in the two data visualisation labs. The task is to be creative and make a really nice plot using the SPE- and ENV-data sets or a relevant subset hereof, remember to include how you arrive at subsets of the data in your assignment\nTry to play around with some custom colouring. There is a nice tool to aid in choosing colours for visualisations here\n\n\n\n\nTorondel, Belen, Jeroen H. J. Ensink, Ozan Gundogdu, Umer Zeeshan Ijaz, Julian Parkhill, Faraji Abdelahi, Viet-Anh Nguyen, et al. 2016. “Assessment of the Influence of Intrinsic Environmental and Geographical Factors on the Bacterial Ecology of Pit Latrines.” Microbial Biotechnology 9 (2): 209–23. https://doi.org/10.1111/1751-7915.12334."
  },
  {
    "objectID": "lab04.html",
    "href": "lab04.html",
    "title": "Lab 4: Data Wrangling I",
    "section": "",
    "text": "dplyr\nreadr\ntibble"
  },
  {
    "objectID": "lab04.html#schedule",
    "href": "lab04.html#schedule",
    "title": "Lab 4: Data Wrangling I",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap of Lab 3\n08.15 - 08.30: Assignment 2 walk-through\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab04.html#learning-materials",
    "href": "lab04.html#learning-materials",
    "title": "Lab 4: Data Wrangling I",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nBook: R4DS2e: Chapter 3 Data transformation\nBook: R4DS2e: Chapter 4 Workflow: Code Style\nBook: R4DS2e: Chapter 7 Data import\nBook: R4DS2e: Chapter 8 Workflow: getting help\nWeb: What is data wrangling? Intro, Motivation, Outline, Setup – Pt. 1 Data Wrangling Introduction\nWeb: (NB! STOP at 7:45, i.e. skip tidyr) Tidy Data and tidyr – Pt 2 Intro to Data Wrangling with R and the Tidyverse\nWeb: Data Manipulation Tools: dplyr – Pt 3 Intro to the Grammar of Data Manipulation with R\n\nUnless explicitly stated, do not do the per-chapter exercises in the R4DS2e book"
  },
  {
    "objectID": "lab04.html#learning-objectives",
    "href": "lab04.html#learning-objectives",
    "title": "Lab 4: Data Wrangling I",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUnderstand and apply the 6 basic dplyr verbs: filter(), arrange(), select(), mutate(), summarise() and group_by()\nConstruct and apply logical statements in context with dplyr pipelines\nUnderstand and apply the additional verbs count(), drop_na(), View()\nCombine dplyr verbs to form a data manipulation pipeline using the pipe |> operator\nDecipher the components and functions hereof in a dplyr pipeline"
  },
  {
    "objectID": "lab04.html#sec-exercises",
    "href": "lab04.html#sec-exercises",
    "title": "Lab 4: Data Wrangling I",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nImportant: As we progress, avoid using black-box do-everything-with-one-command R-packages like e.g. ggpubr - I want you to learn the underlying technical bio data science! …and why is that? Because if you use these types and-then-magic-happens packages, you will be limited to their functionality and what if you want to create something custom? Something beyond stock? Some awesome idea that you got? If you truly understand ggplot - the sky is the limit! Basically, it’s the cliché: “If you give a man a fish, you feed him for a day. If you teach a man to fish, you feed him for a lifetime”\n\nGetting Started\nFirst, make sure to read and discuss the feedback you got from last week’s assignment!\nThen, once again go to the R for Bio Data Science RStudio Cloud Server\nFirst things first:\n\nLet’s create a new Quarto Document for today’s exercises, e.g. lab04_exercises.qmd\n\n\n\nBrief refresh of Quarto so far\nRecall the syntax for a new code chunk, where all your R code goes. Any text and notes must be outside the chunk tags:\n\n```{r}\n# Here the code goes\n1 + 1\nx <- c(1, 2, 3)\nmean(x)\n```\n\n[1] 2\n[1] 2\n\n\nOur markdown goes outside the code-chunks, e.g.:\n  # Header level 1\n  ## Header level 2\n  ### Header level 3\n  *A text in italics*\n  **A text in bold**\n  Normal text describing and explaining\nNow, in your new Quarto document, create a new Header level 2, e.g.:\n  ## Load Libraries\nand under your new header, add a new code chunk, like so\n\n```{r}\n#| message: false\nlibrary(\"tidyverse\")\n```\n\nAnd run the chunk. This will load our data science toolbox, including dplyr (and ggplot). But wait, what does #| message: false do? 🤷️ 🤔\nTry to structure your Quarto document as your course notes, i.e. add notes while solving the exercises aiming to create a reference that you can revisit for your final project work… And… For future reference after this course, where hopefully you’ll really reap what you sow\nBonus info: We are engineers, so of course, we love equations, we can include standard \\(\\LaTeX\\) syntax, e.g.:\n  $E(x) = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} x_{i}$\nTry it!\n(Note, this might not work if you do a copy/paste, so try to type it)\n\n\nA few handy short cuts\nInsert new code chunk:\n\nMac: CMD + OPTION + i\nWindows: CTRL + ALT + i\n\nRender my Quarto document:\n\nMac: CMD + SHIFT + k\nWin: CTRL + SHIFT + k\n\nRun line in chunk:\n\nMac: CMD + ENTER\nWin: CTRL + ENTER\n\nRun entire chunk:\n\nMac: CMD + SHIFT + ENTER\nWin: CTRL + SHIFT + ENTER\n\nInsert the pipe symbol |>:\n\nMac: CMD + SHIFT + m\nWin: CTRL + SHIFT + m\n\nNote, if you’re trying this out and you see %>% instead of |>, then go to Tools, Global Options..., Code and check the box Use native pipeoperator, |>"
  },
  {
    "objectID": "lab04.html#a-few-initial-questions",
    "href": "lab04.html#a-few-initial-questions",
    "title": "Lab 4: Data Wrangling I",
    "section": "A few initial questions",
    "text": "A few initial questions\nFirst, if you don’t feel completely comfortable with the group_by |> summarise workflow, then no worries - Feel free to visit this short R Tutorial: Grouping and summarizing\nThen, in your groups, discuss the following primer questions. Note, when asked for “what is the output”, do not run the code in the console, instead try to talk and think about it and write your answers and notes in your Quarto document for the day:\nFirst, in a new chunk, run tibble(x = c(4, 3, 5, 1, 2)), so you understand what it does, then - Discuss in your group, what is the output of, remember first talk, then check understanding by running code:\n\nQ1: tibble(x = c(4, 3, 5, 1, 2)) |> filter(x > 2)?\nQ2: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(x)?\nQ3: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(desc(x))?\nQ4: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(desc(desc(x)))?\nQ5: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(x)?\nQ6: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(y)?\nQ7: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(-x)?\nQ8: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(-x, -y)?\nQ9: tibble(x = c(4, 3, 5, 1, 2)) |> mutate(x_dbl = 2*x)?\nQ10: tibble(x = c(4, 3, 5, 1, 2)) |> mutate(x_dbl = 2 * x, x_qdr = 2*x_dbl)?\nQ11: tibble(x = c(4, 3, 5, 1, 2)) |> summarise(x_mu = mean(x))?\nQ12: tibble(x = c(4, 3, 5, 1, 2)) |> summarise(x_max = max(x))?\nQ13: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, NA, 5, 1, 2)) |> group_by(lbl) |> summarise(x_mu = mean(x), x_max = max(x))?\nQ14: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, 3, 5, 1, 2)) |> group_by(lbl) |> summarise(n = n())?\nQ15: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, 3, 5, 1, 2)) |> count(lbl)?\n\nIn the following, return to these questions and your answers for reference on the dplyr verbs!"
  },
  {
    "objectID": "lab04.html#load-data",
    "href": "lab04.html#load-data",
    "title": "Lab 4: Data Wrangling I",
    "section": "Load data",
    "text": "Load data\nWait… If your write some R-code, which for some reason does not run, R will let you know what went wrong. Make sure to read these error messages. During this course the teaching team is available, but after the course, you’ll have to be comfortable with debugging, i.e. finding and fixing errors in your code. By-the-way, did you know the term debugging is attested to Admiral Grace Hopper?\nLet’s continue - Again, add a new header to your Quarto document, e.g. ## Load Data, then:\n\nGo to the Vanderbilt Biostatistics Datasets site\nFind Diabetes data and download the diabetes.csv file\nYou should have a data folder, if not, then in the Files pane, click the New Folder button, enter folder name data and click ok\nNow, click on the folder you created\nClick the  Upload button and navigate to the diabetes.csv file you downloaded\nClicking the two dots .. above the file you uploaded, look for  .., will take you one level up in your project path\nInsert a new code chunk in your Quarto document\nAdd and then run the following code\n\n\ndiabetes_data <- read_csv(file = data/diabetes.csv)\ndiabetes_data\n\nThen realise that we could simply have run the following code to do the exact same thing (Yes, readr is pretty nifty):\n\n# Create the data directory programmatically\ndir.create(path = \"data\")\n\n# Retrieve the data directly\ndiabetes_data <- read_csv(file = \"https://hbiostat.org/data/repo/diabetes.csv\")\n\n# Write the data to disk\nwrite_csv(x = diabetes_data,\n          file = \"data/diabetes.csv\")\n\nJust remember the echo/eval trick from last session to avoid retrieving online data each time you render your Quarto document"
  },
  {
    "objectID": "lab04.html#work-with-the-diabetes-data-set",
    "href": "lab04.html#work-with-the-diabetes-data-set",
    "title": "Lab 4: Data Wrangling I",
    "section": "Work with the diabetes data set",
    "text": "Work with the diabetes data set\nFirst, take a few minutes to read about this dataset. Go to the Vanderbilt Department of Biostatistics site and find the Diabetes data header and directly below that, click the link diabetes.html. This will contain some meta data on the data.\nNow, use the pipe |> to use the View() function to inspect the data set. Note, if you click the -button, you will get a spreadsheet-like view of the data, allowing you to get an overview (Psst… No need to use Excel…).\n\nQ1: How many observations and how many variables?\nQ2: Is this a tidy data set? Which three rules must be satisfied?\nQ3: When you run the chunk, then underneath each column name is stated <chr> and <dbl> what is that?\n\nBefore we continue\n\nT1: Change the height, weight, waist and hip from the imperial system (inches/pounds) to the metric system (cm/kg), rounding to 1 decimal\n\n\n\n\nLet us try to take a closer look at various subsets of the data. For the following questions, “How many …” refers to the number of rows in the subset of the data you create:\n\nQ4: How many weigh less than 100kg?\nQ5: How many weigh more than 100kg?\nQ6: How many weigh more than 100kg and are less than 1.6m tall?\nQ7: How many women are taller than 1.8m?\nQ8: How many men are taller than 1.8m?\nQ9: How many women in Louisa are older than 30?\nQ10: How many men in Buckingham are younger than 30 and taller than 1.9m?\nT2: Make a scatter plot of weight versus height and colour by sex for inhabitants of Louisa above the age of 40\nT3: Make a box plot of height versus location stratified on sex for people above the age of 50\n\nSorting columns can aid in getting an overview of variable ranges (don’t use the summary() function yet for this one)\n\nQ11: How old is the youngest person?\nQ12: How old is the oldest person?\nQ13: Of all the 20-year-olds, what is the height of the tallest?\nQ14: Of all the 20-year-olds, what is the height of the shortest?\n\nChoosing specific columns can be used to work with a subset of the data for a specific purpose\n\nQ15: How many columns (variables) starts_with a “b”?\nQ16: How many columns (variables) contains the word “eight”?\n\nCreating new variables is an integral part of data manipulation\n\nT4: Create a new variable, where you calculate the BMI\n\n\n\n\n\nT5: Create a BMI_class variable\n\nTake a look at the following code snippet to get you started:\n\ntibble(x = rnorm(10)) |> \n  mutate(trichotomised = case_when(\n    x < -1 ~ \"Less than -1\",\n    -1 <= x & x < 1 ~ \"larger than or equal to -1 and smaller than 1\",\n    1 <= x ~ \"Larger than or equal to 1\"))\n\nand then go read about BMI classification here and discuss in your group how to extract classifications from the Definition/Introduction section\nNote, the cut() function could be used here, but you should try to use case_when() as illustrated in the example chunk above.\n\n\n\nOnce you have created the variable, you will need to convert it to a categorical variable, in R, these are called a factor and you can set the levels like so:\n\ndiabetes_data <- diabetes_data |>\n  mutate(BMI_class = factor(BMI_class,\n                            levels =  c(\"my 1st category\", \"my 2nd category\",\n                                        \"my 3rd category\", \"my nth category\")))\n\nThis is very important for plotting, as this will determine the order in which the categories appear on the plot!\n\nT6: Create a box plot of hdl versus BMI_class\nQ17: What do you see?\nT7: Create a BFP (Body fat percentage) variable\n\n\n\n\nClick here for hint\n\n\nBFP can be calculated using the following equation from Jackson et al. (2002):\n\\[BFP = 1.39 \\cdot BMI + 0.16 \\cdot age - 10.34 \\cdot sex - 9\\]\nwhere \\(sex\\) is defined as being \\(0\\) for female and \\(1\\) for male.\n\n\n\n\n\nT8: Create a WHR (waist-to-hip ratio) variable\nQ18: What correlates better with BMI: WHR or BFP? GROUP ASSIGNMENT (Important, see: how to)\n\n\n\n\nClick here for hint\n\n\nIs there a certain plot-type, which can visualise if the relationship between two variables and give insights to if they are correlated? Can you perhaps use an R function to compute the “correlation coefficient”?. Do not use e.g. ggpubr or any other and-then-magic-happens package, use the knowledge you’ve gained so far)\n\nNow, with this augmented data set, let us create some summary statistics\n\nQ19: How many women and men are there in the data set?\nQ20: How many women and men are there from Buckingham and Louisa respectively in the data set?\nQ21: How many are in each of the BMI_class groups?\nQ22: Given the code below, explain the difference between A and B?\n\n\n# A\ndiabetes_data |>\n  ggplot(aes(x = BMI_class)) +\n  geom_bar()\n\n# B\ndiabetes_data |>\n  count(BMI_class) |>\n  ggplot(aes(x = BMI_class, y = n)) +\n  geom_col()\n\n\nT9: For each BMI_class group, calculate the average weight and associated standard deviation\nQ23: What was the average age of the women living in Buckingham in the study?\n\nFinally, if you reach this point and there is still time left. Take some time to do some exploratory plots of the data set and see if you can find something interesting.\n\n\n\n\nJackson, AS, PR Stanforth, J Gagnon, T Rankinen, AS Leon, DC Rao, JS Skinner, C Bouchard, and JH Wilmore. 2002. “The Effect of Sex, Age and Race on Estimating Percentage Body Fat from Body Mass Index: The Heritage Family Study.” International Journal of Obesity 26 (6): 789–96. https://doi.org/10.1038/sj.ijo.0802006."
  },
  {
    "objectID": "lab05.html",
    "href": "lab05.html",
    "title": "Lab 5: Data Wrangling II",
    "section": "",
    "text": "Package(s)",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#schedule",
    "href": "lab05.html#schedule",
    "title": "Lab 5: Data Wrangling II",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.30: Recap of Lab 4\n08.30 - 08.35: Lecture\n08.35 - 08.45: Break\n08.45 - 12.00: Exercises",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#learning-materials",
    "href": "lab05.html#learning-materials",
    "title": "Lab 5: Data Wrangling II",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: R4DS2e: Chapter 5 Data tidying\nBook: R4DS2e: Chapter 14 Strings\nBook: R4DS2e: Chapter 16 Factors\nBook: Chapter 19 Joins\nVideo: Tidy Data and tidyr - NB! Start at 7:45 and please note: gather() is now pivot_longer() and spread() is now pivot_wider()\nVideo: Working with Two Datasets: Binds, Set Operations, and Joins\nVideo: stringr (Playlist with 7 short videos)\n\nUnless explicitly stated, do not do the per-chapter exercises in the R4DS2e book",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#learning-objectives",
    "href": "lab05.html#learning-objectives",
    "title": "Lab 5: Data Wrangling II",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUnderstand and apply the various str_*() functions for string manipulation\nUnderstand and apply the family of *_join() functions for combining data sets\nUnderstand and apply pivot_wider() and pivot_longer()\nUse factors in context with plotting categorical data using ggplot",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#sec-exercises",
    "href": "lab05.html#sec-exercises",
    "title": "Lab 5: Data Wrangling II",
    "section": "Exercises",
    "text": "Exercises\n\nPrologue\nToday will not be easy! But please try to remember Hadley’s words of advice:\n\n“The bad news is, whenever you’re learning a new tool, for a long time, you’re going to suck! It’s gonna be very frustrating! But the good news is that that is typical and something that happens to everyone and it’s only temporary! Unfortunately, there is no way to going from knowing nothing about the subject to knowing something about a subject and being an expert in it without going through a period of great frustration and much suckiness! Keep pushing through!” - H. Wickham (dplyr tutorial at useR 2014, 4:10 - 4:48)",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#intro",
    "href": "lab05.html#intro",
    "title": "Lab 5: Data Wrangling II",
    "section": "Intro",
    "text": "Intro\nWe are upping the game here, so expect to get stuck at some of the questions. Remember - Discuss with your group how to solve the task, revisit the materials you prepared for today and naturally, the TAs and I are happy to nudge you in the right direction. Finally, remember… Have fun!\nRemember what you have worked on so far:\n\nRStudio\nQuarto\nggplot\nfilter\narrange\nselect\nmutate\ngroup_by\nsummarise\nThe pipe and creating pipelines\nstringr\njoining data\npivoting data\n\nThat’s quite a lot! Well done - You’ve come quite far already! Remember to think about the above tools in the following as we will synthesise your learnings so far into an analysis!",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#sec-background",
    "href": "lab05.html#sec-background",
    "title": "Lab 5: Data Wrangling II",
    "section": "Background",
    "text": "Background\nIn the early 20s, the world was hit by the coronavirus disease 2019 (COVID-19) pandemic. The pandemic was caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). In Denmark, the virus first confirmed case was on 27 February 2020.\nWhile initially very little was known about the SARS-CoV-2 virus, we did know the general pathology of vira. Briefly, the virus invades the cells and hijacks the intra-cellular machinery. Using the hijacked machinery, components for new virus particles are produced, eventually being packed into the viral envelope and released from the infected cell. Some of these components, viral proteins, is broken down into smaller fragments called peptides by the proteasome. These peptides are transported into the endoplasmic reticulum by the Transporter Associated with antigen Processing (TAP) protein complex. Here, they are aided by chaperones bound to the Major Histocompatilibty Complex class I (MHC-I) and then across the Golgi apparatus they finally get displayed on the surface of the cells. Note, in humans, MHC is also called Human Leukocyte Antigen (HLA) and represents the most diverse genes. Each of us have a total of 6 HLA-alleles, 3 from the maternal and 3 from the paternal side. These are further divided into 3 classes HLA-A, HLA-B and HLA-C and the combination of these constitute the HLA-haplotype for an individual. Once the peptide is bound to the MHC class I at the cell surface and exposed, the MHC-I peptide complex can be recognised by CD8+ Cytotoxic T-Lymphocytes (CTLs) via the T-cell Receptor (TCR). If a cell displays peptides of viral origin, the CTL gets activated and via a cascade induces apoptosis (programmed cell death) of the infected cell. The process is summarised in the figure below (McCarthy and Weinberg 2015).\n\n\n\n\n\nThe data we will be working with today contains data on sequenced T-cell receptors, viral antigens, HLA-haplotypes and clinical meta data for a cohort:\n\n“A large-scale database of T-cell receptor beta (TCR\\(\\beta\\)) sequences and binding associations from natural and synthetic exposure to SARS-CoV-2” (Nolan et al. 2020).",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#your-task-today",
    "href": "lab05.html#your-task-today",
    "title": "Lab 5: Data Wrangling II",
    "section": "Your Task Today",
    "text": "Your Task Today\nToday, we will emulate the situation, where you are working as a Bioinformatician / Bio Data Scientist and you have been given the data and the task of answering these two burning questions:\n\nWhat characterises the peptides binding to the HLAs?\nWhat characterises T-cell Receptors binding to the pMHC-complexes?\n\nGROUP ASSIGNMENT: Today, your assignment will be to create a micro-report on these 2 questions! (Important, see: how to)\nMAKE SURE TO READ THE LAST SECTION ON THE ASSIGNMENT",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#getting-started",
    "href": "lab05.html#getting-started",
    "title": "Lab 5: Data Wrangling II",
    "section": "Getting Started",
    "text": "Getting Started\nFirst, make sure to read and discuss the feedback you got from last week’s assignment!\n\nThen, once again go to the R for Bio Data Science RStudio Cloud Server\nMake sure you are in your r_for_bio_data_science project, you can verify this in the upper right corner\nIn the same place as your r_for_bio_data_science.Rproj file and existing data folder, create a new folder and name it doc\nGo to the aforementioned manuscript. Download the PDF and upload it to your new doc folder\nOpen the PDF and find the link to the data\nGo to the data site (Note, you may have to create and account to download, shouldn’t take too long) . Find and download the file ImmuneCODE-MIRA-Release002.1.zip (CAREFUL, do not download the superseded files)\nUnpack the downloaded file\nFind the files peptide-detail-ci.csv and subject-metadata.csv and compress to .zip files\nUpload the compressed peptide-detail-ci.csv.zip and subject-metadata.csv.zip files to your data folder in your RStudio Cloud session\nFinally, once again, create a new Quarto document for today’s exercises, containing the sections:\n\nBackground\nAim\nLoad Libraries\nLoad Data\nData Description\nAnalysis",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#creating-the-micro-report",
    "href": "lab05.html#creating-the-micro-report",
    "title": "Lab 5: Data Wrangling II",
    "section": "Creating the Micro-Report",
    "text": "Creating the Micro-Report\n\nBackground\nFeel free to copy paste the one stated in the background-section above\n\n\nAim\nState the aim of the micro-report, i.e. what are the questions you are addressing?\n\n\nLoad Libraries\nLoad the libraries needed\n\n\nLoad Data\nRead the two data sets into variables peptide_data and meta_data.\n\n\n\nClick here for hint\n\n\nThink about which Tidyverse package deals with reading data and what are the file types we want to read here?\n\n\n\nData Description\nIt is customary to include a description of the data, helping the reader if the report, i.e. your stakeholder, to get an easy overview\n\nThe Subject Meta Data\nLet’s take a look at the meta data:\n\nmeta_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 30\n   Experiment Subject `Cell Type` `Target Type` Cohort          Age Gender Race \n   &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;         &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;\n 1 eLH44         6501 PBMC        C19_cI        COVID-19-Con…    61 F      &lt;NA&gt; \n 2 eHO127        7143 PBMC        C19_cI        COVID-19-Con…    28 M      &lt;NA&gt; \n 3 eHO134         178 PBMC        C19_cI        COVID-19-Con…    36 M      White\n 4 eHO129         142 PBMC        C19_cI        COVID-19-Con…    66 F      Asian\n 5 eMR23      1566111 PBMC        C19_cI        COVID-19-Con…    22 F      &lt;NA&gt; \n 6 eEE224       19830 naive_CD8   C19_cI        Healthy (No …    24 M      White\n 7 eHO138        1369 PBMC        C19_cI        COVID-19-B-N…    NA &lt;NA&gt;   &lt;NA&gt; \n 8 eLH54         1326 PBMC        C19_cI        COVID-19-Con…    NA &lt;NA&gt;   &lt;NA&gt; \n 9 eAV100        1995 PBMC        C19_cII       COVID-19-Con…    29 F      &lt;NA&gt; \n10 eQD128        1499 PBMC        C19_cI        COVID-19-Con…    53 F      Asian\n# ℹ 22 more variables: `HLA-A...9` &lt;chr&gt;, `HLA-A...10` &lt;chr&gt;,\n#   `HLA-B...11` &lt;chr&gt;, `HLA-B...12` &lt;chr&gt;, `HLA-C...13` &lt;chr&gt;,\n#   `HLA-C...14` &lt;chr&gt;, DPA1...15 &lt;chr&gt;, DPA1...16 &lt;chr&gt;, DPB1...17 &lt;chr&gt;,\n#   DPB1...18 &lt;chr&gt;, DQA1...19 &lt;chr&gt;, DQA1...20 &lt;chr&gt;, DQB1...21 &lt;chr&gt;,\n#   DQB1...22 &lt;chr&gt;, DRB1...23 &lt;chr&gt;, DRB1...24 &lt;chr&gt;, DRB3...25 &lt;chr&gt;,\n#   DRB3...26 &lt;chr&gt;, DRB4...27 &lt;chr&gt;, DRB4...28 &lt;chr&gt;, DRB5...29 &lt;chr&gt;,\n#   DRB5...30 &lt;chr&gt;\n\n\n\nQ1: How many observations of how many variables are in the data?\nQ2: Are there groupings in the variables, i.e. do certain variables “go together” somehow?\nT1: Re-create this plot\n\nRead this first:\n\nThink about: What is on the x-axis? What is on the y-axis? And also, it looks like we need to do some counting stratified by Cohort and Gender. Recall, that we can stick together a dplyr pipeline with a call to ggplot.\n\n\n\n\n\n\n\n\n\n\nDoes your plot look different somehow? Consider peeking at the hint…\n\n\n\nClick here for hint\n\n\nPerhaps not everyone agrees on how to denote NAs in data. I have seen -99, -11, _ and so on… Perhaps this can be dealt with in the instance we read the data from the file? I.e. in the actual function call to your read_csv() function. Recall, how can we get information on the parameters of a ?function\n\n\nT2: Re-create this plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nPerhaps there is a function, which can cut continuous observations into a set of bins?\n\n\nSTOP! Make sure you handled how NAs are denoted in the data before proceeding, see hint below T1\n\nT3: Look at the data and create yet another plot as you see fit. Also skip the redundant variables Subject, Cell Type and Target Type\n\n\nmeta_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 27\n   Experiment Cohort      Age Gender Race  `HLA-A...9` `HLA-A...10` `HLA-B...11`\n   &lt;chr&gt;      &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;       &lt;chr&gt;        &lt;chr&gt;       \n 1 eEE240     Healthy …    23 M      White \"A*02:01\"   \"A*02:01\"    \"B*40:01\"   \n 2 eDH113     Healthy …    56 &lt;NA&gt;   &lt;NA&gt;  \"A*02:01\"   \"A*29:02\"    \"B*18:01\"   \n 3 eDH105     COVID-19…    32 F      &lt;NA&gt;  \"A*24:02:0… \"A*24:02:01\" \"B*40:01:02\"\n 4 eJL161     COVID-19…    31 F      White \"A*01:01:0… \"A*02:01:01\" \"B*08:01:01\"\n 5 eAV100     COVID-19…    29 F      &lt;NA&gt;  \"A*02:01:0… \"A*68:01:02\" \"B*07:02:01\"\n 6 eQD139     COVID-19…    NA &lt;NA&gt;   &lt;NA&gt;  \"A*01:01:0… \"A*29:02:01\" \"B*56:01:01\"\n 7 eHO131     COVID-19…    58 F      &lt;NA&gt;  \"A*02:01:0… \"A*02:01:01\" \"B*15:01:01\"\n 8 ePD85      Healthy …    27 F      &lt;NA&gt;  \"A*02:01\"   \"A*29:01\"    \"B*07:05\"   \n 9 eLH45      COVID-19…    53 M      &lt;NA&gt;  \"A*02:01:0… \"A*03:01:01\" \"B*07:02:01\"\n10 eHO136     COVID-19…    51 M      Hisp… \"\"          \"\"           \"\"          \n# ℹ 19 more variables: `HLA-B...12` &lt;chr&gt;, `HLA-C...13` &lt;chr&gt;,\n#   `HLA-C...14` &lt;chr&gt;, DPA1...15 &lt;chr&gt;, DPA1...16 &lt;chr&gt;, DPB1...17 &lt;chr&gt;,\n#   DPB1...18 &lt;chr&gt;, DQA1...19 &lt;chr&gt;, DQA1...20 &lt;chr&gt;, DQB1...21 &lt;chr&gt;,\n#   DQB1...22 &lt;chr&gt;, DRB1...23 &lt;chr&gt;, DRB1...24 &lt;chr&gt;, DRB3...25 &lt;chr&gt;,\n#   DRB3...26 &lt;chr&gt;, DRB4...27 &lt;chr&gt;, DRB4...28 &lt;chr&gt;, DRB5...29 &lt;chr&gt;,\n#   DRB5...30 &lt;chr&gt;\n\n\nNow, a classic way of describing a cohort, i.e. the group of subjects used for the study, is the so-called table1 and while we could build this ourselves, this one time, in the interest of exercise focus and time, we are going to “cheat” and use an R-package, like so:\nNB!: This may look a bit odd initially, but if you render your document, you should be all good!\n\nlibrary(\"table1\") # &lt;= Yes, this should normally go at the beginning!\nmeta_data |&gt;\n  mutate(Gender = factor(Gender),\n         Cohort = factor(Cohort)) |&gt;\n  table1(x = formula(~ Gender + Age + Race | Cohort),\n         data = _)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCOVID-19-Acute\n(N=4)\nCOVID-19-B-Non-Acute\n(N=8)\nCOVID-19-Convalescent\n(N=90)\nCOVID-19-Exposed\n(N=3)\nHealthy (No known exposure)\n(N=39)\nOverall\n(N=144)\n\n\n\n\nGender\n\n\n\n\n\n\n\n\nF\n1 (25.0%)\n4 (50.0%)\n33 (36.7%)\n1 (33.3%)\n17 (43.6%)\n56 (38.9%)\n\n\nM\n2 (50.0%)\n3 (37.5%)\n36 (40.0%)\n0 (0%)\n21 (53.8%)\n62 (43.1%)\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n21 (23.3%)\n2 (66.7%)\n1 (2.6%)\n26 (18.1%)\n\n\nAge\n\n\n\n\n\n\n\n\nMean (SD)\n50.7 (17.0)\n43.7 (7.74)\n51.5 (15.3)\n35.0 (NA)\n33.3 (9.93)\n44.9 (15.7)\n\n\nMedian [Min, Max]\n52.0 [33.0, 67.0]\n42.0 [33.0, 53.0]\n53.0 [21.0, 79.0]\n35.0 [35.0, 35.0]\n31.0 [21.0, 62.0]\n42.0 [21.0, 79.0]\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n21 (23.3%)\n2 (66.7%)\n0 (0%)\n25 (17.4%)\n\n\nRace\n\n\n\n\n\n\n\n\nAfrican American\n1 (25.0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (2.6%)\n2 (1.4%)\n\n\nWhite\n2 (50.0%)\n7 (87.5%)\n13 (14.4%)\n0 (0%)\n28 (71.8%)\n50 (34.7%)\n\n\nAsian\n0 (0%)\n0 (0%)\n3 (3.3%)\n0 (0%)\n2 (5.1%)\n5 (3.5%)\n\n\nHispanic or Latino/a\n0 (0%)\n0 (0%)\n1 (1.1%)\n0 (0%)\n0 (0%)\n1 (0.7%)\n\n\nNative Hawaiian or Other Pacific Islander\n0 (0%)\n0 (0%)\n0 (0%)\n1 (33.3%)\n0 (0%)\n1 (0.7%)\n\n\nBlack or African American\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n3 (7.7%)\n3 (2.1%)\n\n\nMixed Race\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (2.6%)\n1 (0.7%)\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n73 (81.1%)\n2 (66.7%)\n4 (10.3%)\n81 (56.3%)\n\n\n\n\n\n\n\nNote how good this looks! If you have ever done a “Table 1” before, you know how painful they can be and especially if something changes in your cohort - Dynamic reporting to the rescue!\nLastly, before we proceed, the meta_data contains HLA data for both class I and class II (see background), but here we are only interested in class I, recall these are denoted HLA-A, HLA-B and HLA-C, so make sure to remove any non-class I, i.e. the one after, denoted D-something.\n\nT4: Create a new version of the meta_data, which with respect to allele-data only contains information on class I and also fix the odd naming, e.g. HLA-A...9 becomes A1 oand HLA-A...10 becomes A2 and so on for B1, B2, C1 and C2 (Think: How can we rename variables? And here, just do it “manually” per variable). Remember to assign this new data to the same meta_data variable\n\n\n\n\nClick here for hint\n\n\nWhich tidyverse function subsets variables? Perhaps there is a function, which somehow matches a set of variables? And perhaps for the initiated this is compatible with regular expressions (If you don’t know what this means - No worries! If you do, see if you utilise this to simplify your variable selection)\n\nBefore we proceed, this is the data we will carry on with:\n\nmeta_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 11\n   Experiment Cohort        Age Gender Race  A1    A2    B1    B2    C1    C2   \n   &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 eXL43      Healthy (N…    36 F      White \"A*3… \"A*3… \"B*0… \"B*1… \"C*0… \"C*0…\n 2 eQD112     COVID-19-C…    65 M      &lt;NA&gt;  \"A*2… \"A*2… \"B*0… \"B*3… \"C*0… \"C*0…\n 3 eJL158     COVID-19-A…    33 M      White \"A*0… \"A*2… \"B*1… \"B*4… \"C*0… \"C*1…\n 4 ePD86      COVID-19-C…    58 M      White \"A*0… \"A*2… \"B*4… \"B*5… \"C*0… \"C*1…\n 5 eMR14      COVID-19-C…    NA &lt;NA&gt;   &lt;NA&gt;  \"A*0… \"A*2… \"B*0… \"B*5… \"C*0… \"C*0…\n 6 eAV100     COVID-19-C…    29 F      &lt;NA&gt;  \"A*0… \"A*6… \"B*0… \"B*4… \"C*0… \"C*0…\n 7 eHH174     Healthy (N…    31 F      White \"A*0… \"A*0… \"B*0… \"B*5… \"C*0… \"C*1…\n 8 eNL192     COVID-19-C…    NA &lt;NA&gt;   &lt;NA&gt;  \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 9 eLH54      COVID-19-C…    NA &lt;NA&gt;   &lt;NA&gt;  \"A*0… \"A*0… \"B*0… \"B*4… \"C*0… \"C*0…\n10 eJL147     Healthy (N…    40 M      Mixe… \"A*0… \"A*1… \"B*0… \"B*3… \"C*0… \"C*0…\n\n\nNow, we have a beautiful tidy dataset, recall that this entails, that each row is an observation, each column is a variable and each cell holds one value.\n\n\n\nThe Peptide Details Data\nLet’s start with simply having a look see:\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 7\n   `TCR BioIdentity`            TCR Nucleotide Seque…¹ Experiment `ORF Coverage`\n   &lt;chr&gt;                        &lt;chr&gt;                  &lt;chr&gt;      &lt;chr&gt;         \n 1 CASSDRLAGGNTGELFF+TCRBV02-0… ACAAAGCTGGAGGACTCAGCC… eQD131     ORF3a         \n 2 CASSHPGPYNEQFF+TCRBV04-02+T… CTACACACCCTGCAGCCAGAA… eOX43      ORF1ab        \n 3 CASSLSYVAGGSFSEQYF+TCRBV27-… AGCCCCAACCAGACCTCTCTG… eXL27      membrane glyc…\n 4 CASRMLIM**HSYNEQFF+TCRBV19-… CAAAAGAACCCGACAGCTTTC… eQD114     ORF1ab        \n 5 CASSWSNSGGAFNNEQFF+TCRBV07-… GAGCAGGGGGACTCGGCCATG… eQD123     ORF1ab        \n 6 CASAQVGRINEKLFF+TCRBV11-03+… CAGCCTGCAGAGCTTGGGGAC… eXL27      ORF7b         \n 7 CASSEPTSAWGDTQYF+TCRBV06-01… TCGGCTGCTCCCTCCCAGACA… eAV88      membrane glyc…\n 8 CASRKGQEGLNSPLHF+TCRBV12-X+… CCCTCAGAACCCAGGGACTCA… eXL31      surface glyco…\n 9 CASSLGTGKYNEQFF+TCRBV07-09+… CAGCGCACAGAGCAGGGGGAC… eEE228     surface glyco…\n10 CASSLVGSDGETQYF+TCRBV07-02+… CAGCGCACAGAGCAGGAGGAC… eXL30      ORF1ab        \n# ℹ abbreviated name: ¹​`TCR Nucleotide Sequence`\n# ℹ 3 more variables: `Amino Acids` &lt;chr&gt;, `Start Index in Genome` &lt;dbl&gt;,\n#   `End Index in Genome` &lt;dbl&gt;\n\n\n\nQ3: How many observations of how many variables are in the data?\n\nThis is a rather big data set, so let us start with two “tricks” to handle this, first:\n\nWrite the data back into your data folder, using the filename peptide-detail-ci.csv.gz, note the appending of .gz, which is automatically recognised and results in gz-compression\nNow, check in your data folder, that you have two files peptide-detail-ci.csv and peptide-detail-ci.csv.gz, delete the former\nAdjust your reading-the-data-code in the “Load Data”-section, to now read in the peptide-detail-ci.csv.gz file\n\n\n\n\nClick here for hint\n\n\nJust as you can read a file, you can of course also write a file. Note the filetype we want to write here is csv. If you in the console type e.g. readr::wr and then hit the Tab key, you will see the different functions for writing different filetypes\n\nThen:\n\nT5: As before, let’s immediately subset the peptide_data to the variables of interest: TCR BioIdentity, Experiment and Amino Acids. Remember to assign this new data to the same peptide_data variable to avoid cluttering your environment with redundant variables. Bonus: Did you know you can click the Environment pane and see which variables you have?\n\nOnce again, before we proceed, this is the data we will carry on with:\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 3\n   Experiment `TCR BioIdentity`                       `Amino Acids`             \n   &lt;chr&gt;      &lt;chr&gt;                                   &lt;chr&gt;                     \n 1 eEE226     CASSRSIGAQPQHF+TCRBV04-01+TCRBJ01-05    RQLLFVVEV                 \n 2 eEE228     CASSLNPYEQYF+TCRBV27-01+TCRBJ02-07      FVCNLLLLFV,LLFVTVYSHL,TVY…\n 3 eOX54      CSARYANTEAFF+TCRBV20-X+TCRBJ01-01       AFPFTIYSL,GYINVFAFPF,INVF…\n 4 eOX54      CASSFLLGQGTEQYF+TCRBV12-X+TCRBJ02-07    FLNGSCGSV                 \n 5 eOX52      CASSPVTSGRYEQYF+TCRBV18-01+TCRBJ02-07   TLVPQEHYV                 \n 6 eXL30      CASSYSAGGSYEQYF+TCRBV06-05+TCRBJ02-07   AFLLFLVLI,FLAFLLFLV,FYLCF…\n 7 eEE226     CASSLLAGGPYQETQYF+TCRBV05-06+TCRBJ02-05 FLNGSCGSV                 \n 8 eHH173     CASSVGQAGNQPQHF+TCRBV13-01+TCRBJ01-05   MPASWVMRI                 \n 9 eGK111     CAIRESLGLASTDTQYF+TCRBV10-03+TCRBJ02-03 NSSPDDQIGY,NTNSSPDDQIGYY,…\n10 eXL31      CSVSREGFRQFF+TCRBV20-X+TCRBJ02-01       AFLLFLVLI,FLAFLLFLV,FYLCF…\n\n\n\nQ4: Is this tidy data? Why/why not?\nT6: See if you can find a way to create the below data, from the above\n\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 5\n   Experiment CDR3b             V_gene           J_gene     `Amino Acids`       \n   &lt;chr&gt;      &lt;chr&gt;             &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;               \n 1 eHO141     CASSRGGSYSNTGELFF TCRBV05-04       TCRBJ02-02 AYKTFPPTEPK,KTFPPTE…\n 2 eEE240     CASSQAGVTGDHGYTF  TCRBV04-01       TCRBJ01-02 GMEVTPSGTWL,MEVTPSG…\n 3 eXL31      CSTEDEKLFF        TCRBV20-01       TCRBJ01-04 AFLLFLVLI,FLAFLLFLV…\n 4 eEE226     CASSPDTQYF        TCRBV12-03/12-04 TCRBJ02-03 AFPFTIYSL,GYINVFAFP…\n 5 eEE228     CASSILAYNEQFF     TCRBV19-01       TCRBJ02-01 FLWLLWPVT,FLWLLWPVT…\n 6 eEE228     CASQAGAAQTQYF     TCRBV02-01       TCRBJ02-05 IMLIIFWFSL,MLIIFWFSL\n 7 eOX46      CASSPLGLLMNTEAFF  TCRBV07-08       TCRBJ01-01 KLSYGIATV           \n 8 eXL27      CATVGGSSLTGNTIYF  TCRBV06-X        TCRBJ01-03 AFLLFLVLI,FLAFLLFLV…\n 9 eOX43      CASSFPGQSGEKLFF   TCRBV02-01       TCRBJ01-04 DFLEYHDVR,EDFLEYHDV…\n10 eQD111     CASSFLPSAGELFF    TCRBV27-01       TCRBJ02-02 HTTDPSFLGRY         \n\n\n\n\n\nClick here for hint\n\n\nFirst: Compare the two datasets and identify what happened? Did any variables “disappear” and did any “appear”? Ok, so this is a bit tricky, but perhaps there is a function to separate a composite (untidy) column into a set of new variables based on a separator? But what is a separator? Just like when you read a file with Comma Separated Values, a separator denotes how a composite string is divided into fields. So, look for such a repeated value, which seem to indeed separate such fields. Also, be aware, that character, which can mean more than one thing, may need to be “escaped” using an initial two backslashed, i.e. “\\x”, where x denotes the character needing to be “escaped”\n\n\nT7: Add a variable, which counts how many peptides are in each observation of Amino Acids\n\n\n\n\nClick here for hint\n\n\nWe have been working with the stringr package, perhaps the contains a function to somehow count the number of occurrences of a given character in a string? Again, remember you can type e.g. stringr::str_ and then hit the Tab key to see relevant functions\n\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 6\n   Experiment CDR3b            V_gene     J_gene     `Amino Acids`    n_peptides\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;                 &lt;dbl&gt;\n 1 eHO140     CASSDRSGTDTQYF   TCRBV27-01 TCRBJ02-03 HTTDPSFLGRY               1\n 2 eOX49      CASKAAGDSYNEQFF  TCRBV19-01 TCRBJ02-01 GRLQSLQTY,LITGR…          3\n 3 eEE224     CASSFAGPDSPLHF   TCRBV12-X  TCRBJ01-06 FLWLLWPVT,FLWLL…          7\n 4 eEE240     CATSDLDSYEQYF    TCRBV24-01 TCRBJ02-07 FLQSINFVR,FLQSI…         13\n 5 ePD83      CASSRGQGISYEQYF  TCRBV19-01 TCRBJ02-07 SEHDYQIGGYTEKW,…          3\n 6 eQD111     CASSRDSVSYNEQFF  TCRBV27-01 TCRBJ02-01 HTTDPSFLGRY               1\n 7 eLH48      CASSLGPGQPNTEAFF TCRBV07-09 TCRBJ01-01 HPLADNKFAL,SPFH…          2\n 8 eEE228     CASKADGYEQYF     TCRBV19-01 TCRBJ02-07 GNYTVSCLPF,NYTV…          3\n 9 eEE224     CASTTPHGSYTF     TCRBV28-01 TCRBJ01-02 FLWLLWPVT,FLWLL…          7\n10 eXL27      CASSFGTYNEQFF    TCRBV11-03 TCRBJ02-01 MVMCGGSLYV,VMCG…          2\n\n\n\nT8: Re-create the following plot\n\n\n\n\n\n\n\n\n\n\n\nQ4: What is the maximum number of peptides assigned to one observation?\nT9: Using the str_c() and the seq() functions, re-create the below\n\n\n\n[1] \"peptide_1\" \"peptide_2\" \"peptide_3\" \"peptide_4\" \"peptide_5\"\n\n\n\n\n\nClick here for hint\n\n\nIf you’re uncertain on how a function works, try going into the console and in this case e.g. type str_c(\"a\", \"b\") and seq(from = 1, to = 3) and see if you combine these?\n\n\nT10: Use, what you learned about separating in T6 and the vector-of-strings you created in T9 adjusted to the number from Q4 to create the below data\n\n\n\n\nClick here for hint\n\n\nIn the console, write ?separate and think about how you used it earlier. Perhaps you can not only specify a vector to separate into, but also specify a function, which returns a vector?\n\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 18\n   Experiment CDR3b        V_gene J_gene peptide_1 peptide_2 peptide_3 peptide_4\n   &lt;chr&gt;      &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;     &lt;chr&gt;    \n 1 eOX52      CATRPLLAGGR… TCRBV… TCRBJ… SELVIGAVI SELVIGAV… &lt;NA&gt;      &lt;NA&gt;     \n 2 eXL27      CASSVGWGNEQ… TCRBV… TCRBJ… DGVYFAST… GVYFASTEK LPFNDGVYF LPFNDGVY…\n 3 eOX49      CATSGLAGYLG… TCRBV… TCRBJ… AFLLFLVLI FLAFLLFLV FYLCFLAFL FYLCFLAF…\n 4 ePD83      CASSMGVGSSY… TCRBV… TCRBJ… SEHDYQIG… YQIGGYTEK YQIGGYTE… &lt;NA&gt;     \n 5 eEE243     CAIRGLGQYNE… TCRBV… TCRBJ… FVDGVPFVV &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;     \n 6 eLH47      CASSLAGIELT… TCRBV… TCRBJ… LSPRWYFYY SPRWYFYYL &lt;NA&gt;      &lt;NA&gt;     \n 7 ePD83      CASSGAGELFF  TCRBV… TCRBJ… SEHDYQIG… YQIGGYTEK YQIGGYTE… &lt;NA&gt;     \n 8 eXL30      CASSDRDGAYE… TCRBV… TCRBJ… DTDFVNEF… NRDVDTDF… &lt;NA&gt;      &lt;NA&gt;     \n 9 eQD111     CASSYMGGSSF… TCRBV… TCRBJ… HTTDPSFL… &lt;NA&gt;      &lt;NA&gt;      &lt;NA&gt;     \n10 eQD134     CASSLRPPSQE… TCRBV… TCRBJ… SYFTSDYY… VLHSYFTS… YFTSDYYQ… &lt;NA&gt;     \n# ℹ 10 more variables: peptide_5 &lt;chr&gt;, peptide_6 &lt;chr&gt;, peptide_7 &lt;chr&gt;,\n#   peptide_8 &lt;chr&gt;, peptide_9 &lt;chr&gt;, peptide_10 &lt;chr&gt;, peptide_11 &lt;chr&gt;,\n#   peptide_12 &lt;chr&gt;, peptide_13 &lt;chr&gt;, n_peptides &lt;dbl&gt;\n\n\n\nQ5: Now, presumable you got a warning, discuss in your group why that is?\nQ6: With respect to peptide_n, discuss in your group, if this is wide- or long-data?\n\nNow, finally we will use the what we prepared for today, data pivoting. There are two functions, namely pivot_wider() and pivot_longer(). Also, now, we will use a trick when developing ones data pipeline, while working with new functions, that on might not be completely comfortable with. You have seen the slice_sample() function several times above and we can use that to randomly sample n observations from data. This we can utilise to work with a smaller data set in the development face and once we are ready, we can increase this n gradually to see if everything continues to work as anticipated.\n\nT11: Using the peptide_data, run a few slice_sample() calls with varying degree of n to make sure, that you get a feeling for what is going on\nT12: From the peptide_data data above, with peptide_1, peptide_2, etc. create this data set using one of the data pivoting functions. Remember to start initially with sampling a smaller data set and then work on that first! Also, once you’re sure you’re good to go, reuse the peptide_data variable as we don’t want huge redundant data sets floating around in our environment\n\n\n\n\nClick here for hint\n\n\nIf the pivoting is not clear at all, then do what I do, create some example data:\n\nmy_data &lt;- tibble(\n  id = str_c(\"id_\", 1:10),\n  var_1 = round(rnorm(10),1),\n  var_2 = round(rnorm(10),1),\n  var_3 = round(rnorm(10),1))\n\n…and then play around with that. A small set like the one above is easy to handle, so perhaps start with that and then pivot back and forth a few times using pivot_wider()/pivot_longer(). Use View() to inspect and get a better overview of the results of pivoting.\n\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 7\n   Experiment CDR3b            V_gene        J_gene n_peptides peptide_n peptide\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;         &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  \n 1 eGK120     CASSQEFGIGDEQYF  TCRBV04-03    TCRBJ…          3 peptide_9 &lt;NA&gt;   \n 2 eHH175     CSARGYLRDEQFF    TCRBV20-X     TCRBJ…          1 peptide_7 &lt;NA&gt;   \n 3 eXL30      CASSFQGTEAFF     TCRBV27-01    TCRBJ…          2 peptide_2 KEIIFL…\n 4 eQD113     CASSDRTSGRKVTQYF TCRBV09-01    TCRBJ…          2 peptide_… &lt;NA&gt;   \n 5 eLH47      CASSEQGRYEQYF    TCRBV07-06    TCRBJ…          1 peptide_… &lt;NA&gt;   \n 6 eOX52      CASNSGATQYF      TCRBV12-03/1… TCRBJ…          5 peptide_4 QTGKIA…\n 7 eOX46      CASSLEPNSYNEQFF  TCRBV11-02    TCRBJ…         11 peptide_9 MIELSL…\n 8 eEE228     CASSLASNPLLDEQFF TCRBV07-08    TCRBJ…          6 peptide_1 FPNITN…\n 9 eOX52      CASFEVMNTEAFF    TCRBV09-01    TCRBJ…          1 peptide_4 &lt;NA&gt;   \n10 eOX52      CASSYGTSGEQFF    TCRBV06-05    TCRBJ…          5 peptide_6 &lt;NA&gt;   \n\n\n\nQ7: You will see some NAs in the peptide variable, discuss in your group from where these arise?\nQ8: How many rows and columns now and how does this compare with Q3? Discuss why/why not it is different?\nT13: Now, lose the redundant variables n_peptides and peptide_n, get rid of the NAs in the peptide column, and make sure that we only have unique observations (i.e. there are no repeated rows/observations).\n\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 5\n   Experiment CDR3b              V_gene     J_gene     peptide    \n   &lt;chr&gt;      &lt;chr&gt;              &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;      \n 1 eAV93      CASSLGEWYEQYF      TCRBV27-01 TCRBJ02-07 LEPLVDLPI  \n 2 eEE228     CASSLLGTAAGQTYEQYF TCRBV27-01 TCRBJ02-07 IELSLIDFYL \n 3 eEE226     CASSLRADYEQYF      TCRBV27-01 TCRBJ02-07 ILLIIMRTFK \n 4 eAV93      CASSPDTGSPYEQYF    TCRBV18-01 TCRBJ02-07 RFPNITNLCPF\n 5 eMR25      CASSSPRTSGGLTDTQYF TCRBV28-01 TCRBJ02-03 WLLWPVTLA  \n 6 eQD137     CASSLRTDSPLHF      TCRBV28-01 TCRBJ01-06 GLEAPFLYLY \n 7 eHO134     CASSIGGFEQFF       TCRBV19-01 TCRBJ02-01 LQSINFVRII \n 8 eXL37      CASKGLETQYF        TCRBV06-06 TCRBJ02-05 GVVFLHVTY  \n 9 eEE240     CSAQDRVDYGYTF      TCRBV20-X  TCRBJ01-02 YLCFLAFLL  \n10 eOX54      CSASEELETQYF       TCRBV20-01 TCRBJ02-05 FLAFLLFLV  \n\n\n\nQ8: Now how many rows and columns and is this data tidy? Discuss in your group why/why not?\n\nAgain, we turn to the stringr package, as we need to make sure that the sequence data does indeed only contain valid characters. There are a total of 20 proteogenic amino acids, which we symbolise using ARNDCQEGHILKMFPSTWYV.\n\nT14: Use the str_detect() function to filter the CDR3b and peptide variables using a pattern of [^ARNDCQEGHILKMFPSTWYV] and then play with the negate parameter so see what happens\n\n\n\n\nClick here for hint\n\n\nAgain, try to play a bit around with the function in the console, type e.g. str_detect(string = \"ARND\", pattern = \"A\") and str_detect(string = \"ARND\", pattern = \"C\") and then recall, that the filter() function requires a logical vector, i.e. a vector of TRUE and FALSE to filter the rows\n\n\nT15: Add two new variables to the data, k_CDR3b and k_peptide each signifying the length of the respective sequences\n\n\n\n\nClick here for hint\n\n\nAgain, we’re working with strings, so perhaps there is a package of interest and perhaps in that package, there is a function, which can get the length of a string?\n\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 7\n   Experiment CDR3b                 V_gene      J_gene peptide k_CDR3b k_peptide\n   &lt;chr&gt;      &lt;chr&gt;                 &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;     &lt;int&gt;\n 1 eOX52      CSAREGTSGTYEQYF       TCRBV20-X   TCRBJ… GEIPVA…      15        12\n 2 eOX52      CASSQDRTLLDRGLGSYEQYF TCRBV04-03  TCRBJ… QELYSP…      21         9\n 3 eEE240     CASTAGGETQYF          TCRBV27-01  TCRBJ… SLIDFY…      12        10\n 4 eEE240     CASGSGLAVERSTDTQYF    TCRBV09-01  TCRBJ… FYLCFL…      18         9\n 5 eEE224     CASSSLSVLAKNIQYF      TCRBV06-X   TCRBJ… LEYHDV…      16         9\n 6 eLH51      CASSAYRDTQYF          TCRBV05-05  TCRBJ… NRDVDT…      12        13\n 7 eXL31      CASRLGTVAYEQYF        TCRBV28-01  TCRBJ… MIELSL…      14        10\n 8 eAV93      CASSYRTRGSNQPQHF      TCRBV28-01  TCRBJ… LWLLWP…      16         9\n 9 eLH47      CASSFGGGWQYF          TCRBV12-03… TCRBJ… SLIDFY…      12        10\n10 eQD132     CSAFLLTDFTDTQYF       TCRBV20-X   TCRBJ… YEQYIK…      15         9\n\n\n\nT16: Re-create this plot\n\n\n\n\n\n\n\n\n\n\n\nQ9: What is the most predominant length of the CDR3b-sequences?\nT17: Re-create this plot\n\n\n\n\n\n\n\n\n\n\n\nQ10: What is the most predominant length of the peptide-sequences?\nQ11: Discuss in your group, if this data set is tidy or not?\n\n\npeptide_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 7\n   Experiment CDR3b                V_gene       J_gene peptide k_CDR3b k_peptide\n   &lt;chr&gt;      &lt;chr&gt;                &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;     &lt;int&gt;\n 1 eAV91      CASSFSNEKLFF         TCRBV07-09   TCRBJ… LNDLCF…      12        10\n 2 eXL30      CASSPQMSETQYF        TCRBV04-01   TCRBJ… AELAKN…      13        13\n 3 eEE240     CASSIRDGVDEQYF       TCRBV06-05   TCRBJ… GLEAPF…      14        10\n 4 eEE226     CASSAAGDEQFF         TCRBV05-01   TCRBJ… SLIDFY…      12        10\n 5 eOX46      CASSNSGTGRTEAFF      TCRBV19-01   TCRBJ… FVCNLL…      15        10\n 6 eEE228     CASSQDSKGLAGFSSYEQYF TCRBV03-01/… TCRBJ… NPANNA…      20         9\n 7 eHO124     CASSLSANYGYTF        TCRBV27-01   TCRBJ… QLMCQP…      13        10\n 8 eEE224     CSASSATTAGDEQFF      TCRBV20-01   TCRBJ… YLCFLA…      15         9\n 9 eMR14      CASSSSYRGNQPQHF      TCRBV06-05   TCRBJ… LSPRWY…      15         9\n10 eOX43      CASTFPGDIYNEQFF      TCRBV06-05   TCRBJ… FYLCFL…      15        10\n\n\n\n\nCreating one data set from two data sets\nBefore we move onto using the family of *_join() functions you prepared for today, we will just take a quick peek at the meta data again:\n\nmeta_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 11\n   Experiment Cohort        Age Gender Race  A1    A2    B1    B2    C1    C2   \n   &lt;chr&gt;      &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;\n 1 eNL192     COVID-19-C…    NA &lt;NA&gt;   &lt;NA&gt;  \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 2 eMR25      COVID-19-C…    21 F      &lt;NA&gt;  \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 3 eXL43      Healthy (N…    36 F      White \"A*3… \"A*3… \"B*0… \"B*1… \"C*0… \"C*0…\n 4 ePD91      COVID-19-C…    52 M      White \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 5 eMR20      COVID-19-B…    37 M      White \"A*0… \"A*2… \"B*1… \"B*1… \"C*0… \"C*0…\n 6 eXL27      Healthy (N…    24 M      White \"A*0… \"A*0… \"B*2… \"B*4… \"C*0… \"C*0…\n 7 eQD125     COVID-19-C…    44 M      &lt;NA&gt;  \"A*0… \"A*1… \"B*1… \"B*5… \"C*0… \"C*0…\n 8 eXL31      Healthy (N…    28 M      White \"A*0… \"A*2… \"B*0… \"B*4… \"C*0… \"C*1…\n 9 eQD114     COVID-19-C…    73 M      &lt;NA&gt;  \"A*0… \"A*2… \"B*0… \"B*4… \"C*0… \"C*1…\n10 eLH50      COVID-19-C…    28 M      &lt;NA&gt;  \"A*0… \"A*2… \"B*1… \"B*2… \"C*0… \"C*0…\n\n\nRemember you can scroll in the data.\n\nQ12: Discuss in your group, if this data with respect to the A1, A2, B1, B2, C1 and C2 variables is a wide or a long data format?\n\nAs with the peptide_data, we will now have to use data pivoting again. I.e.:\n\nT18: use either pivot_wider() or pivot_longer() to create the following data:\n\n\nmeta_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 7\n   Experiment Cohort                        Age Gender Race         Gene  Allele\n   &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;        &lt;chr&gt; &lt;chr&gt; \n 1 eLH58      COVID-19-Convalescent          NA &lt;NA&gt;   &lt;NA&gt;         A2    \"A*02…\n 2 eHH170     Healthy (No known exposure)    24 F      Black or Af… A2    \"A*74…\n 3 eHO125     COVID-19-Convalescent          52 M      &lt;NA&gt;         C2    \"C*07…\n 4 eHO129     COVID-19-Convalescent          66 F      Asian        B1    \"B*15…\n 5 eJL152     COVID-19-Convalescent          41 F      &lt;NA&gt;         C2    \"C*14…\n 6 ePD91      COVID-19-Convalescent          52 M      White        A2    \"\"    \n 7 eGK111     COVID-19-Convalescent          50 F      &lt;NA&gt;         C2    \"C*07…\n 8 eMR20      COVID-19-B-Non-Acute           37 M      White        B1    \"B*14…\n 9 eEE224     Healthy (No known exposure)    24 M      White        B2    \"B*40…\n10 eMR13      COVID-19-Convalescent          NA &lt;NA&gt;   &lt;NA&gt;         A2    \"A*24…\n\n\nRemember, what we are aiming for here, is to create one data set from two. So:\n\nQ13: Discuss in your group, which variable(s?) define the same observations between the peptide_data and the meta_data?\n\nOnce you have agreed upon Experiment, then use that knowledge to subset the meta_data to the variables-of-interest:\n\nmeta_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 2\n   Experiment Allele      \n   &lt;chr&gt;      &lt;chr&gt;       \n 1 eJL147     \"A*11:01\"   \n 2 eMR16      \"C*06:02:01\"\n 3 eLH50      \"B*27:05:02\"\n 4 eQD121     \"A*24:02:01\"\n 5 eHO131     \"B*15:01:01\"\n 6 eHO130     \"C*07:01\"   \n 7 eJL154     \"A*02:01:01\"\n 8 eLH42      \"A*02:01:01\"\n 9 eOX52      \"C*04:82\"   \n10 eHO136     \"\"          \n\n\nUse the View() function again, to look at the meta_data. Notice something? Some alleles are e.g. A*11:01, whereas others are B*51:01:02. You can find information on why, by visiting Nomenclature for Factors of the HLA System.\nLong story short, we only want to include Field 1 (allele group) and Field 2 (Specific HLA protein). You have prepared the stringr package for today. See if you can find a way to reduce e.g. B*51:01:02 to B*51:01 and then create a new variable Allele_F_1_2 accordingly, while also removing the ...x (where x is a number) subscripts from the Gene variable (It is an artifact from having the data in a wide format, where you cannot have two variables with the same name) and also, remove any NAs and \"\"s, denoting empty entries.\n\n\n\nClick here for hint\n\n\nThere are several ways this can be achieved, the easiest being to consider if perhaps a part of the string based on indices could be of interest. This term “a part of a string” is called a substring, perhaps the stringr package contains a function work with substring? In the console, type stringr:: and hit tab. This will display the functions available in the stringr package. Scroll down and find the functionst starting with str_ and look for on, which might be relevant and remember you can use ?function_name to get more information on how a given function works.\n\n\nT19: Create the following data, according to specifications above:\n\n\nmeta_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 3\n   Experiment Allele     Allele_F_1_2\n   &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;       \n 1 eLH54      A*02:01:01 A*02:01     \n 2 eQD131     A*32:01:01 A*32:01     \n 3 eQD115     A*02:01:01 A*02:01     \n 4 eGK111     A*01:01:01 A*01:01     \n 5 eOX43      B*27:05    B*27:05     \n 6 eAM23      A*24:02:01 A*24:02     \n 7 eOX49      A*26:01    A*26:01     \n 8 eHO126     B*57:01:01 B*57:01     \n 9 eJL152     B*15:35    B*15:35     \n10 eQD121     C*07:01:01 C*07:01     \n\n\nThe asterisk, i.e. * is a rather annoying character because of ambiguity, so:\n\nT20: Clean the data a bit more, by removing the asterisk and redundant variables:\n\n\nmeta_data |&gt; \n  slice_sample(n = 10)\n\n# A tibble: 10 × 2\n   Experiment Allele\n   &lt;chr&gt;      &lt;chr&gt; \n 1 eMR20      C07:02\n 2 eJL158     A02:01\n 3 eOX43      C03:04\n 4 eMR12      C03:04\n 5 ePD85      C07:04\n 6 eQD116     C04:01\n 7 ePD81      C15:02\n 8 eXL31      C07:02\n 9 eQD112     A24:02\n10 eLH51      A24:07\n\n\n\n\n\nClick here for hint 1\n\n\nAgain, the stringr package may come in handy. Perhaps there is a function remove, one or more such pesky characters?\n\n\n\n\nClick here for hint 2\n\n\nGetting a weird error? Recall, that character ambiguity needs to be “escaped”, you did this somehow earlier on…\n\nRecall the peptide_data?\n\npeptide_data |&gt;\n  slice_sample(n = 10)\n\n# A tibble: 10 × 7\n   Experiment CDR3b               V_gene     J_gene    peptide k_CDR3b k_peptide\n   &lt;chr&gt;      &lt;chr&gt;               &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt;     &lt;int&gt;     &lt;int&gt;\n 1 eAV88      CATSRDGRSYEQYF      TCRBV15-01 TCRBJ02-… FVDGVP…      14         9\n 2 eOX43      CASSSGLAGIEQYF      TCRBV07-08 TCRBJ02-… NPLLYD…      14         9\n 3 eMR16      CASSLTGWENTEAFF     TCRBV07-03 TCRBJ01-… LQSINF…      15        10\n 4 eOX54      CASSQDPGGSETQYF     TCRBV04-01 TCRBJ02-… WMESEF…      15         9\n 5 eEE240     CSTRGMSREGSSYEQYF   TCRBV20-X  TCRBJ02-… LIDFYL…      17         9\n 6 ePD83      CASSFLGTNTGELFF     TCRBV05-06 TCRBJ02-… SEHDYQ…      15        14\n 7 eQD116     CASSQMTGLFAFF       TCRBV04-02 TCRBJ01-… KTFPPT…      13         9\n 8 eEE228     CASSLDGSSYNEQFF     TCRBV11-02 TCRBJ02-… AFLLFL…      15         9\n 9 eEE240     CSAELEISGELFF       TCRBV20-X  TCRBJ02-… IDFYLC…      13        10\n10 eEE226     CASSPNDDTIIATNEKLFF TCRBV07-09 TCRBJ01-… IELSLI…      19        10\n\n\n\nT21: Create a dplyr pipeline, starting with the peptide_data, which joins it with the meta_data and remember to make sure that you get only unqiue observations of rows. Save this data into a new variable names peptide_meta_data (If you get a warning, discuss in your group what it means?)\n\n\n\n\nClick here for hint 1\n\n\nWhich family of functions do we use to join data? Also, perhaps here it would be prudent to start with working on a smaller data set, recall we could sample a number of rows yielding a smaller development data set\n\n\n\n\nClick here for hint 2\n\n\nYou should get a data set of around +3.000.000, take a moment to consider how that would have been to work with in Excel? Also, in case the servers are not liking this, you can consider subsetting the peptide_data prior to joining to e.g. 100,000 or 10,000 rows.\n\n\npeptide_meta_data |&gt;\n  slice_sample(n = 10)\n\n# A tibble: 10 × 8\n   Experiment CDR3b            V_gene    J_gene peptide k_CDR3b k_peptide Allele\n   &lt;chr&gt;      &lt;chr&gt;            &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt;     &lt;int&gt;     &lt;int&gt; &lt;chr&gt; \n 1 eHH175     CASSYSSSRGSTEAFF TCRBV06-… TCRBJ… YLCFLA…      16         9 C16:01\n 2 eOX43      CASSWTGYEQYF     TCRBV27-… TCRBJ… AEVQID…      12         9 C07:04\n 3 eXL31      CSAEGGGTQYF      TCRBV20-… TCRBJ… FYLCFL…      11         9 A02:01\n 4 eAM23      CASSFGTGELFF     TCRBV12-… TCRBJ… VQELYS…      12        10 A11:01\n 5 eXL27      CASSSQDREQPQHF   TCRBV06-… TCRBJ… LLFLVL…      14         9 C07:04\n 6 eXL30      CASSFWVADTQYF    TCRBV07-… TCRBJ… SLIDFY…      13        10 B39:01\n 7 eOX46      CASSPLAGLSYEQYF  TCRBV05-… TCRBJ… FYLCFL…      15        10 B44:02\n 8 eOX54      CASSPPADYSGNTIYF TCRBV07-… TCRBJ… SLIDFY…      16        10 C02:10\n 9 eOX46      CSASLEQQSSYNEQFF TCRBV20-X TCRBJ… FYLCFL…      16         9 C04:01\n10 eEE240     CSADQDGGSYEQYF   TCRBV20-X TCRBJ… YLCFLA…      14         9 B57:01\n\n\n\n\n\nAnalysis\nNow, that we have the data in a prepared and ready-to-analyse format, let us return to the two burning questions we had:\n\nWhat characterises the peptides binding to the HLAs?\nWhat characterises T-cell Receptors binding to the pMHC-complexes?\n\n\nPeptides binding to HLA\nAs we have touched upon multiple times, R is very flexible and naturally you can also create sequence logos. Finally, let us create a binding motif using the package ggseqlogo (More info here).\n\nT22: Subset the final peptide_meta_data data to A02:01 and unique observations of peptides of length 9 and re-create the below sequence logo\n\n\n\n\nClick here for hint\n\n\nYou can pipe a vector of peptides into ggseqlogo, but perhaps you first need to pull that vector from the relevant variable in your tibble? Also, consider before that, that you’ll need to make sure, you are only looking at peptides of length 9\n\n\n\n\n\n\n\n\n\n\n\nT23: Repeat for e.g. B07:02 or another of your favourite alleles\n\nNow, let’s take a closer look at the sequence logo:\n\nQ14: Which positions in the peptide determines binding to HLA?\n\n\n\n\nClick here for hint\n\n\nRecall your Introduction to Bioinformatics course? And/or perhaps ask your fellow group members if they know?\n\n\n\nCDR3b-sequences binding to pMHC\n\nT24: Subset the peptide_meta_data, such that the length of the CDR3b is 15, the allele is A02:01 and the peptide is LLFLVLIML and re-create the below sequence logo of the CDR3b sequences:\n\n\n\n\n\n\n\n\n\n\n\nQ15: In your group, discuss what you see?\nT25: Play around with other combinations of k_CDR3b, Allele, and peptide and inspect how the logo changes\n\nDisclaimer: In this data set, we only get: A given CDR3b was found to recognise a given peptide in a given subject and that subject had a given haplotype - Something’s missing… Perhaps if you have had immunology, then you can spot it? There is a trick to get around this missing information, but that’s beyond scope of what we’re working with here.",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#epilogue",
    "href": "lab05.html#epilogue",
    "title": "Lab 5: Data Wrangling II",
    "section": "Epilogue",
    "text": "Epilogue\nThat’s it for today - I know this is overwhelming now, but commit to it and you WILL be plenty rewarded! I hope today was at least a glimpse into the flexibility and capabilities of using tidyverse for applied Bio Data Science\n…also, noticed something? We spend maybe 80% of the time here on dealing with data-wrangling and then once we’re good to go, the analysis wasn’t that time consuming - That’s often the way it ends up going. You’ll spend a lot of time on data handling, and getting the tidyverse toolbox in your tool belt will allow you to be so much more efficient in your data wrangling, so you can get to the fun part as quickly as possible!",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab05.html#sec-assignment",
    "href": "lab05.html#sec-assignment",
    "title": "Lab 5: Data Wrangling II",
    "section": "Today’s Assignment",
    "text": "Today’s Assignment\nAfter today, we are halfway through the labs of the course, so now is a good time to spend some time recalling what we have been over and practising writing a reproducible Quarto-report.\nYour group assignment today is to condense the exercises into a group micro-report! Talk together and figure out how to distil the exercises from today into one small end-to-end runnable reproducible micro-report. DO NOT include ALL of the exercises, but rather include as few steps as possible to arrive at your results. Be very concise!\nBut WHY? WHY are you not specifying exactly what we need to hand in? Because we are training taking independent decisions, which is crucial in applied bio data science, so take a look at the combined group code, select relevant sections and condense - If you don’t make it all the way through the exercises, then condense and present what you were able to arrive at! What do you think is central/important/indispensable? Also, these hand ins are NOT for us to evaluate you, but for you to train creating products and the get feedback on your progress!\nIMPORTANT: Remember to check the ASSIGNMENT GUIDELINES\n…and as always - Have fun!\n\n\n\n\nMcCarthy, Mary K., and Jason B. Weinberg. 2015. “The Immunoproteasome and Viral Infection: A Complex Regulator of Inflammation.” Frontiers in Microbiology 6 (January). https://doi.org/10.3389/fmicb.2015.00021.\n\n\nNolan, Sean, Marissa Vignali, Mark Klinger, Jennifer N. Dines, Ian M. Kaplan, Emily Svejnoha, Tracy Craft, et al. 2020. “A Large-Scale Database of t-Cell Receptor Beta (TCRβ) Sequences and Binding Associations from Natural and Synthetic Exposure to SARS-CoV-2.” August. https://doi.org/10.21203/rs.3.rs-51964/v1.",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab06.html",
    "href": "lab06.html",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "",
    "text": "Package(s)",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#schedule",
    "href": "lab06.html#schedule",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.45: Recap of Lab 5\n08.45 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#learning-materials",
    "href": "lab06.html#learning-materials",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nImportant: Please fill in this brief midway evaluation questionnaire (est. ~2 min)\nBook (Note, this is intentionally 1.ed.): R4DS: Chapter 22: Introduction\nBook (Note, this is intentionally 1.ed.): Chapter 23: Model Basics\nBook (Note, this is intentionally 1.ed.): Chapter 24: Model Building\nBook (Note, this is intentionally 1.ed.): Chapter 25: Many models\nVideo: Broom: Converting Statistical Models to Tidy Data Frames\nVideo: Alex Hayes | Solving the model representation problem with broom | RStudio (2019)\nVideo: “The Joy of Functional Programming (for Data Science)” with Hadley Wickham\nOptional: If you are completely new to statistical modelling, then click here for a primer\n\nUnless explicitly stated, do not do the per-chapter exercises in the R4DS2e book",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#learning-objectives",
    "href": "lab06.html#learning-objectives",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nFit a simple linear model and interpret model parameters\nUnderstand and apply simple purrr-functions for element-wise function application\nUnderstand and apply grouped supervised models to form nested model objects\nUnderstand and apply the broom-functions for tidying various model objects\nOptional LO: Perform a basic principal component analysis for dimension reduction of high dimensional data\nOptional LO: Perform a basic unsupervised k-means clustering of high dimensional data",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#sec-exercises",
    "href": "lab06.html#sec-exercises",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Exercises",
    "text": "Exercises",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#throwback",
    "href": "lab06.html#throwback",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Throwback…",
    "text": "Throwback…\nUsing the tibble() function, re-create the data from this visualisation and then create your version of how this data could be visualised in a more informative manner:\n\n\n\n\n\nAlso… If you still need to be convinced of the flexibility of ggplot, try running this code:\n\nxy &lt;- seq(from = -3,\n          to = 3, \n          by = 0.01)\nexpand.grid(x = xy,\n            y = xy) |&gt;\n  ggplot(\n    mapping = aes(\n      x = (1 - x - sin(y^2)),\n      y = (1 + y - cos(x^2)))) +\n  geom_point(alpha = 0.05,\n             shape = 20,\n             size = 0) +\n  theme_void() +\n  coord_polar()\n\nIf you are curious about what is going on here, try googling “Generative art”… Anyhoo… Let us move on…",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#prologue",
    "href": "lab06.html#prologue",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Prologue",
    "text": "Prologue\nSo far, we have worked on\n\nLab 2: Genomics, the gravier data from Gravier et al. (2010).\nLab 3: Metagenomics, the pitlatrine data from Torondel et al. (2016).\nLab 4: Clinical data, the diabetes data from Willems et al. (1997) and Schorling et al. (1997).\nLab 5: High throughput immunoinformatics data, the SARS-CoV-2 data from Nolan et al. (2020).\n\n\nGetting Started\nFirst, make sure to read and discuss the feedback you got from last week’s assignment!\nThen, once again go to the R for Bio Data Science RStudio Cloud Server\nCreate a new Quarto document and remember to save it\nToday, we will re-examine the data acquired in Lab 2 to further explore the research question:\n\nWhat genes are significantly up- or down-regulated between the patients with/without early metastasis?“",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#load-data",
    "href": "lab06.html#load-data",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Load Data",
    "text": "Load Data\nBefore we do this, go to the console and enter the command: rm(list = ls())\nThis will clear your environment from old object from previous labs\nGood now that we have a clean slate - Recall, so far we have adjusted the chunk setting to #| eval: true first time we get some data from an online repository and then subsequently set #| eval: false.\nLet’s do that in a bit nicer way.\n\nT1: Create a new “Load Data” header in your document and add the below chunk:\n\n\nraw_dir &lt;- \"data/_raw/\"\ndata_file &lt;- \"gravier.RData\"\ndata_loc &lt;- \"https://github.com/ramhiser/datamicroarray/raw/master/data/\"\n\nif( !dir.exists(raw_dir) ){\n  dir.create(path = raw_dir)\n}\nif( !file.exists(str_c(raw_dir, data_file)) ){\n  download.file(\n    url = str_c(data_loc, data_file),\n    destfile = str_c(raw_dir, data_file))\n}\nload(file = str_c(raw_dir, data_file))\n\n\nQ1: In your group, discuss, what is going on here? Make sure you follow the difference between the first time you run this and the second!\n\n\n\n\nClick here for hint\n\n\nWe have used the str_c() function before, try running e.g.:\n\nx &lt;- \"a\"\ny &lt;- \"b\"\nstr_c(x, y)\n\nFor the functions dir.exists() and file.exists(), the hint is in the title, they return logicals. To get a better understanding of this, try running e.g.:\n\nx &lt;- 2\nx == 2\nif( x == 2 ){\n  print(\"Yes!\")\n}\n\nThen change x == 2 to x != 2, re-run the code and see what happens",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#clean-data",
    "href": "lab06.html#clean-data",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Clean Data",
    "text": "Clean Data\nThe next step is to clean up the data:\n\nUse the ls() function to see what objects you have in your environment\nUse the str() function on the gravier data you retrieved to answer:\n\n\nQ2: Discuss in your group if this is tidy data?\nT2: Create a new “Clean Data” header in your document and add the below chunk:\n\n\ngravier_clean &lt;- gravier |&gt;\n  bind_cols() |&gt;\n  as_tibble()\n\n\nQ3: Discuss in your group if this is tidy data?\nQ4: In fact, specifically state what are the “rules” for tidy data?\nQ5: In your group, discuss why bind_cols can by very very dangerous to use?\n\nNow, moving on, let’s write the clean data to disk:\n\nT3: In your “Clean Data”-section, add a new chunk, where you write a tab-separated-values gzipped (i.e. compressed) file called “02_gravier_clean” (with the correct file type specification) into your “data”-folder\n\n\n\n\nClick here for hint\n\n\nJust as you can read a file, you can of course also write a file. Note the file type we want to write here is tab-separated-values. If you in the console type e.g. readr::wr and then hit the Tab key, you will see the different functions for writing different file types. We previously did a trick to automatically gzip (compress) files?",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#augment-data",
    "href": "lab06.html#augment-data",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Augment Data",
    "text": "Augment Data\n\nT4: Create a new “Augment Data” header in your document and add the below chunk:\n\n\ngravier_clean_aug &lt;- gravier_clean |&gt;\n  mutate(y = case_when(y == \"poor\" ~ 1,\n                       y == \"good\" ~ 0)) |&gt; \n  relocate(early_metastasis = y)\n\n\nQ6: In your group, discuss, what each step of the above work flow does, i.e. what are the specifics of the dplyr pipeline?\nT5: In your “Augment Data”-section, add a new chunk, where you write a tab-separated-values gzipped (i.e. compressed) file called “03_gravier_clean_aug” (with the correct file type specification) into your “data” folder",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#analysis",
    "href": "lab06.html#analysis",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Analysis",
    "text": "Analysis\n\nOne Gene, one model\n\nT6: Create a new “Analysis” header in your document\n\nRecall, in the second lab, we were looking at “our favourite gene”. In the following either look back to what was your favourite gene or choose a new 🤷️\nLet’s fit our first model! If the concept of models and linear regression is unfamiliar, consider checking out the primer on linear models in R before proceeding.\n\nT7: Use the lm() function to create your first model and save it to a new variable e.g. “my_first_model”\n\n\n\n\nClick here for hint\n\n\nUse the formula my_favourite_gene ~ early_metastasis and remember when you pipe into the lm() function, you have to specify data = _\n\n\nQ7: What are your coefficients? Mine are:\n\n\n\n     (Intercept) early_metastasis \n     -0.01616011      -0.03426164 \n\n\n\nT8: Use the group_by() \\(\\rightarrow\\) summarise() workflow to calculate the mean values of the gene expression for your favourite gene stratified on early_metastasis\nQ8: What are your mean values? Mine are:\n\n\n\n# A tibble: 2 × 2\n  early_metastasis      mu\n             &lt;dbl&gt;   &lt;dbl&gt;\n1                0 -0.0162\n2                1 -0.0504\n\n\n\nQ9: Discuss in your group: How are your coefficients related to your mean expression values?\n\n\n\n\nClick here for hint\n\n\nRecall, we have two terms here, intercept and slope. The intercept is the y value at x = 0 and the slope is the change in y value, for one unit change in x\n\n\nQ10: Discuss in your group: Is your gene up- or down-regulated from early_metastasis = 0 to early_metastasis = 1 and use the summary() function to see if is it statistically significant at a level of significance of \\(\\alpha = 0.05\\)?\n\n\n\n\nClick here for hint\n\n\nTry to run my_first_model |&gt; summary() and look at the estimate for early_metastasis, i.e. the slope and also see if you can find the p-value somewhere in this summary…\n\nExcellent! Now we have a feeling for working with the lm() functions and a basic understanding of the meaning of the coefficients, when we are using linear regression to model a binary outcome. The reason that we use a linear regression model in this case is, that for these exercises, we want to investigate the relationship between variables rather than obtaining probability predictions, i.e.\n\nWhat genes are significantly up-/down-regulated between the patients with- and without early metastasis?\n\nSo, without further ado, let’s dive in!\n\n\nAll the Genes, all the models\nFirst, the recent couple of years have seen an immense development in unifying the modelling interface in R, which is notoriously inconsistent. You may be familiar with the caret package, the developer of which has created tidymodels, (which I really wish we had time to explore in details). In the following we will work with some of the principles for tidying model object using broom, having object nested in tibbles and working with these using purrr.\nNow, you saw above how we could fit one model for one gene. So, we could repeat the procedure you worked through for each gene, but first consider:\n\nQ11: How many genes are there in the gravier data set?\n\nNow, we just have to make one model for each gene!\n\n\n\n\n\nHonestly, let’s not! Also, recall: “We don’t loop, we func!”, so let’s see how that would work.\n\n\nModels, models everywhere…\nIn principle, you could overflow your environment with individual model objects, but that would require a lot of code-lines and a lot of hard-coding. But let us instead see if we can come up with something just a tad more clever.\n\nPreparing the Data\nFirst:\n\nQ12: Discuss in your group, if the gravier_clean_aug is a “wide” or a “long” dataset?\n\nOnce you have agreed upon and understood why, this is a wide data set, proceed and:\n\nT9: Create this long version of your gravier_clean_aug data and save it in gravier_clean_aug_long\n\n\n\n# A tibble: 488,040 × 3\n   early_metastasis gene   log2_expr_level\n              &lt;dbl&gt; &lt;chr&gt;            &lt;dbl&gt;\n 1                0 g2E09         -0.00144\n 2                0 g7F07         -0.00144\n 3                0 g1A01         -0.0831 \n 4                0 g3C09         -0.0475 \n 5                0 g3H08          0.0158 \n 6                0 g1A08         -0.0336 \n 7                0 g1B01         -0.136  \n 8                0 g1int1         0.0180 \n 9                0 g1E11          0.0257 \n10                0 g8G02          0.00720\n# ℹ 488,030 more rows\n\n\n\n\n\nClick here for hint\n\n\nRecall which function took a dataset from “wide” to “long” format and also the three parameters of interest for that function is cols, names_to and values_to. If you look at my example, you will see that the columns we have pivoted have types &lt;chr&gt; and &lt;dbl&gt;, these will map to the names_to and values_to parameters respectively. The cols can be used with the so-called “helper-functions”, that we have previously talked about. If you look at the gene-columns in the data, all the genes starts with a “g”, so we can use the helper functions starts_with() and then give the argument “g” to the match parameter\n\nThe reason we want the “long” version of the data set is, that now we have ALL the genes defined in ONE gene column. This means that we can use group_by() to work per gene without looping (Recall: Don’t loop, only func!).\nNow:\n\nT10: Create a dplyr pipeline, use the group_by() function to group your gravier_clean_aug_long dataset by gene and then add the nest() and ungroup() functions to your pipeline\n\n\n\n# A tibble: 2,905 × 2\n   gene   data              \n   &lt;chr&gt;  &lt;list&gt;            \n 1 g2E09  &lt;tibble [168 × 2]&gt;\n 2 g7F07  &lt;tibble [168 × 2]&gt;\n 3 g1A01  &lt;tibble [168 × 2]&gt;\n 4 g3C09  &lt;tibble [168 × 2]&gt;\n 5 g3H08  &lt;tibble [168 × 2]&gt;\n 6 g1A08  &lt;tibble [168 × 2]&gt;\n 7 g1B01  &lt;tibble [168 × 2]&gt;\n 8 g1int1 &lt;tibble [168 × 2]&gt;\n 9 g1E11  &lt;tibble [168 × 2]&gt;\n10 g8G02  &lt;tibble [168 × 2]&gt;\n# ℹ 2,895 more rows\n\n\nNote, this is conceptually a super-tricky data structure!\n\nQ13: Discuss in your group, what happened to the data?\n\n\n\n\nClick here for hint\n\n\nTry to run each of the following code chunks and examine what happens at each step:\n\ngravier_clean_aug_long_nested\n\n\ngravier_clean_aug_long_nested |&gt;\n  filter(gene == \"g2E09\") # Replace \"g2E09\" with whatever was YOUR favourite gene!\n\n\ngravier_clean_aug_long_nested |&gt;\n  filter(gene == \"g2E09\") |&gt; # Replace \"g2E09\" with whatever was YOUR favourite gene!\n  pull(data)\n\n\n\nQ14: Moreover, discuss in your group, what does &lt;tibble [168 × 2]&gt; mean?\n\nNow, if you’re experiencing, that the server seems slow, consider if you want to proceed now with ALL the genes or just a subset. If you just want a subset, then use sample_n() to randomly select e.g. 100 genes for further analysis. Remember you may want to use the set.seed() function to create a reproducible work flow even when sampling.\n\n\nFitting Models\nNow, recall our research question:\n\nWhat genes are significantly up-/down-regulated between the patients with- and without early metastasis?\n\nTo investigate this, we want to fit a linear model to each gene, i.e. as you did initially for your favourite gene, we want to do in a clever way per gene for ALL genes.\n\nT11: Use the group_by() function to let R know, that we want to work per gene\n\n\n\n# A tibble: 2,905 × 2\n# Groups:   gene [2,905]\n   gene   data              \n   &lt;chr&gt;  &lt;list&gt;            \n 1 g2E09  &lt;tibble [168 × 2]&gt;\n 2 g7F07  &lt;tibble [168 × 2]&gt;\n 3 g1A01  &lt;tibble [168 × 2]&gt;\n 4 g3C09  &lt;tibble [168 × 2]&gt;\n 5 g3H08  &lt;tibble [168 × 2]&gt;\n 6 g1A08  &lt;tibble [168 × 2]&gt;\n 7 g1B01  &lt;tibble [168 × 2]&gt;\n 8 g1int1 &lt;tibble [168 × 2]&gt;\n 9 g1E11  &lt;tibble [168 × 2]&gt;\n10 g8G02  &lt;tibble [168 × 2]&gt;\n# ℹ 2,895 more rows\n\n\n\nT12: Then using the map()-function, add a new line to your pipeline, where you add a new variable model_object to your gravier_clean_aug_long_nested dataset, which R will compute per gene\n\n\n\n\nClick here for hint\n\n\nTo do this you will need the following syntax and then wrap that inside the relevant tidyverse-function for creating a new variable:\n\nmodel_object = map(.x = data,\n                   .f = ~lm(formula = log2_expr_level ~ early_metastasis,\n                            data = .x))\n\n\nMake sure to understand the map() function here, it is completely central to functional programming with purrr:\n\nWe need the group_by() to define which variable holds the elements to each of which we want to map\nmodel_object is a new variable, we are creating, which will contain the result of our call to the map() function\n.x to what existing (nested) variable are we mapping?\n.f which function do we want to map to each element in the existing (nested) variable?\nNote that log2_expr_level and early_metastasis are variables “inside” the nested data variable\n\nNote, once again, this is conceptually a super-tricky data structure, not only do we have a per gene nested tibble, but now we also have a per gene nested model object - So please do make sure to discuss in your group, what is going on here, e.g. try running this and discuss what you see:\n\ngravier_clean_aug_long_nested |&gt;\n  filter(gene == \"g2E09\") |&gt; # Replace \"g2E09\" with whatever was YOUR favourite gene!\n  pull(model_object)\n\n\n\nTidying Models\nExcellent! Now we have a per gene model. Let us use the broom package to extract some information from each of the models. First, to get a better understanding of what is going on when calling the tidy() function, try running this:\n\ngravier_clean_aug_long_nested |&gt;\n  \n  # Here, you should replace \"g2E09\" with whatever was YOUR favourite gene!\n  filter(gene == \"g2E09\") |&gt; \n  \n  # Pull() on tibbles: This pulls out the model_object variable.\n  #   Note! This is a list, because we nested!\n  pull(model_object) |&gt; \n  \n  # Pluck() on lists: From the list we got from the last step,\n  #   we \"pluck\" the first element\n  pluck(1) |&gt;\n  \n  # The result of pluck, is a model object,\n  #   upon which we can call the tidy function\n  tidy(conf.int = TRUE,\n       conf.level = 0.95)\n\nNow, we want to apply this tidy() function per model_object:\n\nT13: Scroll a bit back to where we created the model_object and see if you can translate that into mapping the tidy() function to the model_object variable, thereby creating a new variable model_object_tidy - This is tricky, so do make sure to discuss in your group how this can be done!\n\n\n\n\nClick here for hint\n\n\nRemember the parameters to the tidy() functions, which gives us the confidence intervals and defines corresponding level. Also, everything you need is in the previous layout of the map() function. Note, that the calls to the pull() and pluck() functions above, pertains to extracting from a tibble, which we do not want, on the contrary, we want all objects to be contained in our gravier_clean_aug_long_nested data\n\n\n\n# A tibble: 2,905 × 4\n# Groups:   gene [2,905]\n   gene   data               model_object model_object_tidy\n   &lt;chr&gt;  &lt;list&gt;             &lt;list&gt;       &lt;list&gt;           \n 1 g2E09  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n 2 g7F07  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n 3 g1A01  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n 4 g3C09  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n 5 g3H08  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n 6 g1A08  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n 7 g1B01  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n 8 g1int1 &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n 9 g1E11  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n10 g8G02  &lt;tibble [168 × 2]&gt; &lt;lm&gt;         &lt;tibble [2 × 7]&gt; \n# ℹ 2,895 more rows\n\n\nNote, once again, this is conceptually a super-tricky data structure, not only do we have a per gene nested tibble, but now we also have a per gene nested model object and now also a nested tibble of tidyed objects - So please again do make sure to discuss in your group, what is going on here, e.g. try running this and discuss what you see:\n\ngravier_clean_aug_long_nested |&gt;\n  filter(gene == \"g2E09\") |&gt; # Replace \"g2E09\" with whatever was YOUR favourite gene!\n  pull(model_object_tidy)\n\n\n\nWrangling\nWe’re almost there - just a bit of wrangling to go!\nJust as you saw that we could nest() on a variable (recall we did that for the gene variable), you can do the opposite and lo and behold, that is the unnest() function. Before we continue:\n\nT14: Create a dplyr pipeline and save the result in a new variable called gravier_estimates: Use the unnest() function to unpack the model_object_tidy\n\n\n\n# A tibble: 5,810 × 10\n# Groups:   gene [2,905]\n   gene  data     model_object term         estimate std.error statistic p.value\n   &lt;chr&gt; &lt;list&gt;   &lt;list&gt;       &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 g2E09 &lt;tibble&gt; &lt;lm&gt;         (Intercept)  -0.0162     0.0117    -1.38  1.69e-1\n 2 g2E09 &lt;tibble&gt; &lt;lm&gt;         early_metas… -0.0343     0.0201    -1.71  8.99e-2\n 3 g7F07 &lt;tibble&gt; &lt;lm&gt;         (Intercept)   0.0604     0.0139     4.35  2.36e-5\n 4 g7F07 &lt;tibble&gt; &lt;lm&gt;         early_metas… -0.0185     0.0238    -0.778 4.38e-1\n 5 g1A01 &lt;tibble&gt; &lt;lm&gt;         (Intercept)  -0.0290     0.0123    -2.35  1.99e-2\n 6 g1A01 &lt;tibble&gt; &lt;lm&gt;         early_metas… -0.0367     0.0211    -1.73  8.47e-2\n 7 g3C09 &lt;tibble&gt; &lt;lm&gt;         (Intercept)   0.0518     0.0145     3.58  4.55e-4\n 8 g3C09 &lt;tibble&gt; &lt;lm&gt;         early_metas… -0.0148     0.0249    -0.595 5.53e-1\n 9 g3H08 &lt;tibble&gt; &lt;lm&gt;         (Intercept)   0.0142     0.0128     1.11  2.69e-1\n10 g3H08 &lt;tibble&gt; &lt;lm&gt;         early_metas…  0.00247    0.0220     0.112 9.11e-1\n# ℹ 5,800 more rows\n# ℹ 2 more variables: conf.low &lt;dbl&gt;, conf.high &lt;dbl&gt;\n\n\n\nT15: The again, create a dplyr pipeline and save the result in a the same gravier_estimates variable: Subset the rows to only get the slope term and then choose variables as displayed below, finally end with un-grouping your data, as we no longer need the groups\n\n\n\n# A tibble: 2,905 × 5\n   gene   p.value estimate conf.low conf.high\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 g2E09   0.0899 -0.0343   -0.0739   0.00539\n 2 g7F07   0.438  -0.0185   -0.0655   0.0285 \n 3 g1A01   0.0847 -0.0367   -0.0784   0.00508\n 4 g3C09   0.553  -0.0148   -0.0639   0.0343 \n 5 g3H08   0.911   0.00247  -0.0410   0.0459 \n 6 g1A08   0.859  -0.00363  -0.0438   0.0366 \n 7 g1B01   0.279  -0.0218   -0.0615   0.0178 \n 8 g1int1  0.666  -0.0113   -0.0627   0.0402 \n 9 g1E11   0.106  -0.0329   -0.0728   0.00703\n10 g8G02   0.633  -0.0108   -0.0555   0.0338 \n# ℹ 2,895 more rows\n\n\n\nT16: To your gravier_estimates dataset, add a variable q.value, which is the result of calling the p.adjust() function on your p.value variable and also add an indicator variable denoting if a given gene is significant or not\n\n\n\n# A tibble: 2,905 × 7\n   gene   p.value estimate conf.low conf.high q.value is_significant\n   &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;         \n 1 g2E09   0.0899 -0.0343   -0.0739   0.00539       1 no            \n 2 g7F07   0.438  -0.0185   -0.0655   0.0285        1 no            \n 3 g1A01   0.0847 -0.0367   -0.0784   0.00508       1 no            \n 4 g3C09   0.553  -0.0148   -0.0639   0.0343        1 no            \n 5 g3H08   0.911   0.00247  -0.0410   0.0459        1 no            \n 6 g1A08   0.859  -0.00363  -0.0438   0.0366        1 no            \n 7 g1B01   0.279  -0.0218   -0.0615   0.0178        1 no            \n 8 g1int1  0.666  -0.0113   -0.0627   0.0402        1 no            \n 9 g1E11   0.106  -0.0329   -0.0728   0.00703       1 no            \n10 g8G02   0.633  -0.0108   -0.0555   0.0338        1 no            \n# ℹ 2,895 more rows\n\n\nBut… q.value??? What are you on about??? For a nice primer on how multiple testing correction works, please read this article by Noble (2009)!\n\n\nRecap\nIf you (understandably by now) have lost a bit of overview of what is going on, let’s just re-iterate.\n\nWe have the gravier dataset, with the log2-expression levels for 2,905 genes of 168 patients of whom 111 did not have early metastasis and 57 who did\nWe are interested in investigating what genes are significantly up-/down-regulated between the patients with- and without early metastasis\nFirst, we retrieved the data from the data repository, cleaned and augmented it and saved it to disk\nThen pivoted the data, so we could work per gene (The gene variable)\nNext, we grouped per gene and nested the data (The data variable)\nThen, we fitted a linear model to each gene (The model_object variable)\nNext, we used the broom package to tidy the fitted model incl. getting confidence intervals (The model_object_tidy variable)\nLastly, we extracted the model parameters, corrected for multiple testing and added and indicator for significant findings\n\nNow, we actually have everything we need to answer:\n\nWhat genes are significantly up- or down-regulated between the patients with/without early metastasis?\n\nIn the following, we will use a level of significance of \\(\\alpha = 0.05\\) to provide this answer.\n\n\nVisualise\nRight, let’s get to it!\n\nT17: Re-create this forest plot to finally reveal the results of your analysis GROUP ASSIGNMENT (Important, see: how to)\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nHere, you will have to use your indicator variable to identify significant genes before plotting and then it would probably be prudent to take a look at the fct_reorder() function and geom_errorbarh() representation\n\n\nT18: Re-create this volcano plot to finally reveal the results of your analysis GROUP ASSIGNMENT part II\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nHere,before plotting you will have to create a new label-variable which takes value gene for significant genes and otherwise is simply an empty string, which we denote by \"\". Also, perhaps the ggrepel package would be relevant for somehow adding text/labels",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#group-assignment",
    "href": "lab06.html#group-assignment",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "GROUP ASSIGNMENT",
    "text": "GROUP ASSIGNMENT\nFor the group assignment this time, you will use T17 and T18 to again create a reproducible micro-report and make sure to:\n\nRead the assignment instructions\nRead and follow the code styling guidelines",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab06.html#optional",
    "href": "lab06.html#optional",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Optional",
    "text": "Optional\nThe following is only if you can find the time! But, perhaps this would be something interesting to revisit during the project period\n\nPCA\n\nGo and visit this blog post by Claus O. Wilke, Professor of Integrative Biology.\nYour task is to work through the blog post using the gravier dataset to create a PCA analysis\n\n\n\nK-means\n\nGo and visit this K-means clustering with tidy data principles post\nYour task is to work through the blog post using the gravier dataset to create a K-means analysis\n\n\n\n\n\nGravier, Eléonore, Gaëlle Pierron, Anne Vincent‐Salomon, Nadège Gruel, Virginie Raynal, Alexia Savignoni, Yann De Rycke, et al. 2010. “A Prognostic DNA Signature for T1T2 Node‐negative Breast Cancer Patients.” Genes, Chromosomes and Cancer 49 (12): 1125–34. https://doi.org/10.1002/gcc.20820.\n\n\nNoble, William S. 2009. “How Does Multiple Testing Correction Work?” Nature Biotechnology 27 (12): 1135–37. https://doi.org/10.1038/nbt1209-1135.\n\n\nNolan, Sean, Marissa Vignali, Mark Klinger, Jennifer N. Dines, Ian M. Kaplan, Emily Svejnoha, Tracy Craft, et al. 2020. “A Large-Scale Database of t-Cell Receptor Beta (TCRβ) Sequences and Binding Associations from Natural and Synthetic Exposure to SARS-CoV-2.” August. https://doi.org/10.21203/rs.3.rs-51964/v1.\n\n\nSchorling, John B., Julienne Roach, Marjorie Siegel, Natalie Baturka, Dawn E. Hunt, Thomas M. Guterbock, and Herbert L. Stewart. 1997. “A Trial of Church-Based Smoking Cessation Interventions for Rural African Americans.” Preventive Medicine 26 (1): 92–101. https://doi.org/10.1006/pmed.1996.9988.\n\n\nTorondel, Belen, Jeroen H. J. Ensink, Ozan Gundogdu, Umer Zeeshan Ijaz, Julian Parkhill, Faraji Abdelahi, Viet-Anh Nguyen, et al. 2016. “Assessment of the Influence of Intrinsic Environmental and Geographical Factors on the Bacterial Ecology of Pit Latrines.” Microbial Biotechnology 9 (2): 209–23. https://doi.org/10.1111/1751-7915.12334.\n\n\nWillems, James P., J Terry Sanders, Dawn E. Hunt, and John B. Schorling. 1997. “Prevalence of Coronary Heart Disease Risk Factors Among Rural Blacks: A Community-Based Study.” Southern Medical Journal 90 (8): 814–20. https://doi.org/10.1097/00007611-199708000-00008.",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab07.html",
    "href": "lab07.html",
    "title": "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio",
    "section": "",
    "text": "Package(s)",
    "crumbs": [
      "Course Labs",
      "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio"
    ]
  },
  {
    "objectID": "lab07.html#schedule",
    "href": "lab07.html#schedule",
    "title": "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.10: Midway evaluation\n08.10 - 08.30: Recap of Lab 6\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises",
    "crumbs": [
      "Course Labs",
      "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio"
    ]
  },
  {
    "objectID": "lab07.html#learning-materials",
    "href": "lab07.html#learning-materials",
    "title": "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: Happy Git and GitHub for the useR – Read chapters 1 (intro), 20 (basic terms), 22 (branching), 23 (remotes). Do not pay much attention to syntax of specific commands, because we are not going to use them during the exercises, focus on the idea\nBook: Introduction to Data Science - Data Analysis and Prediction Algorithms with R by Rafael A. Irizarry: Chapter 40 Git and GitHub – Some of the information here is redundant with the previous book, but very important thing is a visualization of basic git actions and screenshots of how to perform them using RStudio\nVideo: RStudio and Git - an Overview (Part 1) – Basic git concepts, for those who prefer listen rather than read. Books, however, contain more information\nVideo: How to use Git and GitHub with R – Basic operating on git in RStudio. Complementary to second book. You can skip to 2:50, we are not going to link to git manually either way\n\nUnless explicitly stated, do not do the per-chapter exercises in the R4DS2e book",
    "crumbs": [
      "Course Labs",
      "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio"
    ]
  },
  {
    "objectID": "lab07.html#learning-objectives",
    "href": "lab07.html#learning-objectives",
    "title": "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nExplain why reproducible data analysis is important\nIdentify associated relevant challenges\nDescribe the components of a reproducible data analysis\nUse RStudio and GitHub (git) for collaborative bio data science projects",
    "crumbs": [
      "Course Labs",
      "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio"
    ]
  },
  {
    "objectID": "lab07.html#sec-exercises",
    "href": "lab07.html#sec-exercises",
    "title": "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio",
    "section": "Exercises",
    "text": "Exercises\n\nPrologue\nUsing git is completely central for collaborative bio data science. You can use git not only for R, but for any code-based project in any language or code editor. From small research groups to tech giants, almost every bioinformatics, data science or software engineering project in the world uses git to manage code versions and collaboration.\nLearninggit is a key Bio Data Science skill that will be expected of you for almost any bioinformatics job! Today, you will take your first steps - Happy Learning!\n\n\n\n\n\n\nT1: Find the GitHub repository for the ggplot2 R-package\nQ1: How many Contributors are there?\nT1: Find the GitHub repository for the Linux kernel\nQ2: How many Contributors are there?\nQ3: Discuss in your group why having an organised approach to version control is central? And consider the simple contrast of the challenges you were facing when doing the course assignments.\n\n\n\nGetting Started\nFirst, make sure to read and discuss the feedback you got from last week’s assignment!\nThe following exercises have to be done in your groups! You must move at the same pace and progress together as a team through the exercises!\nBe aware of which editor you are using in RStudio! You must work in the “Visual” editor instead of the “Source” editor. ALL OF YOU! If one of you works in a different editor, it could cause a conflict issue!\nGitHub is the place for collaborative coding and different group members will have to do different tasks in a specific order, to make it through the exercises together, so… Team Up and don’t rush it!\nFirst, select a team Captain. That person will have to carry out specific tasks. Tasks under Team should be carried out by all group members (including the captain), but if Crew is stated, then those tasks are carried out by everyone except the Captain.\nIt is important to note that tasks are sequential, so if a task is assigned to the Captain, then the Crew has to await completion before proceeding!\n\nTeam\n\n\nSetting up your Credentials and Personal Access Token\nBefore you do anything relating to GitHub repositories, you have to inform GitHub of who you are and let GitHub verify what your account is. Without authentication, anyone could be pushing to your repo or pulling from your private repos!\n\nIn the Console, run the command:\n\n\nusethis::use_git_config(user.name = \"YOUR_GITHUB_USERNAME\", user.email = \"THE_EMAIL_YOU_USED_TO_CREATE_YOUR_GITHUB_ACCOUNT\")\n\n\nIn the Console, run the command:\n\n\nusethis::create_github_token()\n\n\nLog into GitHub if not logged in already\nUnder Note, where it says DESCRIBE THE TOKEN’S USE CASE, delete the text and write e.g.R for Bio Data Science lab 7 git exercises\nUnder Expiration, where it says 30 days, change that to 90 days\nDo not change any other setting, but simply scroll down and click Generate token\nA new Personal access tokens (classic) page will appear, stating your personal access token, which starts with ghp_, go ahead and copy it\nStore the PAT somewhere safe, e.g. in a password management tool. It is effectively your password when connecting to the repository via R.\nAgain in the Console, run the below command and enter your PAT when prompted for the password:\n\n\ngitcreds::gitcreds_set()\n\nBefore proceeding, make sure that ALL group members are at this stage of the exercises.\n\n\nMaking your Credentials and Personal Access Token stick around\nBy default, your credentials and PAT are cleaned every 15 minutes. This is annoying, so let’s fix it!\nFirst of all RStudio is a Graphical User Interface (GUI), meaning, that it allows you to do pointy-clicky stuff, but at the end of the day, everything happens at the command line. That also goes for git, so your button-pushing simply gets converted into commands, which are executed in the terminal.\nIf you are not comfortable with the terminal, don’t worry! We will only do a short visit to the command line to do a few things that are not available in the GUI and then return to the pointy-clicky stuff:\n\nIn the Console pane, click the Terminal tab, which gains you access to the Linux server that RStudio is running on. You should see something like:\n\nuser@pupilX:~/projects/lab07_git_exercises$\n\nEnter the command:\n\ngit config --global --list\n\nRecall you entered your GitHub username and mail? This is where it ended up! Note the credential.helper=cache, which tells us that the credentials are being cached. Now, enter:\n\ngit config --global credential.helper 'cache --timeout=86400'\n\nRe-run the command git config --global --list and confirm the change\nGo back to your Console and run the command gitcreds::gitcreds_set(). Select option 2, and re-enter your PAT.\nIf you didn’t get a list of options in the prior step, restart your R session and retry.\n\nCongratulations! You have now used the command line and you will forever be part of an elite few, who know that everything you see in hacker-movies is BS, except perhaps for Wargames and Mr. Robot! Also, your PAT will now be good for 24h\n\n\nTeam\n\nGo to GitHub and log in\nIn the upper right corner, click on your profile picture. Then in the menu, click on “Your Organizations” and go into rforbiodatascienceYY (where YY is the current year). If you are not a member, contact a TA before proceeding.\nGo into repositories (on the top left).\n\n\n\nCaptain\n\n\nCreating the repository\n\nClick “New repository”\nSince you created the project from the organisation, the default owner of the repository is the organisation (rforbiodatascienceYY). Verify and keep that, please.\nName the repository groupXX, where XX is your group number, e.g. 02\nSelect Public\nTick Add a README file\nClick Create repository\nClick Settings in the menu line starting with &lt;&gt; Code\nUnder Access, click Collaborators and teams\nClick Add people\nWrite a group members username and select the role “Maintain”.\nSelect Add username to this repository\nRepeat for all group members\n\n\n\nCrew\n\nRefresh the page and go into the new repository. If you don’t see it, check your mail and accept your invitation to join the repository.\n\n\n\nCaptain\n\n\nCloning the repository\n\nClick here to go to the course RStudio cloud server and login\nIn the upper right corner, click on &lt;your_current_project_name&gt;\nChoose  New Project...\nSelect  Version Control\nSelect  Git\nUnder Repository URL:, enter https://github.com/rforbiodatascienceYY/groupXX, where again you replace YY with the current year and replace XX with your group number, e.g. 02.\nUnder Project directory name:, enter lab07_git_exercises\nUnder Create project as a subdirectory of:, make sure that is says ~/projects\nClick Create Project\n\nCongratulations! You have just cloned your first GitHub repository!\nIn your Files pane, you should now see:\n\n\n\n\n\n…and now in your Environment pane, you should see a new Git tab:\n\n\n\n\n\nIf you click the Git tab, you should see:\n\n\n\n\n\n\n\nPushing the project basics to GitHub\n\nIn the Git tab, tick the 2 boxes under staged (we will get into what .gitignore is later)\nClick commit\nIn the upper right corner, add a Commit message, e.g “Project basics”\nClick the Commit button\nA pop-up, will give you details on your commit, look through them and then click Close\nClick the  Pull button, then the  Push button\n\n\n\nCrew\n\nMake sure that you can see the lab07_git_exercises.Rproj and .gitignore files that your captain just pushed in the GitHub repository before proceeding further\nFollow the instructions under Cloning the repository that your captain followed a few moments ago (but not the Pushing the project basics to GitHub)\n\n\n\nTeam\n\n\nSetting up git’s pull strategy\nIn the Console, run the command:\n\nusethis::use_git_config(pull.rebase = \"false\")\n\n\n\n\nFor now, you can ignore what pull.rebase does, but click here if you are curious\n\n\nWhen pulling from a repository that has received new commits between your last pull and your last commit, you can run into the problem that there are new commits with changes in the same places of the code as you have been editing. In this case, a rebase will simply apply your changes on top of whatever modifications happened in those new commits. This can sometimes break the code and it can be hard to understand why. Setting pull.rebase = \"false\" will make it so that you are forced to solve the conflicting changes manually when you run into that situation. You can find a much more technical explanation on merging vs. rebasing here.\nIf you do not set pull.rebase at all, the first time that your pull conflicts with existing commits git will simply ask you to set it. Doing it now simply avoids the error message from appearing later.\n\n\n\n\nYour first collaboration\n\nTeam\n\nCreate a new Quarto document, title it student_id and save id as student_id.qmd, where student_id is your… You guessed it! Make sure that “Use visual markdown editor” is checked.\nIn the Environment pane, click the Git tab\nTick the box under Staged corresponding to your newly created student_id.qmd. If you do not see the document in this window after saving, click the  refresh button on the top right of the Git tab.\nClick Commit\nIn the upper right corner, add a Commit message, e.g “First commit by student_id”\nClick the Commit button\nA pop-up, will give you details on your commit, look through them and then click Close\nNow, very important ALWAYS click the  Pull button BEFORE clicking the  Push button\nClicking Pull, you should see “Already up to date.”\nThen click Push\n\n\nQ4: Discuss in your group what each of the steps did and why they are performed?\n\n\n\n\nYour next collaboration\nThat first collaboration was easy right!? Well, you were all working on different files…\n\nCaptain\n\nCreate a new Quarto Document, title it “Group Document” and save it as group_document.qmd\nUsing markdown headers, create one section for each group member (including yourself). Here, you can use your names, student ids or whatever you deem appropriate\nAgain in the Environment pane, make sure you are in the git tab and then tick the box next to group_document.qmd. As before, click the  refresh button if you can’t see it.\nClick Commit\nAdd a commit message, e.g. “Add group document”\nClick the Commit button\nClick Pull (You should be “Already up to date.”)\nClick Push\nClick Close\nAgain, go to the group GitHub and confirm that you see the new document you just created\n\n\n\nTeam\n\nClick Pull and confirm that you now also have the file group_document.qmd\nOpen group_document.qmd and find your assigned section\nIn your section and your section only, enter some text, add a few code chunks with some R code\nSave the document\nNow again, find the group_document.qmd in the git tab of the Environment pane and tick the box under Staged\nClick Commit\nNote how your changes to the document are highlighted in green\nAdd a commit message, e.g. “Update the STUDENT_ID section” and click Commit and then Close\nLike before, always  Pull before  Pushing\nGo to the group GitHub, find the group_document.qmd and click it, do you see your changes?\n\nOnce everyone has added to their assigned section, everyone should do a pull/push, so everyone has the complete version of the group_document.qmd. If everything was done correctly there should be no merge conflicts, but contact a TA if anything went wrong.\n\n\n\nYour first branching\nOk that’s pretty great so far - Right? The thing is… Consider, the ggplot2 repository, that you found in T1. Thousands of companies and even more thousands of people rely on ggplot for advanced data visualisation. What would happen, if you wanted to add a new feature or wanted to optimise an existing one, while people were actively installing your package? They would get what stage your code was in, which may or may not be functional - Enter branching!\nBelow here is an illustrated example where each circle is a git commit. The main branch is the stable version of the software that people can download. Your Work will be the feature or update that you personally are working on and Someone Else’s Work will be another team working on a different new feature or update. Before the last commit in main (the right-most green circle), note how both the Your Work and Someone Else’s Work branches have been merged onto the main branch.\n\n\n\n\n\nNote: until not so long ago, the default name for the main branch used to be “master”. For obvious reasons, the software engineering world is moving away from the master/slave terminology…\nIn a git repository, there can be as many branches as you want. Software teams usually have guidelines on how to organize branches on a specific repository and it can vary substantially between teams and/or projects.\n\nQ5: Find the ggplot GitHub repo again. How many branches there are?\n\n\nTeam\n\nGo to your RStudio session and in the Git tab of the Environment pane, click  New Branch\nIn branch name, enter your student id\nYou should see a pop-up with the message Branch 'STUDENT_ID' set up to track remote branch 'STUDENT_ID' from 'origin'. at the bottom, go ahead and click Close\nNext to where you clicked New Branch just before, it should now say STUDENT_ID, click it and confirm that you see STUDENT_ID and main\nLook at the illustration above and understand that you have created a new branch STUDENT_ID that branches out from main and is equivalent to Your Work. Discuss this with your group if it is not clear.\nClick STUDENT_ID and you will get a confirmation that you are already on that branch and that you are up-to-date, click Close\nIn the group_document.qmd under your section, using markdown, enter a new sub-header and name it e.g. New feature or New analysis. As a reminder, make sure that you all use the “Visual” editor for the .qmd file.\nEnter some text, a few code chunks with a bit of R-code of your choosing and save the document\nAgain, in the Git tab, tick the box under staged and click Commit\nAdd a commit message, e.g. New feature from STUDENT_ID and click Commit\nClick Close, Pull, Close, Push and Close and then close the commit window\nGo to your group GitHub and confirm that you now have at least 2 branches\nMake sure you are in the main branch and then click the group_document.qmd, you should now not see your new feature/analysis that you added\nOn the left, where it says main, click and select your STUDENT_ID, you should now see your new feature/analysis that you added\n\nCongratulations! You have now successfully done your first branching!\n\n\n\nYour first branch merging\n\nTeam\n\nGo to your groups GitHub page, at the top it should say STUDENT_ID had recent pushes..., click the Compare & pull request\nYour commit message will appear and where it says Leave a comment, add a comment like e.g. I'm done, all seems to be working now! or similar\nClick Create pull request\nIt should now say This branch have no conflicts with the base branch, confirm and click Merge pull request\nClick Confirm merge after which it should now say Pull request successfully merged and closed\nYou have now fully merged, so go ahead and click Delete branch\nRevisit the previous illustration and compare with your branch workflow, make sure that everyone in the groups are on par here\nFinally return to your RStudio session and make sure you switch to the main branch\n\nCongratulations! You have now successfully done your first branch merge!\nBut wait, what was this Pull request??? What you actually did, was:\n\nCreated a new branch\nCompleted a new feature/analysis\nPushed the new feature/analysis to GitHub\nCreated a Pull request for merging your branch STUDENT_ID into the main branch\nApproved and completed the Pull request\n\nThink about e.g. again the ggplot2 repo, if anyone could create a new branch and then do as described above, then there would be no way of making quality control. Therefore, typically such pull requests will have to be approved by someone. This can either be someone who is close-to-the-code e.g. in the case of ggplot2, that’d be someone like Hadley. In a company, then that might be some senior developer approving junior developers pull requests. At one point you might have seen something like “The main branch is unprotected”, this is exactly that!\n\n\n\nSmall note about the term Pull request\n\n\nThe name Pull request can be a bit counter-intuitive, because you are not requesting to pull the repository, but rather requesting to the maintainers that they pull your changes. Maybe, a more intuitive name would have been Merge request, which doesn’t leave much room for ambiguity. Actually, in most other git repository hosting services aside from GitHub (e.g. GitLab) the equivalent feature is actually called Merge request.\n\n\n\n\nYour first merge conflict\nOkay, that’s all good an well. Seems easy and straight forward, right? Well, that is as long as we don’t have a conflict. A conflict is when two or more changes to one file are not compatible. In fact, let’s screw things up! We will be working with this file:\nTop 10 Bio Data Science Languages of ALL Time:\n10. It is \n9. Impossible\n8. to rank them\n7. Because\n6. programming is subjective\n5. and everyone\n4. has\n3. different\n2. tastes\n1. R\n\nCaptain\n\nGo to the RStudio session and where you usually click Quarto Document..., this time let’s just create a simple Text File\nCopy/paste the Top 10 Bio Data Science Languages of ALL Time into the file\nSave the file as best_ds_langs.txt\nMake sure you are in the main branch\nDo a commit/pull/push and check that the file can be seen in your group’s GitHub repo\n\n\n\nCrew\nRrrrrrr… Let’s commit mutiny! (Double pun intended)\n\nMake sure you are in the main branch and then  Pull\nOpen the best_ds_langs.txt\nEach of you separately (wrongfully) replace R with an (inferior) bio data science language of your choice\nThen commit, pull and push.\nThe first crewmember to push will have succeeded. But if you weren’t so lucky, you will now be notified about a merge conflict. Close and re-open the best_ds_langs.txt file, it should look something like:\n\nTop 10 Bio Data Science Languages of ALL Time:\n10. It is \n9. Impossible\n8. to rank them\n7. Because\n6. programming is subjective\n5. and everyone\n4. has\n3. different\n2. tastes\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\n1. C++\n=======\n1. Python\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; 85283ca3246b7f7462ab633085f92ac5f173d3e7\n\n\n\ngit’s merge conflict syntax\n\n\nA merge conflict happens when you try to pull from a repository that has had changes in the same lines of a file that you have also changed. Git doesn’t know which version to pick, so it asks you to resolve it. When such conflicts occur, git leaves markers in the file, showing:\n\nEverything below &lt;&lt;&lt;&lt;&lt;&lt;&lt; is your current changes (HEAD means “your current commit”)\n======= is simply the separator between the 2 versions\nEverything above &gt;&gt;&gt;&gt;&gt;&gt;&gt; is the version you are pulling. That long “random” code is called the commit hash and is a unique identifier for each commit in your git repository. You can use this code to return back (or go forward to) any committed version of your code.\n\n\n\nKeep only your preferred Bio Data Science language and also remove the lines containing &lt;&lt;&lt;&lt;&lt;&lt;&lt;, ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt;\n\nCongratulations! You have now fixed your first merge conflict!\n\nYou will need to commit your changes again before pulling and pushing. However, if another crewmemeber has (again) pushed before you, you will have yet another merge conflict to solve! Return to step 5 if that is the case. This is the chaos of mutiny and why a well organized team and modular code are critical for collaborative software development.\n\nDiscuss with your group what just happened and make sure that everyone understands why merge conflicts can happen.\n\n\nCaptain\nGet your Crew in order!\n\nPull the changes made by your crew and simply edit the file so that you delete everything below 2. and add the final line, which emphasises that 1. R is superior!\nCommit your changes and add a commit message, e.g. Got crew back in order!\nPull and Push to the GitHub repo\nReturn to your GitHub repository and confirm all is in order and that you can continue with confidence knowing that R is superior! (Check the file to make sure)\n\n\n\nTeam\n\nPull from RStudio and make sure that your best_ds_langs.txt has the changes that your captain just made.\nClick the History button next to the Push button and explore the history of how your branches and commits have changed through these exercises. Discuss with your group what you see.\n\n\n\n\nThe .gitignore file\nYou may have noticed the .gitignore file?\n\nOne crew member\n\nGo to your RStudio session, find the .gitignore file and click it\n\nIt contains a list of files and folders, which should not end up at your git repo, i.e. files which should be ignored. An important aspect of working with GitHub is that GitHub is meant for code, not data! Let’s say we had a data and a data/_raw/, we could add those to the .gitignore file to avoid the data being included in our commits. Currently, files like .RData and folders like .Rproj.user should be listed in the .gitignore file, as these are unique to each of your own sessions and shouldn’t be committed to the repo.\n\nUpdate the .gitignore file with the suggestions above and get the updated version to GitHub (we’ll assume you know how to do that by now 😉).\n\n\n\n\nSummary\nYou have now gotten to play around with collaborative coding using git - Well done!\nIf you are more curious for more, please feel free to play around with a new file, edit commit/pull/push etc. Perhaps also take an extra look at the GitHub, explore and learn more 👍 We have also only just scratched the surface here, you can read much more here.\nIf you are handy with the terminal, it is a good idea to learn how to use git from the terminal. The Primer on git command line vs. RStudio shows the equivalent git commands of everything we have done in these exercises.\n\n\nGROUP ASSIGNMENT (Important, see: how to)\nYour assignment this time, will be to:\n\nGo to this post on “PCA tidyverse style” by Claus O. Wilke, Professor of Integrative Biology\nAgain, create a reproducible micro-report together, this time, where you do a code-along with either the data in the post, the gravier data, and/or any other dataset.\nUse your GitHub group repository to collaborate - you should all contribute. Again, make sure that you are using the “Visual” editor while working on this!\nYour hand in will be a link to your micro-report in your GitHub group repository\nNote, focus here is on the git learning objectives and doing a code-along, which is not a copy/paste of the code, but using it as inspiration to create your own nice concise tidy micro-report!\nMake sure to check the Assignment Guidelines\nAnd also follow the Course Code Styling\nHOW TO HAND IN: Go to http://github.com/rforbiodatascienceYY/groupXX, replace YY with year and groupXX with appropriate group number, then copy the link and paste it into an empty text (.txt) file. Hand in this text file. No need to zip your html file, since we are accessing it through the repository!",
    "crumbs": [
      "Course Labs",
      "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio"
    ]
  },
  {
    "objectID": "lab08.html",
    "href": "lab08.html",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "",
    "text": "Package(s)",
    "crumbs": [
      "Course Labs",
      "Lab 8: Creating a Simple R-package"
    ]
  },
  {
    "objectID": "lab08.html#schedule",
    "href": "lab08.html#schedule",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap of Lab 7\n08.15 - 08.45: Lecture\n08.45 - 09.00: Break\n09.00 - 12.00: Exercises",
    "crumbs": [
      "Course Labs",
      "Lab 8: Creating a Simple R-package"
    ]
  },
  {
    "objectID": "lab08.html#learning-materials",
    "href": "lab08.html#learning-materials",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nBook: R Packages (2e): Welcome!\nBook: R Packages (2e): Introduction\nBook: R Packages (2e): Chapter 1 The Whole Game\nVideo: Building R packages with devtools and usethis | RStudio (This is optional and is basically a video walk-through of the reading material)",
    "crumbs": [
      "Course Labs",
      "Lab 8: Creating a Simple R-package"
    ]
  },
  {
    "objectID": "lab08.html#learning-objectives",
    "href": "lab08.html#learning-objectives",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nPrepare a simple R package for distributing documented functions\nExplain the terms Repository, Dependencies, and Namespace\nImplement testing in an R package\nCollaboratively work on an R package on GitHub",
    "crumbs": [
      "Course Labs",
      "Lab 8: Creating a Simple R-package"
    ]
  },
  {
    "objectID": "lab08.html#sec-exercises",
    "href": "lab08.html#sec-exercises",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Exercises",
    "text": "Exercises\nRead the steps of this exercises carefully while completing them\n\nIntroduction\nFirst, make sure to read and discuss the feedback you got from last week’s assignment!\nThe aim of these exercises is to set up a collaborative coding project using GitHub and collaborate on creating a simple R package that replicates the central dogma of molecular biology.\nThe exercises will build upon what you learned in Lab 7 and its exercises. But don’t worry too much, the setup steps will be given here as well.\n\n\n\nTASKS\nIf you haven’t read the Primer on R packages yet, consider reading it now.\n\n\nTask 1 - Setting up the R package\nIn the first task, one team member (you decide who) will initiate a package project and push it to Github. Then, when instructed, the remaining group members will connect to that repository, and the real package building will begin.\n\nTask 1.1 - Create R Package\n\nGo to R for Bio Data Science RStudio Server and login.\nClick Create a project in the top left and choose New Directory.\nSelect R Package and pick a fitting Package name.\n\nThe package you will create will replicate the central dogma of molecular biology. Let that inspire you\nLook up naming rules here\nThe most important rule: _, -, and ' are not allowed.\n\nTick the “Create git repository” box.\nClick “Create project”.\n\nRStudio will now create an R project for you that contains all necessary files and folders.\n\nOpen the DESCRIPTION file and write a title for your package, a small description, and add authors.\n\n\nWhen adding authors, the format is:\n\n\nAuthors@R:\n  c(person(given = \"firstname\",\n           family = \"lastname\",\n           role = c(\"aut\", \"cre\"), # There must be a \"cre\", but there can only be one\n           email = \"your@email.com\"), \n    person(given = \"firstname\",\n           family = \"lastname\",\n           role = \"aut\",\n           email = \"your@email.com\"))\n\n\nCreate an MIT license (from the console) usethis::use_mit_license(\"Group name\")\nIf you didn’t tick the “Create git repository”: Run usethis::use_git(). If the “Git” tab is not in the lower left panel: reopen the R project.\n\n\nOptional setup steps (generally good, but not essential for this course)\n\n\n\nAdd a Readme file usethis::use_readme_rmd( open = FALSE ). You will do that in the Group Assignment later\n\n\nAdd a lifecycle badge: usethis::use_lifecycle_badge( \"Experimental\" )\n\n\nWrite vignettes: usethis::use_vignette(\"Vignette name\")\n\n\n\n\n\n\nTask 1.2 - Setup GitHub Repository for your group’s R package\nDuring the previous class, you have been using a combination of console and terminal to set up GitHub. This week, you will solely be using the console.\nStill only the first team member\n\nGo to https://github.com/rforbiodatascience24 and go to repositories.\nClick the green New button\nCreate a new repository called group_X_package. Remember to replace the X\nSelect rforbiodatascience24 as owner\nMake the repository Public\nClick the green Create repository button\nIn this new repository, click the settings tab\nClick Collaborators and Team\nClick the green Add people button\nInvite the remaining group members\nAll other team members should now have access to the repository, but do not create your own project yet.\n\n\n\nTask 1.3 - Connect your RStudio Server Project to the GitHub Repository\nStill only the first team member\n\nFind your PAT key or create a new\n\n\nHow to create a new one\n\n\n\nType in the console usethis::create_github_token(). You’re going to be redirected to GitHub website.\n\nIn case you did that step manually, remember to give permission to access your repositories.\n\nYou need to type your password. Don’t change the default settings of creating the token except for the description – set ‘RStudio Server’ or something similar. Then hit Generate token.\n\nCopy the generated token (should start with ghp_) and store it securely (e.g., in a password manager). Do not keep it in a plain file.\n\n\nGo back to the RStudio Server project\nType in the console gitcreds::gitcreds_set() and paste the PAT key\nStage all files (in the git window in the top right) by ticking all boxes under Staged\nStill in the console, run the following commands. Replace your group number and use your GitHub username and email. You run these to link your project to the GitHub repository you created. Remember to replace the X.\n\n\nusethis::use_git_remote(name = \"origin\", \"https://github.com/rforbiodatascience24/group_X_package.git\", overwrite = TRUE) # Remember to replace the X\nusethis::use_git_config(user.name = \"USERNAME\", user.email = \"USEREMAIL@EXAMPLE.ORG\")\ngert::git_commit_all(\"Package setup\")\ngert::git_push(\"origin\")\n\nAll other team members\nAfter your teammate has pushed to GitHub.\n\nIn the RStudio Server, create a new project based on the GitHub Repository just created by your teammate.\n\nClick Create a project in the top left.\nChoose Version Control and then Git.\nPaste in the repository URL: https://github.com/rforbiodatascience24/group_X_package.git. Remember to replace the X.\nGive the directory a fitting name and click Create Project.\n\nFind your PAT key or create a new\n\n\nHow to create a new one\n\n\n\nType in the console usethis::create_github_token(). You’re going to be redirected to GitHub website.\n\nIn case you did it manually, remember to give permission to access your repositories.\n\nYou need to type your password. Don’t change the default settings of creating the token except for the description – set ‘RStudio Server’ or something similar. Then hit Generate token.\n\nCopy the generated token (should start with ghp_) and store it securely (e.g. in password manager). Do not keep it in a plain file.\n\n\nGo back to the RStudio Server project\nType in the console gitcreds::gitcreds_set() and paste the PAT key\nStill in the console, run the following commands. Replace your GitHub username and email.\n\n\nusethis::use_git_config(user.name = \"USERNAME\", user.email = \"USEREMAIL@EXAMPLE.ORG\")\n\nIf RStudio at any point asks you to log in to GitHub, redo step 4 and 5.\nNow you are ready to work on the R package.\n\n\n\nTask 2 - Build the package\n\nEach team member should build and implement their own function. The code will be given, but you will be asked to come up with a name for your function and many of the variables, so discuss in the team what naming convention you want to use. Do you want to use snake_case (common in tidyverse) or camelCase (common in Bioconductor)? It doesn’t really matter, but be consistent.\n\n\nTask 2.1 - Incorporate data\nIn this task you will include the following codon table in your package.\nThis task should be done by one team member. The rest should follow along.\n\nBelow is a standard codon table. In the console, store the table in an R object with a name of your own choosing.\n\n\nc(\"UUU\" = \"F\", \"UCU\" = \"S\", \"UAU\" = \"Y\", \"UGU\" = \"C\",\n  \"UUC\" = \"F\", \"UCC\" = \"S\", \"UAC\" = \"Y\", \"UGC\" = \"C\",\n  \"UUA\" = \"L\", \"UCA\" = \"S\", \"UAA\" = \"_\", \"UGA\" = \"_\",\n  \"UUG\" = \"L\", \"UCG\" = \"S\", \"UAG\" = \"_\", \"UGG\" = \"W\",\n  \"CUU\" = \"L\", \"CCU\" = \"P\", \"CAU\" = \"H\", \"CGU\" = \"R\",\n  \"CUC\" = \"L\", \"CCC\" = \"P\", \"CAC\" = \"H\", \"CGC\" = \"R\",\n  \"CUA\" = \"L\", \"CCA\" = \"P\", \"CAA\" = \"Q\", \"CGA\" = \"R\",\n  \"CUG\" = \"L\", \"CCG\" = \"P\", \"CAG\" = \"Q\", \"CGG\" = \"R\",\n  \"AUU\" = \"I\", \"ACU\" = \"T\", \"AAU\" = \"N\", \"AGU\" = \"S\",\n  \"AUC\" = \"I\", \"ACC\" = \"T\", \"AAC\" = \"N\", \"AGC\" = \"S\",\n  \"AUA\" = \"I\", \"ACA\" = \"T\", \"AAA\" = \"K\", \"AGA\" = \"R\",\n  \"AUG\" = \"M\", \"ACG\" = \"T\", \"AAG\" = \"K\", \"AGG\" = \"R\",\n  \"GUU\" = \"V\", \"GCU\" = \"A\", \"GAU\" = \"D\", \"GGU\" = \"G\",\n  \"GUC\" = \"V\", \"GCC\" = \"A\", \"GAC\" = \"D\", \"GGC\" = \"G\",\n  \"GUA\" = \"V\", \"GCA\" = \"A\", \"GAA\" = \"E\", \"GGA\" = \"G\",\n  \"GUG\" = \"V\", \"GCG\" = \"A\", \"GAG\" = \"E\", \"GGG\" = \"G\")\n\n\nRun usethis::use_data(NAME_OF_YOUR_OBJECT, overwrite = TRUE, internal = TRUE)\nYou have now made the data available to our functions, but we also want to make it visible for our users.\n\nRun usethis::use_data(NAME_OF_YOUR_OBJECT, overwrite = TRUE).\n\nWrite a data manual (document the data).\n\nAll non-internal data should be documented in a data.R file in the R folder. Create it with usethis::use_r(\"data\")\nAdd the following scaffold to R/data.R and write a very brief description of the data (see an example here). Don’t spend a lot of time here.\n\n\n\n#' Title\n#' \n#' Description\n#' \n#' \n#' @source \\url{https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi?chapter=tgencodes#SG1}\n\"NAME_OF_YOUR_OBJECT\"\n\nNormally, you should also describe how the raw data was cleaned. You would do that in the file that opens after running usethis::use_data_raw( name = \"NAME_OF_YOUR_OBJECT\", open = TRUE ), but that is less relevant here, so we will skip that part.\nYour package now includes some data.\nRestart R, clean your Environment (small broom in the Environment tab), run the three lines of code from The Package Workflow section:\n\nrstudioapi::documentSaveAll()  # Saves all you files\ndevtools::document()  # Writes all your manuals for you\ndevtools::load_all()  # Simulates library(\"your package\"), allowing you to use your functions\n\nAnd run ?NAME_OF_YOUR_OBJECT. OBS For some unknown reason, this does not work on the current R version (4.3.1). Instead, you can click the “Build” tab next to the “Git” tab and then “install”. After installing and attaching, your manual should pop up. Try printing the object as well to see what it looks like print(NAME_OF_YOUR_OBJECT).\nPush your changes to GitHub. The other team members pull the changes to have the data available to you as well.\n\n\n\nTask 2.2 - Implement functions\nIn this task you will be working individually to implement a function each. If you are fewer in the team than the number of functions. The quickest to finish can implement the remaining, or separate them out as you see fit.\nIf you want an additional challenge, each team member can create their own Git branch gert::git_branch_checkout(\"NAME_OF_BRANCH\") and work there. When done, merge your branch with the main branch. That is a more clean and ‘proper’ workflow, but completely optional.\nIf you are in doubt what the underlying functions do, run ?function_name to get a hint about their purpose. OBS For some unknown reason, this does not work on the current R version (4.3.1). As mentioned with the data, you can install your package, and then load the manual. Also, remember the functions are replicating the central dogma. Let that inspire you, when naming the functions and variables. If you get stuck, ask your teammates about their functions.\nFunction five is a bit more involved. Do it together or help your teammate out if it causes problems.\nFor each function (found below), complete the following steps:\n\nLook carefully at your function. Choose a fitting name for it\nRun usethis::use_r(\"function_name\") to create an .R file for the function\nPaste in the function and rename all places it says name_me with fitting names\nClick somewhere in the function. In the toolbar at the very top of the page, choose Code and click Insert Roxygen Skeleton\nFill out the function documentation\n\nGive the function a title\nDo not write anything after @export\nThe parameters should have the format: @param param_name description of parameter\nThe return has the format: @return description of function output\nExamples can span multiple lines and what you write will be run, so make it runnable.\nImportant! Either fill out everything or delete what you don’t. Otherwise, the package check will fail (Do not delete @export for these functions).\n\nRun the three lines of codes from The Package Workflow section\n\n\nIf at this point, you get a warning that NAMESPACE already exists, delete it and try again.\n\n\nrstudioapi::documentSaveAll()  # Saves all you files\ndevtools::document()  # Writes all your manuals for you\ndevtools::load_all()  # Simulates library(\"your package\"), allowing you to use your functions\n\n\nView your function documentation with ?function_name\nDefining a test or a series of tests around your newly created function ensures future corrections will not yield undesired results. Create such a test for your function. Run usethis::use_test(\"function_name\") and write a test. Draw some inspiration from here or from running ?testthat::expect_equal.\n\nSkip this step for function five. Instead, write inline code comments for each chunk of code. Press Cmd+Shift+c / Ctrl+Shift+C to comment your currently selected line.\n\nRerun the three lines from The Package Workflow section and check that the package works with devtools::check()\n\n\n\nBriefly about check\n\ndevtools::check() will check every aspect of your package and run the tests you created. You will likely get some warnings or notes like ‘No visible binding for global variables’. They are often harmless, but, if you like, you can get rid of them as described here. The check will tell you what might cause problems with your package and often also how to fix it. If there are any errors, fix those. Warnings and notes are also good to address. Feel free to do that if any pops up.\n\n\nIf it succeeds without errors, push your changes to GitHub\n\nIf you decided to create branches review last week’s exercises for guidance\nAlways pull before pushing\nUse the RStudio GUI if you prefer\nRemember to pull first\nIf you chose to create your own branch, merge it with master/main.\n\n\n\n\nFunction one\n\n\nname_me1 &lt;- function(name_me2){\n  name_me3 &lt;- sample(c(\"A\", \"T\", \"G\", \"C\"), size = name_me2, replace = TRUE)\n  name_me4 &lt;- paste0(name_me3, collapse = \"\")\n  return(name_me4)\n}\n\n\n\n\nFunction two\n\n\nname_me1 &lt;- function(name_me2){\n  name_me3 &lt;- gsub(\"T\", \"U\", name_me2)\n  return(name_me3)\n}\n\n\n\n\nFunction three\n\n\nname_me1 &lt;- function(name_me2, start = 1){\n  name_me3 &lt;- nchar(name_me2)\n  codons &lt;- substring(name_me2,\n                      first = seq(from = start, to = name_me3-3+1, by = 3),\n                      last = seq(from = 3+start-1, to = name_me3, by = 3))\n  return(codons)\n}\n\n\n\n\nFunction four\n\nNAME_OF_YOUR_OBJECT refers to the codon table you stored in Task 2.\n\nname_me &lt;- function(codons){\n  name_me2 &lt;- paste0(NAME_OF_YOUR_OBJECT[codons], collapse = \"\")\n  return(name_me2)\n}\n\n\n\n\nFunction five\n\nThis function will be the first to use dependencies. As a reminder, a dependency is a package that your package depends on. In this case, it will be stringr and ggplot2. They are already installed, so you don’t need to do that. The best way to add these packages to your own is to first add them to your package dependencies with usethis::use_package(\"package_name\"). If you care about reproducibility, you can add min_version = TRUE to the function call to specify a required minimum package version of the dependency. Run usethis::use_package for both dependencies.\nFor the ggplot2 functions, we will use ggplot2::function_name everywhere a ggplot2 function is used. Also add @import ggplot2 to the function description (this is often done with ggplot2 because it has a lot of useful plotting functions with only rare name overlaps). If you want to be more precise, add @importFrom ggplot2 ggplot aes geom_col theme_bw theme instead. The same applies for stringr, but since we only use a few functions, add @importFrom stringr str_split boundary str_count to the function description.\n\nname_me1 &lt;- function(name_me2){\n  name_me3 &lt;- name_me2 |&gt;  \n    stringr::str_split(pattern = stringr::boundary(\"character\"), simplify = TRUE) |&gt;\n    as.character() |&gt; \n    unique()\n  \n  counts &lt;- sapply(name_me3, function(amino_acid) stringr::str_count(string = name_me2, pattern =  amino_acid)) |&gt;  \n    as.data.frame()\n  \n  colnames(counts) &lt;- c(\"Counts\")\n  counts[[\"Name_me2\"]] &lt;- rownames(counts)\n  \n  name_me4 &lt;- counts |&gt;  \n    ggplot2::ggplot(ggplot2::aes(x = Name_me2, y = Counts, fill = Name_me2)) +\n    ggplot2::geom_col() +\n    ggplot2::theme_bw() +\n    ggplot2::theme(legend.position = \"none\")\n  \n  return(name_me4)\n}\n\n\n\n\n\n\nTask 3 - Group discussion\n\nDescribe each function to each other in order - both what it does and which names you gave them and their variables.\nThe person(s) responsible for function five, describe how you added the two packages as dependencies.\nDiscuss why it is a good idea to limit the number of dependencies your package has. When can’t it always be avoided?\nDiscuss the difference between adding an @importFrom package function tag to a function description compared to using package::function(). Read this section if you are not sure or just want to learn more.",
    "crumbs": [
      "Course Labs",
      "Lab 8: Creating a Simple R-package"
    ]
  },
  {
    "objectID": "lab08.html#group-assignment-important-see-how-to",
    "href": "lab08.html#group-assignment-important-see-how-to",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "GROUP ASSIGNMENT (Important, see: how to)",
    "text": "GROUP ASSIGNMENT (Important, see: how to)\n\nFor this week’s group assignment, write a vignette (user guide) for your package (max 2 pages). The vignette should include a brief description of what the package is about and demonstrate how each function in the package is used (individually and in conjunction with each other). As a final section, discuss use cases for the package and what other functions could be included. Also include the main points from your discussion in Task 3.\nInclude a link to your group’s GitHub repository at the top of the vignette. Hand it in as a pdf in DTU Learn.\n\nCreate a vignette with usethis::use_vignette(\"your_package_name\").\nWhen you are done writing it, run devtools::build_vignettes().\nAt the top line of your vignette, change rmarkdown::html_vignette to rmarkdown::pdf_document\nRerun devtools::build_vignettes() - the created pdf-file in the doc folder is the document to hand it.\nLastly, duplicate the vignette as the GitHub README\n\nCreate a README with usethis::use_readme_rmd( open = TRUE ).\nCopy the content of the vignette Rmarkdown into the readme Keep the top part of the README as is. If you overwrote it anyway, change rmarkdown::pdf_document to rmarkdown::github_document.\nRun devtools::build_readme()\nPush the changes to GitHub.\n\n\n\nLast tip on packages\n\nNext week, I will introduce the golem package for building production-grade Shiny apps. However, I personally also use it to quickly get going with packages.\nWhen starting out with an R package, it may seem complicated with a lot of things to remember. golem remembers these things for you. When setting up your package / Shiny app with golem::create_golem(\"Name of your awesome package/app\"), it creates a dev folder with a few files listing all you need to get started with an R package.\nIt does also give you a lot of other things that you will rarely use, and it also sets up some basic structures for Shiny apps. These can simply be deleted if you are not also building a Shiny app (which you will next week).",
    "crumbs": [
      "Course Labs",
      "Lab 8: Creating a Simple R-package"
    ]
  },
  {
    "objectID": "lab09.html",
    "href": "lab09.html",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "",
    "text": "Package(s)",
    "crumbs": [
      "Course Labs",
      "Lab 9 Creating a Simple Shiny Application"
    ]
  },
  {
    "objectID": "lab09.html#schedule",
    "href": "lab09.html#schedule",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.30: Recap of Lab 8\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises",
    "crumbs": [
      "Course Labs",
      "Lab 9 Creating a Simple Shiny Application"
    ]
  },
  {
    "objectID": "lab09.html#learning-materials",
    "href": "lab09.html#learning-materials",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: Mastering Shiny by Hadley Wickham – Read Chapter 1 (Chapter 2 and 3 are good to read as well, if you want).\nBook: Engineering Production-Grade Shiny Apps – Read Chapter 2 - 5 (they are fairly short, but if you don’t find Shiny Apps super cool, feel free to skip Chapter 3 and 5 and Sections 2.2.2 and 4.2.3 - 4.2.4), the rest are quite important for the exercises.\nCheatsheet: Shiny – This cheatsheet is a bit cluttered, but useful\nCheatsheet: Golem – Look through this after reading the chapters in “Engineering Production-Grade Shiny Apps” - the exercises will remind you to look at the cheatsheet as well.\n\nNote: The following are suggested learning materials, i.e., do not go over everything, but poke around. You will use these materials as a point of reference for the group exercises\n\nShiny Input Gallery\nWeb: Shiny from RStudio\nWeb: RStudio tutorials on Shiny\nVideo: Playlist: Web Apps in R: Building your First Web Application in R | Shiny Tutorial\nExample: nnvizRt\nMore inspiration: Shiny Gallery\n\nUnless explicitly stated, do not do the per-chapter exercises in the R4DS2e book",
    "crumbs": [
      "Course Labs",
      "Lab 9 Creating a Simple Shiny Application"
    ]
  },
  {
    "objectID": "lab09.html#learning-objectives",
    "href": "lab09.html#learning-objectives",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nPrepare a simple shiny application\nUsing relevant online resources to autonomously identify and obtain new and expand on existing knowledge of R",
    "crumbs": [
      "Course Labs",
      "Lab 9 Creating a Simple Shiny Application"
    ]
  },
  {
    "objectID": "lab09.html#sec-exercises",
    "href": "lab09.html#sec-exercises",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Exercises",
    "text": "Exercises\n\nGetting Started\nFirst, make sure to read and discuss the feedback you got from last week’s assignment!\nThen, for this particular session, we will make use of Posit’s free Cloud infrastructure. Setting it up is quite easy:\n\nGo to posit Cloud\nClick the blue GET STARTED button\nUnder Cloud Free, click the blue Learn more button\nClick the long blue Sign Up button\nEnter your information for setting up an account\nMake sure to Verify Your Email as instructed\nThen return to where you set up the account and click the blue Continue button\n\nCongratulations! You have now created your personal Posit cloud account. You will land in Your Workspace. Here you will have to create a new project, which you will use to work on the exercises today, so please:\n\nIn the upper right corner, click the blue New Project button\nChoose New RStudio Project\nYour project will be prepared and then deployed and you will land in the familiar RStudio IDE. Next to Your Workspace in the upper left corner, click where it says Untitled Project and name your project, e.g. R4BDS Shiny Exercises\n\nNow, let us do some quick setup:\n\nClick Tools and then Global Options...\nUnder Workspace, untick Restore .RData into workspace on startup\nNext to Save workspace to .RData on exit:, choose Never\nIn the lower right corner, click Apply\nNow on the left, find and click Pane Layout and click Environment, History, Conne and choose Console\nIn the lower right corner, click Apply\n\nOptionally you can adjust the visual appearance:\n\nNow on the left, find and click Appearance\nUnder Editor theme:, click a few options and see if you can find something you like\nIf you want to return to the default, simply find and click Textmate (default)\n\nOptionally, you can add a third column enabling you to e.g. have two Quarto Documents / Scripts / Text files open simultaneously:\n\nOn the left, find and click Pane Layout\nClick Add Column\nNote, this is best suited for wide desktop screens, so if you change your mind, simply click Remove Column\n\nOnce you are done, click OK to close the Options\n\n\nInitial “Hello Shiny World”\nIMPORTANT: If the Posit Cloud “hangs” for some reason, look in the upper right corner, there you will find a circle with three dots, click it and choose Relaunch Project. Also, you have limited ram, so in case you clutter your work environment, you may have to run a rm(list = ls()) to clear your ram\nNow, let us get started on Shiny! First, find the console and enter:\n\nlibrary(\"shiny\")\n\nYou are going to get an error message, make sure to read it, so you can fix it.\n\n\n\nClick here for hint\n\n\nR does not know what you are asking for here. You are missing “something”, how can we install “something” we are missing? Note, here on the posit cloud you are allowed to install tools you need\n\nOnce you have fixed the error, in the console, run:\n\nlibrary(\"shiny\")\nrunExample(\"01_hello\")\n\nYou may get a Popup Blocked message, if so, simply click Try Again. The you should see:\n\nNow, this is the frontend of a real life Shiny app. You can try to alter the Number of bins: and see what happens.\nThere are a multitude of such built in examples.\n\nTask: Try to play around with a few of the following to get an initial feel for what you can do with Shiny and discuss what you see in your group:\n\n\nrunExample(\"01_hello\")      # a histogram\nrunExample(\"02_text\")       # tables and data frames\nrunExample(\"03_reactivity\") # a reactive expression\nrunExample(\"04_mpg\")        # global variables\nrunExample(\"05_sliders\")    # slider bars\nrunExample(\"06_tabsets\")    # tabbed panels\nrunExample(\"07_widgets\")    # help text and submit buttons\nrunExample(\"08_html\")       # Shiny app built from HTML\nrunExample(\"09_upload\")     # file upload wizard\nrunExample(\"10_download\")   # file download wizard\nrunExample(\"11_timer\")      # an automated timer\n\n\n\nYour First Shiny App\nNow, let us build the 01_hello-app from scratch!\n\nGo to your Posit Cloud session and click File \\(\\rightarrow\\) New File \\(\\rightarrow\\) R Script\nNow, click File \\(\\rightarrow\\) Save\nName the file 01_hello.R\nCopy/paste the code below here into the empty file and again remember to save\nThen at the menu just above your open file, click Run App or use the console: runApp(\"PATH_TO_MY_APP\")\nCheck if it works and compare with the 01_hello-example\n\nImportant: This is an R-script file NOT a Quarto document! Think of it as if you take all the code boxes and only include those in your file.\n\n\n\nCode for your first app\n\n\n\n# Load the Shiny library\nlibrary(\"shiny\")\n\n# Define the User Interface (Frontend)\nui &lt;- fluidPage(\n  titlePanel(\"Hello Shiny!\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(inputId = \"bins\",\n                  label = \"Number of bins:\",\n                  min = 1,\n                  max = 50,\n                  value = 30)\n    ),\n    mainPanel(\n      plotOutput(outputId = \"distPlot\")\n    )\n  )\n)\n\n# Define the Server (Backend)\nserver &lt;- function(input, output) {\n  output$distPlot &lt;- renderPlot({\n    x &lt;- faithful$waiting\n    bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n    hist(x, breaks = bins, col = \"#75AADB\", border = \"white\",\n         xlab = \"Waiting time to next eruption (in mins)\",\n         main = \"Histogram of waiting times\")\n  })\n}\n\n# Launch the shiny app\nshinyApp(ui = ui, server = server)\n\n\n\nTask: In your group, compare the code with the below illustration of the anatomy of your first shiny app. Pay specific attention to the input- and output-variables and how the inputId = \"bins\" end up in input$bins. This front-to-back-end communication is central!\n\n\n\nTask: To further understand what is going on, we can ask constructively for input. Let us use your favourite LLM-technology as a sparring partner, e.g. you can cope/paste the following into a chatGPT prompt:\n\nI am following the university course \"R for Bio Data Science\".\nToday we are learning about Shiny, which is completely new to me.\nThe first thing we have done is to re-create a small shiny app,\nwhich is part of the examples in the Shiny-package. Moreover,\nit is the example which can be seen by running the code:\n  \n`runExample(\"01_hello\")`\n\nNow, we have manually implemented the app, but I am not certain\nof how it works and the details. In the following, I will\npaste the code from the app. Please in details, minding that I\nam a begginer, explain the details of the code:\n\nPASTE THE APP CODE HERE\n\nTask: Furthermore, let us also get some input on how the illustration above maps to the code. Right-click the “Shiny” image above and save it to disk, then copy/paste the following into the same prompt:\n\nThe lecturer has given me this graphical illustration,\nbut I am not sure I understand how it maps to the code,\ncan you please explain?\n\nREMEMBER TO UPLOAD THE IMAGE\n  \n\nTask: Now, look at the answers you got, if something is unclear, ask for an elaboration, e.g.:\n\nI have not seen the terms \"user interface\" or \"frontend\"\nor \"backend\" before. Please briefly explain, what these\nmean and how they are related to building shiny apps  \nNote here, how we using the LLM-technology productively as a sparring-partner. Instead of asking it to produce code, we ask it to explain existing code and to elaborate on any unclear specifics\n\n\nYour Second Shiny App\nWe will continue where we left of last week. To make sure, that everyone is working with the same functions, we will use the functions outlined below here. Remember the task here is to build an interface, which allow people without coding skills to interact with data.\nLet’s get started!\n\n\nClick here for functions\n\n\nPlease note, that as an example of reducing dependencies, these functions are implemented using base R. In case you are not familiar, here again is a prime example of using LLM-technology such as e.g. chatGPT as a sparring partner. Try to submit the following prompt:\nI am not familiar with base R, I only know tidyverse R.\nFor the following function and R-code, please explain\nin detail how the used base R functions work and what\nthe code in its entirety does:\n\nPASTE THE BASE R CODE OF A FUNCTION HERE\n\n\nVirtual Gene\n\n\n\n# Virtual gene\ngene_dna &lt;- function(length, base_probs = c(0.25, 0.25, 0.25, 0.25)){\n  if( length %% 3 != 0 ){\n    stop(\"The argument to the parameter 'l' has to be divisible by 3\")\n  }\n  dna_vector &lt;- sample(\n    x = c(\"A\", \"T\", \"C\", \"G\"),\n    size = length,\n    replace = TRUE,\n    prob = base_probs)\n  dna_string &lt;- paste0(\n    x = dna_vector,\n    collapse = \"\")\n  return(dna_string)\n}\n\n\n\n\n\nVirtual RNA Polymerase\n\n\n\n# Virtual RNA polymerase\ntranscribe_dna &lt;- function(dna){\n  rna &lt;- gsub(\n    pattern = \"T\",\n    replacement = \"U\",\n    x = dna)\n  return(rna)\n}\n\n\n\n\n\nVirtual Ribosome\n\n\n\n# Virtual Ribosome\ntranslate_rna &lt;- function(rna){\n  if( is.null(rna) || rna == \"\" ){ return(\"\") }\n  l &lt;- nchar(x = rna)\n  firsts &lt;- seq(\n    from = 1,\n    to = l,\n    by = 3)\n  lasts &lt;- seq(\n    from = 3,\n    to = l,\n    by = 3)\n  codons &lt;- substring(\n    text = rna,\n    first = firsts,\n    last = lasts)\n  codon_table &lt;- c(\n    \"UUU\" = \"F\", \"UCU\" = \"S\", \"UAU\" = \"Y\", \"UGU\" = \"C\",\n    \"UUC\" = \"F\", \"UCC\" = \"S\", \"UAC\" = \"Y\", \"UGC\" = \"C\",\n    \"UUA\" = \"L\", \"UCA\" = \"S\", \"UAA\" = \"_\", \"UGA\" = \"_\",\n    \"UUG\" = \"L\", \"UCG\" = \"S\", \"UAG\" = \"_\", \"UGG\" = \"W\",\n    \"CUU\" = \"L\", \"CCU\" = \"P\", \"CAU\" = \"H\", \"CGU\" = \"R\",\n    \"CUC\" = \"L\", \"CCC\" = \"P\", \"CAC\" = \"H\", \"CGC\" = \"R\",\n    \"CUA\" = \"L\", \"CCA\" = \"P\", \"CAA\" = \"Q\", \"CGA\" = \"R\",\n    \"CUG\" = \"L\", \"CCG\" = \"P\", \"CAG\" = \"Q\", \"CGG\" = \"R\",\n    \"AUU\" = \"I\", \"ACU\" = \"T\", \"AAU\" = \"N\", \"AGU\" = \"S\",\n    \"AUC\" = \"I\", \"ACC\" = \"T\", \"AAC\" = \"N\", \"AGC\" = \"S\",\n    \"AUA\" = \"I\", \"ACA\" = \"T\", \"AAA\" = \"K\", \"AGA\" = \"R\",\n    \"AUG\" = \"M\", \"ACG\" = \"T\", \"AAG\" = \"K\", \"AGG\" = \"R\",\n    \"GUU\" = \"V\", \"GCU\" = \"A\", \"GAU\" = \"D\", \"GGU\" = \"G\",\n    \"GUC\" = \"V\", \"GCC\" = \"A\", \"GAC\" = \"D\", \"GGC\" = \"G\",\n    \"GUA\" = \"V\", \"GCA\" = \"A\", \"GAA\" = \"E\", \"GGA\" = \"G\",\n    \"GUG\" = \"V\", \"GCG\" = \"A\", \"GAG\" = \"E\", \"GGG\" = \"G\")\n  protein &lt;- paste0(\n    x = codon_table[codons],\n    collapse = \"\")\n  return(protein)\n}\n\n\n\n\n\nSimple base counts\n\n\n\n# Simple base counts\nbase_freqs &lt;- function(dna){\n  if (is.null(dna) || dna == \"\" ){\n    return( data.frame(dna_vec = factor(c(\"A\", \"C\", \"G\", \"T\")),\n                       Freq = c(0, 0, 0, 0)) ) }\n  dna_vec &lt;- strsplit(x = dna,\n                      split = \"\")\n  base_counts &lt;- table(dna_vec)\n  return( as.data.frame.table(base_counts) )\n}\n\n\n\n\n\n\nBuilding a Virtual Gene Generator\n\n\nVersion 1, the basics\nBelow here, you will find some boilerplate code, which is based on what you saw in the 01_hello-example.\n\nTask: Look at this code and compare it to your first shiny app, discuss the changes in your group, what do you see?\nTask: Create your second Shiny App in a new file virtual_gene_generator.R, copy/paste making sure to insert the code needed for the function gene_dna() (See code for functions above)\nTask: Run the App and in your group, discuss how it works\n\n\nlibrary(\"shiny\")\n\n# Define the \"Virtual Gene\"-function (See code for functions above)\ngene_dna &lt;- ...\n\n# Define the User Interface (Frontend)\nui &lt;- fluidPage(\n  titlePanel(\"Virtual Gene Generator\"),\n  sidebarLayout(\n    sidebarPanel(\n      sliderInput(inputId = \"n_bases\",\n                  label = \"Number of bases:\",\n                  min = 1,\n                  max = 60,\n                  value = 30)\n    ),\n    mainPanel(\n      textOutput(outputId = \"dna\")\n    )\n  )\n)\n\n# Define the Server (Backend)\nserver &lt;- function(input, output) {\n  output$dna &lt;- renderText({\n    gene_dna(length = input$n_bases)\n  })\n}\n\n# Launch the shiny app\nshinyApp(ui = ui, server = server)\n\n\n\nVersion 2, expanding\nNow, if you look at the code for the gene_dna-function, it has a second parameter base_probs with the default argument c(0.25, 0.25, 0.25, 0.25). Let us now expand, so your app allows the user to define the individual base probabilities.\n\nTask: Expand the ui with one numericInput() for each of the four bases defining the probability of drawing that base and pass those numbers onto the server\n\n\n\n\nNeed a hint?\n\n\nRecall, that you in the console can type ?numericInput and get help on the function. Try to do so and compare with the sliderInput() you already have defined in your app.\nTip: Start with just one probability and then hardcode the other 3 to e.g. 0.25\n\n\n\n\nNeed a hint more?\n\n\nThe inputId defines the “name” of what is passed to the server, e.g. if you look at the sliderInput() it uses inputId = \"n_bases\", which is then passed to the server as input$n_bases\n\n\n\n\nNeed just one hint more?\n\n\nIn your ui, you have to find room for:\n\nnumericInput(inputId = \"prob_A\",\n             label = \"Probability of A\",\n             value = 0.25,\n             min = 0,\n             max = 1)\n\nIn your server you have to find room for:\n\nbase_probs = c(input$prob_A, 0.25, 0.25, 0.25)\n\nand then figure out how to expand that to also include prob_T, prob_C and prob_G\n\nOnce you are up and running, your app should look something like this:\n\n\nTask: Update the gene_dna()-function in your app to check if the probabilities sum to 1 and make sure output a meaningful error message, if they do not\n\n\n\n\nNeed a hint?\n\n\nThe gene_dna()-function already checks it the length of the DNA is divisible by 3, use this as inspiration to add your own check. If you are uncertain on how to compute the sum() of a vector in R, try to google a bit…\n\n\n\n\nA bit more advanced hint\n\n\nActually, you should be careful when comparing numbers when you create apps or any analysis code. If you sum a large vector, you might experience, that even when the sum should be equal to 1, it will fail a direct comparison, such as sum(p) == 1. This has to do with internal representation of numbers in a computer resulting in small rounding errors. Therefore, it is safer to use e.g.: abs(sum(p) - 1) &lt; 1e-10, i.e. is the difference between my sum and the number 1 smaller than some very small number.\n\n\n\n\nControlling the Layout\nBy now your base app is up and running, but you may want to tinker with the layout of the user interface. We can do so using the card()-function from the bslib-package:\n\n\nClick to expand app code\n\n\n\n# Define the User Interface (Frontend)\nui &lt;- page_fluid(\n  layout_columns(\n    col_widths = 12,\n    card(\n      titlePanel(\"Virtual Central Dogma\"),\n      style = \"background-color: #f0f0f0; padding: 15px;\"\n    )),\n  layout_columns(\n    col_widths = 12,\n    card(\n      titlePanel(\"About\"),\n      helpText(\"Describe what your app does...\")\n    )),\n  layout_columns(\n    col_widths = 12,\n    card(\n      card_header(\"Virtual Gene Generator\"),\n      sliderInput(inputId = \"n_bases\",\n                  label = \"Number of bases:\",\n                  min = 1,\n                  max = 60,\n                  value = 30,\n                  width = \"100%\"),\n      layout_columns(\n        col_widths = c(3, 3, 3, 3),\n        numericInput(inputId = \"prob_A\",\n                     label = \"Probability of A\",\n                     value = 0.25,\n                     min = 0,\n                     max = 1,\n                     step = 0.1),\n        numericInput(inputId = \"prob_T\",\n                     label = \"Probability of T\",\n                     value = 0.25,\n                     min = 0,\n                     max = 1,\n                     step = 0.1),\n        numericInput(inputId = \"prob_C\",\n                     label = \"Probability of C\",\n                     value = 0.25,\n                     min = 0,\n                     max = 1,\n                     step = 0.1),\n        numericInput(inputId = \"prob_G\",\n                     label = \"Probability of G\",\n                     value = 0.25,\n                     min = 0,\n                     max = 1,\n                     step = 0.1)\n    ))),\n  layout_columns(\n    col_widths = 12,\n    card(\n      card_header(\"Virtual Gene output\"),\n      mainPanel(\n        verbatimTextOutput(outputId = \"dna\")\n      )\n    ))\n)\n\n\n\nTask: Update your app using the above card()-function layout. Remember to load the bslib-package.\n\n\n\nIt’s Complicated…\nAs you have probably realised by now, the final app can be rather many lines of code defining the user interface and likewise for the server and also associated functions. To address this, it is recommendable to divide your app into seperate files mimicking the illustration of the anatomy of a Shiny app. To do so, we will need four files:\n\nThe functions: app_functions.R\nThe user interface: ui.R\nThe server: server.R\nThe app: virtual_central_dogma.R\n\nThe latter will look something like this:\n\n# Load needed libraries\nlibrary(\"PACKAGE_NAME\")\n\n# Load needed functions\nsource(file = \"app_functions.R\")\n\n# Run the frontend user interface ui.R\nsource(file = \"ui.R\")\n\n# Run the backend server server.R\nsource(file = \"server.R\")\n\n# Run the Shiny app\nshinyApp(ui = ui, server = server)\n\n\nTask: First make sure, that your app runs as is and then create these four files and copy/paste the approproiate code into each of the files\n\n\n\nCompleting the Central Dogma\nSo far you have just been working with the gene_dna()-function, but we naturally have to also include:\n\nTranscription\nTranslation\nA bit of analysis\n\nIf you scroll a bit back, you were given some pre-built code for the following functions:\n\ngene_dna()\ntranscribe_dna()\ntranslate_rna()\nbase_freqs()\n\nRight then:\n\nTask: Expand your app to include the central dogma functions gene_dna(), transcribe_dna() and translate_rna(). Note that any further analysis, such as e.g. base_freqs() or similar is optional\n\nA bit of help to get you started:\n\nYou can find an overview of the different input controls here\nUse manual copy/paste when transferring DNA to RNA to Protein (Naturally, this can be done automatically, but that’s a completely different can of worms)\nPerhaps you could look at the 01_hello-example to get some inspiration on how you can include a ggplot of your nucleotide frequency analysis\n\n\n\nGetting Your Shiny App LIVE!\nNote, when we publish our app, each file will be “build” and this will create errors if some functions are not found. Therefore, make sure to include library(\"PACKAGE_NAME\") in each files for all needed libraries and source the app_functions.R file where it’s needed too.\n\nGo to shinyapps.io\nLog in or create an account\nGo back to your RStudio session and launch your Shiny App\nIn the upper right corner, it’ll say “Publish”\nYou may get prompted to install some packages, so go ahead and do so\nYou will then get prompted to “Connect Publishing Account”, click “Next”\nChoose your “ShinyApps.io” account\nFollow the outlined connect procedure\nChoose a “Title” for your app\nClick “Publish”\n\nAfter some automated build procedures, your Shiny App is LIVE - CONGRATULATIONS!\nNB! If you an error saying:\n\n“An error has occurred! An error has occurred. Check your logs or contact the app author for clarification.”\n\nThen likely, there is a problem withs paths or libraries, did you remember to start this last part as a separate project? Also, did you remember to include library definitions and source the app_functions.R-file in all the necessary files?",
    "crumbs": [
      "Course Labs",
      "Lab 9 Creating a Simple Shiny Application"
    ]
  },
  {
    "objectID": "lab09.html#group-assignment-important-see-how-to",
    "href": "lab09.html#group-assignment-important-see-how-to",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "GROUP ASSIGNMENT (Important, see: how to)",
    "text": "GROUP ASSIGNMENT (Important, see: how to)\nFor this week’s assignment you have to:\n\nCreate a new Github repository, name it group_XX_shiny, where XX is your group name, e.g. 06\nFinish the Central Dogma shiny app by Github collaborating (Psst… Perhaps branching and delegate specific cards to group members would be prudent)\nPublish your finished shiny app to shinyapps.io (See How to Publish from Posit Cloud)\nPlace the link to your published app in the README of the repository\nFor your assignment hand in, simply create a text file containing the link to your group Github repository containing your shiny app\n\nYou app should contain the central dogma functions gene_dna(), transcribe_dna() and translate_rna(), but any further analysis, such as e.g. base_freqs() or similar is optional. Please feel free to style your app as you please. Also, when creating your app, try to use your favourite LLM-technology, BUT use it wisely, use it to EXPLAIN code, NOT to GENERATE code!",
    "crumbs": [
      "Course Labs",
      "Lab 9 Creating a Simple Shiny Application"
    ]
  },
  {
    "objectID": "lab09.html#how-to-publish-from-posit-cloud",
    "href": "lab09.html#how-to-publish-from-posit-cloud",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "How to Publish from Posit Cloud",
    "text": "How to Publish from Posit Cloud\nSince October 2024 publishing on Posit Cloud has been deprecated. Therefore, we will have to setup publishing via shinyapps.io:\n\nGo to your finished app on Posit Cloud\nClick Run App above your central_dogma_app.R-file and check that everything works as expected\nNow, click Tools \\(\\Rightarrow\\) Global Options... \\(\\Rightarrow\\) Publishing (in the left panel) \\(\\Rightarrow\\) Connect... in the upper right corner\nYou may get prompted for some packages, which needs installing, click Yes\nOnce they are installed, you will be prompted to Connect Account, click ShinyApps.io\nNow you will have to click on the your account on ShinyApps-link\nOnce on shinyapps.io, click Log In in the upper right corner (It’s the same login as to Posit Cloud)\nIf this is your first time to shinyapps.io, you will see something like “Hi! You must be new here…”, nevermind this\nIn the upper right corner, click your name and choose Tokens\nThen click the green + Add Tokens-button\nIt’s okay if you have en existing token, make sure to check the date-of-creation, which for the relevant token should be today and then click Show\nNow you will see a new box and you should be in the With R-tab, find the green Show secret-button and click it\nNow, click Copy to clipboard\nThen go back to your Posit Cloud tab and paste the token and code, i.e. all of it, it should start with rsconnect::setAccountInfo(...)\nThen still on the Posit Cloud, click Connect Account\nUnder Publishing Accounts, make sure you see id-name: shinyapps.io\nClick OK\nThen still on Posit Cloud, make sure you are in your central_dogma_app.R-file and then run the app\nClick the small triangle in the upper right corner of the your app and choose Other Destination...\nClick your shinyapps.io accout name\nClick Publish\nYour will get a message Deployment Started..., click OK - Your app will be deployed by rsconnect, this might take a few minutes, so be patient\nNow, back in your shinyapps.io session, on the left click Applications and then click All\nFind your app, likely named project and it will likely have status Running, and then click on the name of your app\nIn the OVERVIEW, find the URL and click it, this will be the public version of your app!\n\nPhheeww! Well done!",
    "crumbs": [
      "Course Labs",
      "Lab 9 Creating a Simple Shiny Application"
    ]
  },
  {
    "objectID": "lab10.html",
    "href": "lab10.html",
    "title": "Lab 10 Project Startup & Industry Talks",
    "section": "",
    "text": "Schedule",
    "crumbs": [
      "Course Labs",
      "Lab 10 Project Startup & Industry Talks"
    ]
  },
  {
    "objectID": "lab10.html#sec-symposium",
    "href": "lab10.html#sec-symposium",
    "title": "Lab 10 Project Startup & Industry Talks",
    "section": "Mini Symposium: Applications of R for Bio Data Science in Industry",
    "text": "Mini Symposium: Applications of R for Bio Data Science in Industry\nThis mini symposium on Applications of R for Bio Data Science in Industry offers participants a glimpse into how modern Bio Data Science is transforming value creation in the pharmaceutical and healthcare sectors.\nEach talk features a Bio Data Science professional sharing unique insights into real-world projects that apply cutting-edge Bio Data Science techniques, while also providing a glimpse into the career paths that have enabled their exciting work. Every talk will be followed by a Q&A session.\n\n2024\n\nCompanies Represented\n\nGubra\nQIAGEN\nUCPH Center for Health Data Science\n\n\n\nSymposium Programme\n\n09.00 – 09.25: “R Shiny Apps as Game-Changers in Preclinical Trials” by Abril Diosdado Hernández, Associate Research Scientist, Gubra\n09.25 – 09.30: Short break and switch\n09.30 – 09.55: “Accessing QIAGEN OmicSoft Data using the R API” by Ruth Stoney, Senior Field Applications Scientist, QIAGEN\n09.55 – 10.00: Short break and switch\n10.00 – 10.25: “From Student to Employee” by Helene Wegener, Junior Data Scientist, UCPH Center for Health Data Science\n10.25 – 10.35: Symposium round off\n\n\n\n\n2023\n\nCompanies Represented\n\nClinical Microbiomics\nGubra\nLundbeck\nNovo Nordisk\nZS\n\n\n\nSymposium Programme\n\n09.00 – 09:25: “Harnessing R Packages for Microbiome Analysis and Reporting” by Maria Novosolov, Data Scientist, Clinical Microbiomics\n09.25 – 09:50: “Plotly and Shiny apps for exploratory analysis of transcriptomics data” by Christina Bligaard Pedersen, Senior Bioinformatician & Mikhail Osipovitch, Lead Bioinformatician, ZS\n09.50 – 10:00: Break\n10.00 – 10:25: “An ADA monitoring tool developed with Rshiny” by Carlotta Porcelli, Senior Statistical Programmer, Novo Nordisk\n10.25 – 10:50: “The R in R&D efficiency” by Victor A. O. Carmelo, Principal Bioinformatician, Lundbeck\n10.50 – 11:00: Break\n11.00 – 11:25: “From data analysis to infrastructure: A journey with R” by Sebastian Tølbøl Thrane, Senior Scientist, Gubra\n11.25 – 11:50: “The long and winding road to use R in a pharmaceutical company” by Claus Dethlefsen, Statistical Director, Novo Nordisk\n11.50 – 12:00: Symposium round-off\n\n\n\n\n2022\n\nCompanies Represented\n\nAbzu\nBristol Myers Squibb\nClinical Microbiomics\nNovo Nordisk Bioinformatics\nNovo Nordisk Biostatistics\nSteno Diabetes Center Copenhagen\n\n\n\nSpeaker List\n\nAndrea Marquard, Head of Data Science and Automation, Clinical Microbiomics: “How we use internal R packages, and why you should learn to make one”\nAnders Gorst-Rasmussen, Statistical Director, Novo Nordisk Biostatistics, Novo Nordisk: “How we use R in Novo Nordisk Biostatistics”\nAnne-Mette Bjerregaard, Senior Scientist in Computational Biology, Novo Nordisk: “The Evolution of a Computational Biologist”\nSam Demharter, Bioinformatics Specialist, Abzu: “Explainable AI in Precision Medicine - Closing the Loop from Patient to Drug”\nLykke Pedersen, Head of RNA Therapeutics, Abzu: “Explainable AI in Precision Medicine - Closing the Loop from Patient to Drug”\nSimon Papillon-Cavanagh, Principal Scientist, Bristol Myers Squibb (USA): “A Career Built on Mistakes”\nSofie Olund Villumsen, Bioinformatician, Steno Diabetes Center Copenhagen: “Using R for Data Analysis in Clinical Studies”\nTarun Veer Singh Ahluwalia, Assoc. Prof., Steno Diabetes Center Copenhagen: “Using R for Data Analysis in Clinical Studies”\n\n\n\n\n2021\n\nCompanies Represented\n\nAbzu\nALK\nChr. Hansen\nLundbeck\nNovozymes\nTeraData\n\n\n\nSpeaker List\n\nSamuel Demharter, Senior Bioinformatician, Abzu\nLykke Pedersen, Senior Bioinformatician, Abzu\nMarie-Catherine Le Bihan, Senior Data Scientist, Chr. Hansen\nThomas Strantzl, Head of Bioinformatics, ALK\nMaria Dalby, Postdoctoral Researcher, Lundbeck\nThomas Poulsen, Science Manager, Novozymes\nMikkel Freltoft Krogsholm, Team lead, Senior Data Scientist, TeraData",
    "crumbs": [
      "Course Labs",
      "Lab 10 Project Startup & Industry Talks"
    ]
  },
  {
    "objectID": "primers.html",
    "href": "primers.html",
    "title": "Primers",
    "section": "",
    "text": "Various short introductions (primers) pertaining to relevant course items",
    "crumbs": [
      "Primers"
    ]
  },
  {
    "objectID": "primer_linear_models.html",
    "href": "primer_linear_models.html",
    "title": "Primer on Linear Models in R",
    "section": "",
    "text": "Example\nIn a basic linear regression model, one independent variable and one dependent variable are involved. The terms independent and dependent are literal meaning, that one variable does depend on the value of the other, whereas for the other the value is independent of the other. In a basic linear regression, we find the line that best fit the data. We will illustrate this, with the following example.",
    "crumbs": [
      "Primers",
      "Primer on Linear Models in `R`"
    ]
  },
  {
    "objectID": "primer_linear_models.html#example",
    "href": "primer_linear_models.html#example",
    "title": "Primer on Linear Models in R",
    "section": "",
    "text": "Background\nLet’s say we wanted to study the genetic mechanism protecting a plant from heat shock, then:\n\nIndependent: Environmental Condition (temperature)\nDependent: Gene Expression Level (related to heat shock protection)\n\nHere, the independent variable is the temperature and the dependent variable is the gene expression level. It is clear, that the temperature, does not rely on the gene expression level, but the gene expression level of heat shock related genes, does rely on the temperature.\nSo, we keep plants under different temperatures and collect samples, from which we can extract RNA and run a transcriptomics analysis uncovering gene expression levels.\n\n\nData\nFor the data here, we are going to simulate the relationship between gene expression levels and temperature, as a function in R:\n\nrun_simulation &lt;- function(temp){\n  measurement_error &lt;- rnorm(n = length(temp), mean = 0, sd = 3)\n  gene_expression_level &lt;- 2 * temp + 3 + measurement_error\n  return( gene_expression_level )\n}\n\nNote, how we’re adding some measurement error to our simulation, otherwise we would get a perfect relationship, which we all know never happens.\nNow, we can easily run simulations:\n\nrun_simulation(temp = c(15, 20, 25, 30, 35))\n\n[1] 33.03043 40.09036 49.12772 65.07048 70.11977\n\n\nLet’s just go ahead and create some data, we can work with. For this example, we take samples starting at 5 degree celsius and then in increments of 1 up to 50 degrees:\n\nset.seed(806017)\nexperiment_data &lt;- tibble(\n  temperature = seq(from = 5, to = 50, by = 1),\n  gene_expression_level = run_simulation(temp = temperature)\n)\nexperiment_data |&gt; \n  sample_n(10) |&gt; \n  arrange(temperature)\n\n# A tibble: 10 × 2\n   temperature gene_expression_level\n         &lt;dbl&gt;                 &lt;dbl&gt;\n 1          12                  28.7\n 2          13                  30.2\n 3          16                  35.6\n 4          19                  44.0\n 5          22                  48.8\n 6          27                  64.6\n 7          32                  61.1\n 8          33                  73.9\n 9          35                  76.7\n10          40                  84.9\n\n\n\n\nVisualising\nNow, that we have the data, we can visualise the relationship between the temperature- and gene_expression_level-variables:\n\nmy_viz &lt;- experiment_data |&gt; \n  ggplot(aes(x = temperature,\n             y = gene_expression_level)) +\n  geom_point() +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0)\nmy_viz\n\n\n\n\n\n\n\n\nNow, we can easily add the best fit line using the geom_smooth()-function, where we specify that we want to use method = \"lm\" and for now, we exclude the confidence interval, by setting se = FALSE:\n\nmy_viz +\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n\n\n\n\n\n\n\n\nWhat happens here, is that a best-fit line is added to the plot by calculating the line, such that the sum of the squared errors is as small as possible, where the error is the distance from the line to a given point. This is a basic linear regression and is known as Ordinary Least Squares (OLS). But what if we want to work with this regression model, beyond just adding a line to a plot?\n\n\nModelling\nOne of the super powers of R is the build in capability to do modelling. Because we simulated the data (see above), we know that the true intercept is 3 and the true slope of the temperature variable is 2. Let see what we get, if we run a linear model:\n\nmy_lm_mdl &lt;- lm(formula = gene_expression_level ~ temperature,\n   data = experiment_data)\nmy_lm_mdl\n\n\nCall:\nlm(formula = gene_expression_level ~ temperature, data = experiment_data)\n\nCoefficients:\n(Intercept)  temperature  \n      2.816        2.021  \n\n\nImportant, the formula notation gene_expression_level ~ temperature is central to R and should be read as: “gene_expression_level modelled as a function of temperature”, i.e. gene_expression_level is the dependent variable often denoted y and temperature is the independendt variable often denoted x.\nOkay that’s pretty close! Recall the reason for the difference is, that we are adding measurement error, when we run the simulation (see above).\nIn other words our model says, that:\n\\[gene\\_expression\\_level = 2.816 + 2.021 \\cdot temperature\\]\nI.e. the estimate of the intercept is 2.816 and the estimate of the slope is 2.021, meaning that when the temperature = 0, we estimate that the gene_expression_level is 2.816 and for each 1 degree increase in temperature, we estimate, that the increase in gene_expression_level is 2.021.\nThese estimates are pretty close to the true model underlying our simulation:\n\\[gene\\_expression\\_level = 3 + 2 \\cdot temperature\\]\nIn general form, such a linear model can be written like so:\n\\[y = \\beta_{0} + \\beta_{1} \\cdot x_{1}\\]\nWhere the \\(\\beta\\)-coefficients are termed estimates, because that is exactly what we do, given the observed data, we estimate their values.\n\n\nWorking with a lm-object:\nThe model format you saw above, is a bit quirky, but luckily, there is a really nice way to get these kind of model object into a more tidy-format:\n\nlibrary(\"broom\")\nmy_lm_mdl |&gt; \n  tidy()\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)     2.82    1.16        2.44 1.89e- 2\n2 temperature     2.02    0.0378     53.4  1.16e-41\n\n\nBriefly, here we term, estimate, std.error, statistic and p.value. We discussed the term and estimate. The std.error pertains to the estimate and the statistic is used to calculate the p.value.\n\nThe P-value\nNow, because we now have a tidy object, we can simply plug-‘n’-play with other tidyverse tools, so let us visualise the p.value. Note, because of the often vary large differences in p.values, we use a -log10-transformation, this means that larger values are “more significant”. Below, the dashed line signifies \\(p=0.05\\), so anything above that line is considered “statistically significant”:\n\nmy_lm_mdl |&gt; \n  tidy() |&gt; \n  ggplot(aes(x = term,\n             y = -log10(p.value))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05),\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\nNow, as mentioned the p-values are computed based on the statistic and are defined as: “The probability of observing a statistic as or more extreme given, that the null-hypothesis is true”. Where the null-hypothesis it that there is no effect, i.e. the estimate for the term is zero.\nFrom this, it is quite clear, that there very likely is a relationship between the gene_expression_level and temperature. In fact, we know there is, because we simulated the data.\n\n\nThe Confidence Intervals\nWe can further easily include the confidence intervals of the estimates:\n\nmy_lm_mdl_tidy &lt;- my_lm_mdl |&gt; \n  tidy(conf.int = TRUE,\n       conf.level = 0.95)\nmy_lm_mdl_tidy\n\n# A tibble: 2 × 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     2.82    1.16        2.44 1.89e- 2    0.488      5.14\n2 temperature     2.02    0.0378     53.4  1.16e-41    1.94       2.10\n\n\n…and as before easily do a plug’n’play into ggplot:\n\nmy_lm_mdl_tidy |&gt; \n  ggplot(aes(x = estimate,\n             y = term,\n             xmin = conf.low,\n             xmax = conf.high)) +\n  geom_errorbarh(height = 0.1) +\n  geom_point()\n\n\n\n\n\n\n\n\nNote, what the 0.95 = 95% confidence intervals means is that: “If we were to repeat this experiment 100 times, then 95 of the times, the generated confidence interval would contain the true value”.\n\n\n\nSummary\nWhat we have gone through here is a basic linear regression, where we are aiming to model the continuous variable gene_expression_level as a function of yet another continous variable temperature. We simulated data, where the true intercept and slope were 3 and 2 respectively and by fitting a linear regression model, the estimates of the intercept and slope respectively were 2.82 [0.49;5.14] and 2.02 [1.94;2.10].\nLinear models allow us to gain insights into data, by modelling relationships.",
    "crumbs": [
      "Primers",
      "Primer on Linear Models in `R`"
    ]
  },
  {
    "objectID": "primer_logistic_regression.html#logistic-regression",
    "href": "primer_logistic_regression.html#logistic-regression",
    "title": "Primer on Logistic Regression in R",
    "section": "",
    "text": "Data\nAs before, let’ us simply simulate some data and let’s say that we have 100 plants at 50 degrees celsius and the mean gene_expression_level for plants, which did not survive (denoted 0) was 90 and for those who did survive (denoted 1), it was 100:\n\nset.seed(718802)\nmean_survived_no &lt;- 90\nmean_survived_yes &lt;- 100\nsurvival_data &lt;- tibble(\n  gene_expression_level = c(rnorm(n = 50,\n                                  mean = mean_survived_no,\n                                  sd = 3),\n                            rnorm(n = 50,\n                                  mean = mean_survived_yes,\n                                  sd = 3)),\n  survived = c(rep(x = 0, times = 50), rep(x = 1, times = 50))\n)\nsurvival_data |&gt; \n  sample_n(10)\n\n# A tibble: 10 × 2\n   gene_expression_level survived\n                   &lt;dbl&gt;    &lt;dbl&gt;\n 1                  95.3        0\n 2                  96.6        1\n 3                 104.         1\n 4                  93.6        0\n 5                  95.0        0\n 6                  91.8        0\n 7                  90.2        0\n 8                  88.2        0\n 9                  86.2        0\n10                  89.3        0\n\n\n\n\nVisualising\nAs mentioned, we are interested in survived as a function of gene_expression_level, visualising this looks like so:\n\nsurvival_data |&gt; \n  ggplot(aes(x = gene_expression_level,\n             y = survived)) +\n  geom_point(alpha = 0.5,\n             size = 3)\n\n\n\n\n\n\n\n\nDoing what we did before with a straight line evidently isn’t super meaningful. What we’re interested in understanding is at what gene_expression_level does the plant not survive/survive? Clearly, at e.g. 80, the plant does not survive and at 105 it clear does! But what about 95? Well, that isn’t really clear. Here, plants are both observed to survive and not-survive. What about 93? Here, most do survive, but not all, although it seems that there is more chance of not-surviving and vice versa for 97, than surviving. It’s this chance we’re interested in. So, at different values of gene_expression_levels, what is the probability of surviving-/not-surviving respectively? This is the quesion a logistic regression answers, so let’s get to it!\n\n\nModelling\nTo do a logistic regression, we use the glm()-function:\n\nmy_glm_mdl &lt;- glm(formula = survived ~ gene_expression_level,\n                  family = binomial(link = \"logit\"),\n                  data = survival_data)\nmy_glm_mdl\n\n\nCall:  glm(formula = survived ~ gene_expression_level, family = binomial(link = \"logit\"), \n    data = survival_data)\n\nCoefficients:\n          (Intercept)  gene_expression_level  \n             -86.0134                 0.9095  \n\nDegrees of Freedom: 99 Total (i.e. Null);  98 Residual\nNull Deviance:      138.6 \nResidual Deviance: 37.75    AIC: 41.75\n\n\nNow, with the model in place, we can visualise. Below here, the points are the observed data and the line is the model of how the probability of survival changes with gene_expression_level:\n\nsurvival_data |&gt; \n  mutate(my_glm_model = pluck(my_glm_mdl, \"fitted.values\")) |&gt; \n  ggplot(aes(x = gene_expression_level, y = survived)) +\n  geom_point(alpha = 0.5,\n             size = 3) +\n  geom_line(aes(y = my_glm_model))\n\n\n\n\n\n\n\n\nThis allows us to answer the question from before:\n\nAt 80, the plant does not survive?\nAt 105 it clearly does!\nWhat about 95?\nWhat about 93?\nIs it vice versa for 97?\n\n\npredict.glm(object = my_glm_mdl,\n            newdata = tibble(gene_expression_level = c(80, 105, 95, 93, 97)),\n            type = \"response\")\n\n           1            2            3            4            5 \n1.755625e-06 9.999240e-01 5.962687e-01 1.932429e-01 9.010511e-01 \n\n\nSo:\n\nAt 80, the plant does not survive? TRUE, the probability of survival is close to zero\nAt 105 it clearly does! TRUE, the probability of survival is close to one\nWhat about 95? Around 60% survival probability\nWhat about 93? Around 20% survival probability\nIs it vice versa for 97? Around 90% survival probability\n\nAgain, this model object is a bit quirke, so broom to the rescue:\n\nmy_glm_mdl |&gt;\n  tidy(conf.int = TRUE,\n       conf.level = 0.95) |&gt; \n  mutate(estimate = exp(estimate))\n\n# A tibble: 2 × 7\n  term                  estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)           4.41e-38    18.1       -4.74  2.11e-6 -130.       -56.6 \n2 gene_expression_level 2.48e+ 0     0.191      4.76  1.97e-6    0.599      1.37",
    "crumbs": [
      "Primers",
      "Primer on Logistic Regression in `R`"
    ]
  },
  {
    "objectID": "primer_functions.html#the-basics",
    "href": "primer_functions.html#the-basics",
    "title": "Primer on Functions in R",
    "section": "The Basics",
    "text": "The Basics\nA function in its simplest form takes some “input” and does “something” and then returns the “result”:\n\nf <- function(input){\n  # Do \"something\" to get the result\n  return(result)\n}\n\nSo the function “acts” upon the “input” and thereby creates the result. Since we are working with R for Bio Data Science, let us use DNA as an example of input and then choose transcription as the act upon the input, whereby messenger RNA would be the output. This would look like so:\n\ntranscribe_dna <- function(DNA){\n\n  # First we initiate\n  mRNA <- \"\"\n  \n  # Then we elongate by going through all\n  # of the nucleotides in the input DNA\n  for( i in 1:nchar(DNA) ){\n    \n    # To elongate, first we specify\n    # the i'th nucleotide in the input DNA\n    nucleotide <- substr(DNA, i, i)\n    \n    # Then we define the as of yet unknown\n    # result of the transcription\n    transcribed_nucleotide <- \"\"\n    \n    # And then we see if the input nucleotide\n    # is \"a\", \"c\", \"g\" or \"t\" and perform the \n    # appropriate transcription:\n    if( nucleotide == \"a\" ){\n      transcribed_nucleotide <- \"u\"\n    } else if(nucleotide == \"c\"){\n      transcribed_nucleotide <- \"g\"\n    } else if(nucleotide == \"g\"){\n      transcribed_nucleotide <- \"c\"\n    } else if(nucleotide == \"t\"){\n      transcribed_nucleotide <- \"a\"\n    }\n    \n    # Perform the elongation\n    mRNA <- paste0(mRNA, transcribed_nucleotide)\n  }\n  \n  # And finally, we terminate\n  return(mRNA)\n}"
  },
  {
    "objectID": "primer_r_packages.html",
    "href": "primer_r_packages.html",
    "title": "Primer on R package development",
    "section": "",
    "text": "How to work with an R package\nThere are a few things to know when creating a package before you jump in. These are not strict rules, but they make your life easier when bug-fixing and make the package much easier to use for the users. Learn about the dos and don’ts in the following.",
    "crumbs": [
      "Primers",
      "Primer on R package development"
    ]
  },
  {
    "objectID": "primer_paths_and_projects.html",
    "href": "primer_paths_and_projects.html",
    "title": "Paths & Projects",
    "section": "",
    "text": "Getting Familiar with Paths in RStudio\nYou step onto the road, and if you don’t keep your feet, there’s no knowing where you might be swept off to\nIn context of R, a path is a way to tell R, where to look for a file. First, let us familiarise us a bit with paths in RStudio.\nLog on to the RStudio Cloud Server and in case you do so for the first time, your files-pane should look something like:\nHere,  Home defines your home, that is where you “live” on the server. Now, click the button  New Folder and create a new folder called projects. Hereafter, your files-pane should look something like:\nNow, click the projects folder you created and then you should see:\nNote how the Home now is extended to  Home &gt; projects. This signifies, that you are looking at the projects folder in your home.\nTry to click on the 2 dots in .. (the green arrow won’t do it, so hit those dots!), this will take you one level up, so you again see:\nNote, that you are now back in  Home",
    "crumbs": [
      "Primers",
      "Paths & Projects"
    ]
  },
  {
    "objectID": "primer_paths_and_projects.html#getting-familiar-with-paths-in-rstudio",
    "href": "primer_paths_and_projects.html#getting-familiar-with-paths-in-rstudio",
    "title": "Paths & Projects",
    "section": "Getting Familiar with Paths in RStudio",
    "text": "Getting Familiar with Paths in RStudio\nLog on to the RStudio Cloud Server and in case you do so for the first time, your files-pane should look something like:\n Here,  Home defines your home, that is where you “live” on the server. Now, click the button  New Folder and create a new folder called projects. Hereafter, your files-pane should look something like:\n\nNow, click the projects folder you created and then you should see:\n\nNote how the Home now is extended to  Home > projects. This signifies, that you are looking at the projects folder in your home.\nTry to click on the 2 dots in .. (the green arrow won’t do it, so hit those dots!), this will take you one level up, so you again see:\n\nNote, that you are now back in  Home"
  },
  {
    "objectID": "primer_paths_and_projects.html#intermezzo-creating-a-project",
    "href": "primer_paths_and_projects.html#intermezzo-creating-a-project",
    "title": "Paths & Projects",
    "section": "Intermezzo: Creating a Project",
    "text": "Intermezzo: Creating a Project\nOk, so far so good. Now again click into projects and click  Project: (None) and from here select  New Project.... Now you will se a dialogue window open, i.e. RStudio requires input from you:\n\nClick  New Directory and you should see:\n\nClick  New Project:\n\nIn the Directory name:, enter e.g. r_for_bio_data_science and note how we are creating the folder (In this case Directory name, which is equivalent) as a sub-folder of projects. The funny wavy symbol followed by a slash ~/ is simply a short hand for “in this users home folder”. So let us proceed:\n\nand then click Create Project. Now, depending on your choice of directory name, you should see:\n\nBriefly, the created r_for_bio_data_science.Rproj file contains the settings for your project. You can verify this, by clicking on the file and you should see:\n\nWe will leave this as is for now, so go ahead and click OK.\nNow, back to folders and paths - note how you now see  Home &gt; projects &gt; r_for_bio_data_science, this means that you are now in the r_for_bio_data_science folder, which is inside the projects folder which in turn is inside the Home folder. If you take a look at the  Home &gt; projects &gt; r_for_bio_data_science, you can in fact also click directly on e.g. projects - Try it!\nBut why? Don’t worry, we will return to why RStudio Projects are indispensable when working with reproducible Bio Data Science",
    "crumbs": [
      "Primers",
      "Paths & Projects"
    ]
  },
  {
    "objectID": "primer_paths_and_projects.html#locating-data",
    "href": "primer_paths_and_projects.html#locating-data",
    "title": "Paths & Projects",
    "section": "Locating Data",
    "text": "Locating Data\nLet’s move on. Now, again click the  New Folder-button and create a data-folder, make sure it ends up in the r_for_bio_data_science-folder. This should result in:\n\nNote here how we now have  data and  r_for_bio_data_science.Rproj. These are different, one is a folder, data, and the other is a file, r_for_bio_data_science.Rproj, containing settings for your RStudio Project.\nLet us try to put som data into the data-folder. In the console window, run the following command\n\nwrite.table(x = datasets::Puromycin, file = \"data/puromycin.tsv\", sep = \"\\t\")\n\nThis should look like so:\n\nThis command write a table containing the Puromycin from an R-package named datasets, this is done using the x-parameter. The next paramter is file and we set that to indicate, that the file should go into the data-folder we created and that we would like the file to be named puromycin.tsv, where tsv is an abbreviation for tab-separated-values and then the last parameter sep is set to \"\\t\" indicating, that we want the values to be separated by a tab, as indicated, when we named the file.\nOnce you have run the command, click the data-folder and you should now see:\n\nAgain, click .., this will take you one level up, so you again see:\n\nNow, we have a data file called puromycin.tsv. Let us read that file into R. We can do that like so:\n\nmy_data &lt;- read.table(file = \"puromycin.tsv\")\n\nEnter the command into the console and run it like we did before. You will now see the following:\n\nSo, what happend? The blue writing is your command and the red is an error message from R. Always read error messages carefully, they will inform you what went wrong. In this case, we can see that cannot open file 'puromycin.tsv': No such file or directory.\nThis happened because we forgot to specify where the puromycin.tsv-file is located. R is very picky here, you have to specify exactly where R should find things. Recall, that we decided to create a data-folder and that we placed the puromycin.tsv-file into that folder. This we have to specify, when we use the file parameter in the read.table()-function. So, let us fix that:\n\nmy_data &lt;- read.table(file = \"data/puromycin.tsv\")\n\nThis data/puromycin.tsv is the path to the file and now, that we have got that correct, you will see no error message and furthermore, you will see in the environment-pane, that we now have an object called my_data, containing 23 obs. of 3 varibles, i.e. a data set with 23 rows and 3 columns.",
    "crumbs": [
      "Primers",
      "Paths & Projects"
    ]
  },
  {
    "objectID": "primer_paths_and_projects.html#absolute-versus-relative-paths",
    "href": "primer_paths_and_projects.html#absolute-versus-relative-paths",
    "title": "Paths & Projects",
    "section": "Absolute versus Relative Paths",
    "text": "Absolute versus Relative Paths\nLet us get back to why we have to work using RStudio Projects, recall we created the r_for_bio_data_science.Rproj-file, defining out project. You can verify, that we are indeed working in that project, by looking in the upper right corner of the RStudio IDE and you should see  r_for_bio_data_science.\nGood, now in the console, enter the command:\n\ngetwd()\n\nYou should see something along the lines of:\n\n\"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science\"\n\nSo, when we read the puromycin.tsv-file using the path data/puromycin.tsv, we specify, that R should look for the file puromycin.tsv in the data folder. So why did we not have to specify /net/pupilx/home/people/student_id/projects/r_for_bio_data_science? Well indeed, we could have specified the full location of the puromycin.tsv-file, which would be:\n\n\"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science/data/puromycin.tsv\"\n\nThis is called the absolute path and here you should note, that it begins with a /. But let us say, that we had indeed in our code stated:\n\nmy_data &lt;- read.table(file = \"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science/data/puromycin.tsv\")\n\nThen that would work… On OUR computer. If we were to share our code to a colleague or a collaborator, then that would not work, because that person would have a different path, e.g. a different student_id. The code would break! Imagine that you have thousands of line of code with hundreds of absolute paths - You would spend hours-and-hours on fixing all the absolute paths, so they matched that particular computer. Then every time we would want to share the analysis project, we would have to redo this tedious proces!\nThis is why we work in RStudio Projects! RStudio Projects allows us to specifiy where everything is located relative to where the .Rproj-file is. So in our case, the r_for_bio_data_science.Rproj-file is located in the same place as the data-folder, namely in the folder containing our entire project, the r_for_bio_data_science-folder, which in turn is located in the projects-folder.\nThis means, that all paths in the analysis project, can be stated relative to the location of the .Rproj-file and hence we have relative paths, meaning that anyone can receive the project and run it straight-out-of-the-box!",
    "crumbs": [
      "Primers",
      "Paths & Projects"
    ]
  },
  {
    "objectID": "primer_paths_and_projects.html#working-in-multiple-projects",
    "href": "primer_paths_and_projects.html#working-in-multiple-projects",
    "title": "Paths & Projects",
    "section": "Working in Multiple Projects",
    "text": "Working in Multiple Projects\nNow, we did add that plural s, when we created the projects-folder. When you have completed this course, perhaps you want to attend the “Introduction to Systems Biology”-course. In that case, we would setup a new project, so use the Files-pane to navigate to the projects-folder:\n\nThen, we simply repeat the proces: Click  r_for_bio_data_science in the upper right corner and from here select  New Project.... Now you will se a dialogue window open, i.e. RStudio requires input from you:\n\nClick  New Directory and you should see:\n\nClick  New Project:\n\nIn the Directory name:, enter e.g. introduction_to_systems_biology:\n\nand then click Create Project. Now, you should see:\n\nNow, note how you now see  Home &gt; projects &gt; introduction_to_systems_biology, meaning that you are now in your Home and then in your folder containing projects, one of which is your introduction_to_systems_biology project.\nClick .. and you will see:\n\nThis is now your two project folders and you can add others, such as yet another course or e.g. special_course, bachelor_thesis or master_thesis.\nNow, we can easily switch between different projects. In the upper right corner you will see, that you are currently in the introduction_to_systems_biology project, meaning that R will look for all files relative to the introduction_to_systems_biology.Rproj-file. Naturally, we would want to switch back to the project, we created for the “R for Bio Data Science”-course. To do this, we simply click  introduction_to_systems_biology and if you look at the drop-down menu, you should see r_for_bio_data_science - Click it! Notice how R automatically restarts and you are moved to the correct folder for this project.\nMake sure to change your active project back to r_for_bio_data_Science before doing further lab exercises!",
    "crumbs": [
      "Primers",
      "Paths & Projects"
    ]
  },
  {
    "objectID": "primer_paths_and_projects.html#learning-objectives",
    "href": "primer_paths_and_projects.html#learning-objectives",
    "title": "Paths & Projects",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIf you made it this far, you should now be able to:\n\nNavigate the RStudio IDE in context of folders and projects\nCreate a new project\nCreate a new folder\nRead and write data files from relative paths\nWork in and switch between different projects\nExplain the difference between a folder, a data file and a Rproj-file\nExplain the difference between absolute and relative paths\nExplain why RStudio Projects are an essential part of doing reproducible bio data science",
    "crumbs": [
      "Primers",
      "Paths & Projects"
    ]
  },
  {
    "objectID": "primer_paths_and_projects.html#epilogue",
    "href": "primer_paths_and_projects.html#epilogue",
    "title": "Paths & Projects",
    "section": "Epilogue",
    "text": "Epilogue\nThat’s all folks! I hope this cleared up some things - Please feel free to revisit this chapter, as needed!\nRemember - Have fun! No one ever got really good at something they didn’t think was fun!",
    "crumbs": [
      "Primers",
      "Paths & Projects"
    ]
  },
  {
    "objectID": "primer_variable_assignment.html",
    "href": "primer_variable_assignment.html",
    "title": "Variable Assignment in R",
    "section": "",
    "text": "Assigning a Value to a Variable\nIn R we operate with variables. A variable can be seen as a container for a value. To get a better conceptual understanding of this, you can go through the following and code-along in your own R-session.\n3\n\n[1] 3\nx &lt;- 3\nx\n\n[1] 3",
    "crumbs": [
      "Primers",
      "Variable Assignment in `R`"
    ]
  },
  {
    "objectID": "primer_variable_assignment.html#assigning-a-value-to-a-variable",
    "href": "primer_variable_assignment.html#assigning-a-value-to-a-variable",
    "title": "Variable Assignment in R",
    "section": "",
    "text": "In R, we state values directly in the chunk or the console, e.g.:\n\n\n\nHere, we just state 3, so R simply “throws” that right back at you!\nNow, if want to “catch” that 3 we have to assign it to a variable, e.g.:\n\n\n\nNotice how now we “catch” the 3 and nothing is “thrown” back to you, because we now have the 3 stored in x:",
    "crumbs": [
      "Primers",
      "Variable Assignment in `R`"
    ]
  },
  {
    "objectID": "primer_variable_assignment.html#updating-the-value-of-a-variable",
    "href": "primer_variable_assignment.html#updating-the-value-of-a-variable",
    "title": "Variable Assignment in R",
    "section": "Updating the Value of a Variable",
    "text": "Updating the Value of a Variable\n\nNow, we can of course use x moving forward, e.g. by adding 2:\n\n\nx + 2\n\n[1] 5\n\n\n\nNotice how this does not change x and the result is simply “thrown” right-back-at-ya\n\n\nx\n\n[1] 3\n\n\n\nIf we wanted to update x by adding 2, we would have to “catch” the result as before:\n\n\nx &lt;- x + 2\n\n\nNow, we have updated x:\n\n\nx\n\n[1] 5",
    "crumbs": [
      "Primers",
      "Variable Assignment in `R`"
    ]
  },
  {
    "objectID": "primer_variable_assignment.html#use-one-variable-in-the-creation-of-another",
    "href": "primer_variable_assignment.html#use-one-variable-in-the-creation-of-another",
    "title": "Variable Assignment in R",
    "section": "Use one Variable in the Creation of Another",
    "text": "Use one Variable in the Creation of Another\n\nAnalogue, we can create a new variable using x:\n\n\ny &lt;- x + 3\n\n\nAgain, this does not change x\n\n\nx\n\n[1] 5\n\n\n\nBut rather the result is now stored in y\n\n\ny\n\n[1] 8",
    "crumbs": [
      "Primers",
      "Variable Assignment in `R`"
    ]
  },
  {
    "objectID": "primer_variable_assignment.html#summary",
    "href": "primer_variable_assignment.html#summary",
    "title": "Variable Assignment in R",
    "section": "Summary",
    "text": "Summary\n\nIn R, we use the assignment operator &lt;- to perform assignment\nVariables are not change in place, but needs to be stored\nNote, this also applies to running e.g. a dplyr-pipeline, where we do not change the dataset by running the pipeline, but we must store the result of the pipeline\n\nBefore continuing, make sure that you are on track with the above concepts!\n\nCreate a new variable my_age containing… You guessed it!\nAdd 0.5 to the variable (I.e. your age, when you’re done with this course)\nCheck the value of my_age, did you remember to assign, thereby updating?",
    "crumbs": [
      "Primers",
      "Variable Assignment in `R`"
    ]
  },
  {
    "objectID": "primer_variable_assignment.html#pipeline-example",
    "href": "primer_variable_assignment.html#pipeline-example",
    "title": "Variable Assignment in R",
    "section": "Pipeline Example",
    "text": "Pipeline Example\n\nLet us create some example sequence data:\n\n\ntibble(sequence = c(\"aggtgtgag\", \"tggaatgaaccgcctacc\",\n                    \"aagaatgga\", \"tct\", \"tgtatt\", \"tgg\",\n                    \"accttcaacgagtcccactgt\", \"cgt\",\n                    \"gaggctgagctggttgta\", \"ggggaacag\"))\n\n# A tibble: 10 × 1\n   sequence             \n   &lt;chr&gt;                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nNotice, that our data creation is just “thrown” back at us, we forgot something!\n\n\nmy_dna_data &lt;- tibble(sequence = c(\"aggtgtgag\", \"tggaatgaaccgcctacc\",\n                                   \"aagaatgga\", \"tct\", \"tgtatt\", \"tgg\",\n                                   \"accttcaacgagtcccactgt\", \"cgt\",\n                                   \"gaggctgagctggttgta\", \"ggggaacag\"))\n\n\nNow, we have stored the data in the variable my_dna_data\n\n\nmy_dna_data\n\n# A tibble: 10 × 1\n   sequence             \n   &lt;chr&gt;                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nNote here, that a variable can as we saw before with x and y store a single value, e.g. 2, but here, we are storing a tibble-object in the variable my_dna_data and in that tibble-object, we have a variable sequence, which contains some randomly generated dna.\nBut what if we wanted to add a new variable to the tibble-object, which is the lenght of each of the dna-sequences?\n\n\nmy_dna_data |&gt;\n  mutate(dna_length = str_length(sequence))\n\n# A tibble: 10 × 2\n   sequence              dna_length\n   &lt;chr&gt;                      &lt;int&gt;\n 1 aggtgtgag                      9\n 2 tggaatgaaccgcctacc            18\n 3 aagaatgga                      9\n 4 tct                            3\n 5 tgtatt                         6\n 6 tgg                            3\n 7 accttcaacgagtcccactgt         21\n 8 cgt                            3\n 9 gaggctgagctggttgta            18\n10 ggggaacag                      9\n\n\nNice! Let’s see that data again then:\n\nmy_dna_data\n\n# A tibble: 10 × 1\n   sequence             \n   &lt;chr&gt;                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nWait! What? Where is the variable we literally just created?\nWe forgot something… We did not update the my_dna_data, let’s fix that:\n\n\nmy_dna_data &lt;- my_dna_data |&gt;\n  mutate(dna_length = str_length(sequence))\n\n\nNote, nothing is “trown” back at us! Let’s verify, that we did indeed update the my_dna_data:\n\n\nmy_dna_data\n\n# A tibble: 10 × 2\n   sequence              dna_length\n   &lt;chr&gt;                      &lt;int&gt;\n 1 aggtgtgag                      9\n 2 tggaatgaaccgcctacc            18\n 3 aagaatgga                      9\n 4 tct                            3\n 5 tgtatt                         6\n 6 tgg                            3\n 7 accttcaacgagtcccactgt         21\n 8 cgt                            3\n 9 gaggctgagctggttgta            18\n10 ggggaacag                      9\n\n\nDid it make sense? Check yourself, add a new variable to my_dna_data called sequence_capital by using the function str_to_upper()\nThat’s it - Hope it helped and remember… Bio data science in R is really fun!",
    "crumbs": [
      "Primers",
      "Variable Assignment in `R`"
    ]
  },
  {
    "objectID": "guides.html",
    "href": "guides.html",
    "title": "Guides",
    "section": "",
    "text": "Various guides pertaining to the course",
    "crumbs": [
      "Guides"
    ]
  },
  {
    "objectID": "guide_rstudio_server.html",
    "href": "guide_rstudio_server.html",
    "title": "Guide for Cloud server and the RStudio IDE",
    "section": "",
    "text": "Course Cloud Server Logon\nGo to the R for Bio Data Science Cloud Server and follow the login procedure.",
    "crumbs": [
      "Guides",
      "Guide for Cloud server and the RStudio IDE"
    ]
  },
  {
    "objectID": "guide_rstudio_server.html#the-rstudio-ide",
    "href": "guide_rstudio_server.html#the-rstudio-ide",
    "title": "Guide for Cloud server and the RStudio IDE",
    "section": "The RStudio IDE",
    "text": "The RStudio IDE\nUpon login, you will be presented with the RStudio integrated development environment (IDE) for R:\n\n\n\n\n\nFirst, just a quick change of settings to enable graphics. Click Tools \\(\\rightarrow\\) Global Options. Then under General, choose the tab Graphics and under Graphics Device, choose Cairo as your Backend. Then click Apply and finally OK. That was easy right? Now, please continue.\nWelcome to the RStudio IDE. It allows you to consolidate all features needed to develop R code for analysis. Now, click Tools \\(\\rightarrow\\) Global Options... \\(\\rightarrow\\) Pane Layout and you will see this:\n\n\n\n\n\nThis outlines the four panes you have in your RStudio IDE and allow you rearrange them as you please. Now, re-arrange them, so that they look like this:\n\n\n\n\n\nClick Apply \\(\\rightarrow\\) OK and you should see this:\n\n\n\n\n\n\nSource\n\nWhere you write and edit scripts (.R files, Quarto, etc.).\nCode written here is not executed until you send it to the Console (e.g., with Ctrl+Enter / Cmd+Enter).\nUsed for developing reproducible workflows instead of typing everything directly into the Console.\n\n\n\nConsole\n\nWhere commands are executed immediately.\nYou can test quick snippets of code here.\nIt shows results, error messages, and feedback from R in real time.\n\n\n\nEnvironment\n\nDisplays the objects, variables, and data currently loaded into memory.\nHelps you keep track of what is available in your R session.\nYou can clear objects or inspect them here.\n\n\n\nFiles etc. (Files, Plots, Packages, Help, Viewer, etc.)\n\nA set of utility tabs:\n\nFiles: navigate your working directory, upload/download files.\nPlots: displays plots generated by your code.\nPackages: manage installed R packages.\nHelp: access R documentation.\nViewer/Presentation: view HTML outputs, reports, and apps.",
    "crumbs": [
      "Guides",
      "Guide for Cloud server and the RStudio IDE"
    ]
  },
  {
    "objectID": "guide_working_locally.html",
    "href": "guide_working_locally.html",
    "title": "Guide for Working Locally",
    "section": "",
    "text": "Download Course Materials",
    "crumbs": [
      "Guides",
      "Guide for Working Locally"
    ]
  },
  {
    "objectID": "guide_working_locally.html#download-course-slides",
    "href": "guide_working_locally.html#download-course-slides",
    "title": "Guide for Working Locally",
    "section": "Download Course Slides",
    "text": "Download Course Slides\nThe course slides are available via the course site. They are self-contained .html-files and can be opened offline in a browser like e.g. google chrome. The slides can be downloaded as follows:\n\nGo to the Course Site\nIn the menu on the left, choose a course lab and click it\nUnder Schedule, find and click the link to the lecture\nHit command+s (mac) or ctrl+s (windows) or choose file and then save depending on your OS\nPut the downloaded .html-files where you want to save them.\nRight click and choose open with or similar and then choose your favourite browser",
    "crumbs": [
      "Guides",
      "Guide for Working Locally"
    ]
  },
  {
    "objectID": "guide_working_locally.html#installing-r-on-your-computer",
    "href": "guide_working_locally.html#installing-r-on-your-computer",
    "title": "Guide for Working Locally",
    "section": "Installing R on your computer",
    "text": "Installing R on your computer\n\nGo to The R Project for Statistical Computing\nUnder Getting Started it says To download R, please..., click the download R-part\nScroll down and find Denmark and click\nClick the Aalborg University-mirror link https://mirrors.dotsrc.org/cran/\nUnder Download and Install R, click your appropriate OS Linux, macOS or Windows\n\nIf you have Linux, you’ll know what to do\nFor mac, be aware that you have to choose either R-4.3.2-arm64.pkg or R-4.3.2-x86_64.pkg (versions as of 30-11-23) depending on whether you have a M- or an Intel-mac. This you can find out by clicking the apple in the upper left corner of your screen and select About This Mac and look at what is says next to Chip\nFor Windows, there is a install R for the first time-guide you will have to follow.\n\n\nIf any of this is unclear, I’m absolutely sure, there will be tons of how-to-install-R tutorials on youtube",
    "crumbs": [
      "Guides",
      "Guide for Working Locally"
    ]
  },
  {
    "objectID": "guide_working_locally.html#installing-rstudio",
    "href": "guide_working_locally.html#installing-rstudio",
    "title": "Guide for Working Locally",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nGo to the Posit site\nClick DOWNLOAD RSTUDIO in the upper right corner\nUnder RStudio Desktop, click DOWNLOAD RSTUDIO\nThe first point 1: Install R is covered above\nUnder 2: Install RStudio, click the blue box below, it will vary depending on your OS, mine says DOWNLOAD RSTUDIO DESKTOP FOR MACOS 11+\nOpen the downloaded installer and follow the instructions",
    "crumbs": [
      "Guides",
      "Guide for Working Locally"
    ]
  },
  {
    "objectID": "guide_working_locally.html#installing-packages",
    "href": "guide_working_locally.html#installing-packages",
    "title": "Guide for Working Locally",
    "section": "Installing packages",
    "text": "Installing packages\nOnce you have downloaded and installed R and RStudio, you can start installing packages. E.g. we can install Tidyverse using the command:\n\ninstall.packages(\"tidyverse\")\n\nLook through the different exercise labs at the top, the packages you have been working with are stated.",
    "crumbs": [
      "Guides",
      "Guide for Working Locally"
    ]
  },
  {
    "objectID": "guide_working_locally.html#open-course-materials",
    "href": "guide_working_locally.html#open-course-materials",
    "title": "Guide for Working Locally",
    "section": "Open course materials",
    "text": "Open course materials\nThe downloaded course materials can simply be opened directly in RStudio, so be sure to organise them, so you can easily access them on your own computer.",
    "crumbs": [
      "Guides",
      "Guide for Working Locally"
    ]
  },
  {
    "objectID": "guide_building_r4ds2e_locally.html",
    "href": "guide_building_r4ds2e_locally.html",
    "title": "Guide for Building the R4DS2e Book Locally",
    "section": "",
    "text": "Clone Repository\nA Clone Repository process now starts, wait for it to finish\nCongratulations! You have now sucessfully cloned a GitHub repository and build an entire Quarto book from scratch!\nIf you get stuck in a loop where RStudio says “Restart R prior to install” on the same package. Select Cancel, quit the project, re-open the project and as the first thing, type in the install.packages(\"PACKAGE_NAME\")-command and then select No, when it prompts for restart.\nStop for a moment and consider that you get all of this is for free! All the tons of hours being put in by the 287 contributors have done so in the name of open source and open learning! That’s nothing short of awesome!",
    "crumbs": [
      "Guides",
      "Guide for Building the R4DS2e Book Locally"
    ]
  },
  {
    "objectID": "code_styling.html",
    "href": "code_styling.html",
    "title": "Code Styling",
    "section": "",
    "text": "This is a condensed primer on code styling, based on The tidyverse style guide"
  },
  {
    "objectID": "code_styling.html#why",
    "href": "code_styling.html#why",
    "title": "Code Styling",
    "section": "Why?",
    "text": "Why?\n“Good coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread” source\nIt is not uncommon in a bio data science industry setting, where there is an aspect of production code, i.e. the code you are writing is being used professionally by other users either in an organisation or as an actual software product, that you will have to adhere to a certain style, when coding. This facilitates lower maintanence on code. Imagine the case, where you are perhaps 10 bio data scientists working on e.g. an R-package, which will used downstream by another department. You really need to make sure, that everything is top notch, so you decide to do code reviewing. You write your code up and then your colleague will go over the code to check it. But you and your colleague have completely different opinions on how the code should be styled. This will result in an unnecessary time overhead. Another case, could be a colleague leaving for another position, where you then have to take over that colleague’s code base and you end up re-styling the code, so it matches your preferences, again this will increase maintenance unnecessarily. The solution is to agree on a set of rules and principles on how to style your code - This is code styling!\nBelow follows the code styling you will have to adhere to in this course"
  },
  {
    "objectID": "code_styling.html#so-how-should-we-style-the-code",
    "href": "code_styling.html#so-how-should-we-style-the-code",
    "title": "Code Styling",
    "section": "So, how should we style the code?",
    "text": "So, how should we style the code?\nIn this course, we will use principles from the The tidyverse style guide. At first, it may seem constraining, but you will quickly get used to it and then it will be easier moving forward. Note, for your project, being able to review consistant code, will save you valuable time!\nRecall the cancer_data (gravier) dataset, we worked with:\n\nlibrary(\"tidyverse\")\nlibrary(\"curl\")\nbase_url <- \"https://github.com/\"\ntarget_file <- \"ramhiser/datamicroarray/raw/master/data/gravier.RData\"\noutput_file <- \"data/gravier.RData\"\ncurl_download(url = str_c(base_url,\n                          target_file),\n              destfile = output_file)\n\nPrinciples:\n\nQuote the packages you load using the library-function\nUse the proper variable assignment in R, namely <-\nStay within 80 characters width. Note, you can set a “help line”: Tools \\(\\rightarrow\\) Global Options... \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Show margin \\(\\rightarrow\\) Margin column: 80\nUse double quotes. This is for consistency with other languages\nTab out parameters of functions\nDo line breaks after commas\nDo one space on each side of =, when assigning arguments to parameters, e.g. my_function(parameter_1 = argument_1), etc.\nMatch indentations, this RStudio will do for you in most cases\n\nOk, a bit of data wrangling:\n\nload(file = \"data/gravier.RData\")\ncancer_data <- gravier |> \n  bind_cols() |> \n  rename(early_metastasis = y) |> \n  mutate(pt_id = str_c(\"pt_\", row_number()),\n         pt_has_early_metastasis = case_when(\n    early_metastasis == \"good\" ~ \"No\",\n    early_metastasis == \"poor\" ~ \"Yes\"))\n\nPrinciples:\n\nLine break after each pipe |>, recall we prononuce the pipe as “then”\nLine break after commas inside functions, such as here with mutate()\nIf the line in mutate() becomes wide, then consider using a single linebreak after opening the function call\nuse proper descriptive variable names in snake case like pt_has_early_metastasis, there is no overhead in understanding what this variable means\n\nLet’s do a simple plot:\n\ncancer_data |>\n  ggplot(aes(x = early_metastasis,\n             y = g8A08)) +\n  geom_hline(yintercept = 0,\n             linetype = \"dashed\") +\n  geom_boxplot() +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw() +\n  labs(x = \"Early Metastasis\")\n\n\n\n\nPrinciples:\n\nLinebreak after +, just like with the pipe\nSpace after comma inside a vector, like when we define the limits\n\nDo it - It’ll be fun and it does not take a long time to adapt!"
  },
  {
    "objectID": "external_resources.html",
    "href": "external_resources.html",
    "title": "External Resources",
    "section": "",
    "text": "A few quick ones…\nHere, you will find various valuable resources, to aid your bio data science workflow",
    "crumbs": [
      "Guides",
      "External Resources"
    ]
  },
  {
    "objectID": "external_resources.html#a-few-quick-ones",
    "href": "external_resources.html#a-few-quick-ones",
    "title": "External Resources",
    "section": "",
    "text": "A very handy ggplot cheat-sheet can be found here\nSo which plot to choose? Check this handy guide\nExplore ways of plotting here\nThere is a nice tool to aid in choosing colours for visualisations here\nThe Posit community pages is a very nice place to get help if you’re stuck",
    "crumbs": [
      "Guides",
      "External Resources"
    ]
  },
  {
    "objectID": "external_resources.html#open-source-data-science-books",
    "href": "external_resources.html#open-source-data-science-books",
    "title": "External Resources",
    "section": "Open source data science books",
    "text": "Open source data science books\n\nHands-On Programming with R by Garrett Grolemund\nStatistical Inference via Data Science - A moderndive into R and the tidyverse by Chester Ismay and Albert Y. Kim\nIntroduction to Data Science, Data Analysis and Prediction Algorithms with R by Rafael A. Irizarry\nMastering Shiny by Hadley Wickham\nAn Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\nSTAT 545 - Data wrangling, exploration, and analysis with R by Jenny Bryan\nHappy Git and GitHub for the useR by Jenny Bryan, the STAT 545 TAs, Jim Hester",
    "crumbs": [
      "Guides",
      "External Resources"
    ]
  },
  {
    "objectID": "external_resources.html#software-links",
    "href": "external_resources.html#software-links",
    "title": "External Resources",
    "section": "Software Links",
    "text": "Software Links\n\nThe R Project for Statistical Computing\nRStudio - Open Source and Enterprise-ready professional software for R\nTidyverse website",
    "crumbs": [
      "Guides",
      "External Resources"
    ]
  },
  {
    "objectID": "external_resources.html#some-useful-links",
    "href": "external_resources.html#some-useful-links",
    "title": "External Resources",
    "section": "Some Useful Links",
    "text": "Some Useful Links\n\nFrom data to Viz: Find the graphic you need\nR for Data Science: Exercise Solutions\nR colour guide by Tian Zheng\nThe tidyverse style guide - By Hadley Wickham\nRStudio Primers - Learn data science basics with the interactive tutorials\nThe tidyverse style guide\nswirl - Learn R, in R\nRStudio Cheat Sheets\nRStudio Community - Stuck? Ask a question and get help moving on\nHarvardX Biomedical Data Science Open Online Training\nData Analysis Playlist",
    "crumbs": [
      "Guides",
      "External Resources"
    ]
  },
  {
    "objectID": "external_resources.html#on-data-science",
    "href": "external_resources.html#on-data-science",
    "title": "External Resources",
    "section": "On Data Science",
    "text": "On Data Science\n\nThe Role of Academia in Data Science Education",
    "crumbs": [
      "Guides",
      "External Resources"
    ]
  },
  {
    "objectID": "external_resources.html#guides-on-good-data-practices",
    "href": "external_resources.html#guides-on-good-data-practices",
    "title": "External Resources",
    "section": "Guides on Good Data Practices",
    "text": "Guides on Good Data Practices\n\nA Guide to Reproducible Code by the British Ecology Society\nA Quick Guide to Organizing Computational Biology Projects\nHow to pick more beautiful colors for your data visualizations\nTalk: Steps toward reproducible research by Karl Broman",
    "crumbs": [
      "Guides",
      "External Resources"
    ]
  },
  {
    "objectID": "course_elements.html",
    "href": "course_elements.html",
    "title": "Course Elements",
    "section": "",
    "text": "Various elements central to the course"
  },
  {
    "objectID": "ai_in_r4bds.html",
    "href": "ai_in_r4bds.html",
    "title": "Use of AI in the R for Bio Data Science Course",
    "section": "",
    "text": "Please note, that rules on the use of Generative AI (GAI) in “written exams with all aids - without internet.” are such that in addition to written aids, students are allowed to bring laptops, tablets, and similar devices but it is NOT allowed to use generative AI, even if the tool is available in an offline version!\nFailure to comply with the above, will be considered cheating and reported as such.\nThe following is a copy of Study announcement s1885. It is very important that students are familiar with the official rules concerning use of AI and also that it remains the responsibility of the student to be in full compliance with the following"
  },
  {
    "objectID": "ai_in_r4bds.html#use-of-artificial-intelligence-generative-ai-in-teaching-and-exams-at-dtu",
    "href": "ai_in_r4bds.html#use-of-artificial-intelligence-generative-ai-in-teaching-and-exams-at-dtu",
    "title": "Use of AI in the R for Bio Data Science Course",
    "section": "Use of Artificial Intelligence (generative AI) in teaching and exams at DTU",
    "text": "Use of Artificial Intelligence (generative AI) in teaching and exams at DTU\nDTU embraces the use of artificial intelligence (generative AI like ChatGPT), and therefore allows its integration in teaching and exams with open internet access. In the future, lecturers will have the opportunity to incorporate generative AI in their teaching, and in certain courses, students will be encouraged to use generative AI to solve tasks - all aimed at enhancing learning and fostering the development of better engineering skills.\nHowever, particularly concerning exams, there are several conditions that you must be aware of when employing generative AI. Further details can be found below.\n\nThe Use of Generative AI for Exam E2023\nFor the E2023 exams, the rules regarding the use of aids, as outlined in the course description (the course base), are applicable.\nIn instances where the use of generative AI, such as ChatGPT, is permitted for exams, it is imperative to adhere to DTU’s code of honour and comply with the rules of good academic practice, which are consistently applicable.\nYou must remember the following:\n\nAlways explicitly declare if any part of your submission is not your original work. Failure to do so may be considered exam cheating, in accordance with DTU’s code of honour and the rules for referencing when using generative AI.\nGenerative AI can produce output that may be inaccurate or incorrect. It should be viewed as a tool, and you are responsible for ensuring the quality and correctness of the work you hand in.\nAI-generated output may include material protected by copyright without clear indication. It is your responsibility to ensure compliance with copyright laws.\nGenerative AI can exhibit biases and may be trained based on specific attitudes. Consequently, critical evaluation of its output is essential.\nGenerative AI reuses the information you feed into it. Consequently, avoid providing sensitive information.\nThe guideline “All aids, no internet access” for the exam implies that you must not use generative AI.\n\nBy adhering to these guidelines, you contribute to maintaining academic integrity and responsible use of generative AI in the educational context at DTU.\n\n\nQuestions?\nYou can contact the Student Guidance if you have general questions about exam rules, cheating, and plagiarism. If you have questions about the exam rules for a specific course, you must contact your lecturer."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "As part of this course, starting from Lab 2, there will be a weekly hand in. Here, follows the instructions for creating and handing in course assignments."
  },
  {
    "objectID": "assignments.html#feedback",
    "href": "assignments.html#feedback",
    "title": "Assignments",
    "section": "Feedback",
    "text": "Feedback\nThe aim of these weekly group hand ins, is for you to get feedback on your work and your progress.\nYou will then receive group feedback on your assignment, which you should make sure to go over in your group. Please note, that some groups may have members across 22100 and 22160, in that case, the feedback will only appear under the course number, where the assignment was submitted.\nTherefore, please make sure to go over the feedback from last week’s assignment at the beginning of each exercise session"
  },
  {
    "objectID": "assignments.html#assignment-instructions",
    "href": "assignments.html#assignment-instructions",
    "title": "Assignments",
    "section": "Assignment Instructions",
    "text": "Assignment Instructions\nThe assignment will be marked “Group Assignment” with red font in the exercises. In your group, you are to prepare an answer to just this one question. Think about reproducibility from the get-go, so include in your assignment, what is needed to re-create your micro-report, e.g. if you’re using external data, from where and how did you get it?\nPlease note, that in order to include all elements in a self-contained html file, you will have to use the following YAML-header in your Quarto document:\n\n---\ntitle: \"Lab 2 Assignment: Group 02\"\nformat:\n  html:\n    embed-resources: true\neditor: visual\n---\n\nOr at least one similar, format title and other elements as you see fit, key here is the html-format with embedded resources."
  },
  {
    "objectID": "assignments.html#how-to-hand-in",
    "href": "assignments.html#how-to-hand-in",
    "title": "Assignments",
    "section": "How to hand in",
    "text": "How to hand in\n\nCheck that your assignment conforms to the assignment checklist, as defined in the next section\nGo to DTU Learn\nFind and click your R for Bio Data Science course\nMake sure you are enrolled in the correct group, as defined by the Group Formation Sheet (see the Getting Started Section)\nIn case groups consists of a mix of BSc. and MSc. students, then enroll under your respective courses\nOnly hand in one assignment, meaning either under the BSc. or the MSc. course, not both\nYou should hand in the rendered html-file, make sure to compress it to a zip-file prior to upload"
  },
  {
    "objectID": "assignments.html#deadline",
    "href": "assignments.html#deadline",
    "title": "Assignments",
    "section": "Deadline",
    "text": "Deadline\n\nThe deadline for handing in is Thursday 23.59"
  },
  {
    "objectID": "assignments.html#assignment-checklist",
    "href": "assignments.html#assignment-checklist",
    "title": "Assignments",
    "section": "Assignment Checklist",
    "text": "Assignment Checklist\nDid you?\n\nMatch your group number with DTU Learn groups\nAdd Group number, names and student ids?\nOnly answer what is defined as “Group Assignment”\nCreate the assignment using Quarto\nModify the YAML-header to ensure encapsulation (see above)\nUse proper markdown headers as defined by #, ##, ###\nWrite a few sentences under each section\nSeparate text clearly from code chunks\nFollow course code styling\nThink about reproducibility with respect to data and libraries?\nRender to html\nCompress the html-file to a zip-file, before upload to DTU Learn\nMake sure to download the upload’ed zip-file and check that everything looks right?"
  },
  {
    "objectID": "project_description.html",
    "href": "project_description.html",
    "title": "Project Description",
    "section": "",
    "text": "Project Groups\nMake sure you have read the exam description and that you are familiar with the rules concerning the use of Artificial Intelligence (generative AI) in teaching and exams at DTU",
    "crumbs": [
      "Course Elements",
      "Project Description"
    ]
  },
  {
    "objectID": "project_description.html#project-groups",
    "href": "project_description.html#project-groups",
    "title": "Project Description",
    "section": "",
    "text": "You will be working in the groups assigned from the beginning of the course\nIMPORTANT: All groups members are responsible for all parts of the project!\nNote, completing the project is considered your exam preparation",
    "crumbs": [
      "Course Elements",
      "Project Description"
    ]
  },
  {
    "objectID": "project_description.html#project-as-a-collaborative-effort",
    "href": "project_description.html#project-as-a-collaborative-effort",
    "title": "Project Description",
    "section": "Project as a Collaborative effort",
    "text": "Project as a Collaborative effort\nAs per the course description at the DTU course base: Active participation in the group work and timely submission of project and code base are both indispensable prerequisites for exam participation.\nThis means, that each group member is expected to:\n\nGenerally participate actively in the group project\nTake responsibility for solving assigned tasks within the project\nWrite and review code and perform commit-/push-/pulls to the project GitHub repository\nMeet and discuss actively with the group members\nSpend 9-10h per week on the project for the full 3 week project period\n\nBio Data Science is a collaborative effort, which is reflected in the design of the project module of this course!",
    "crumbs": [
      "Course Elements",
      "Project Description"
    ]
  },
  {
    "objectID": "project_description.html#expected-time-usage",
    "href": "project_description.html#expected-time-usage",
    "title": "Project Description",
    "section": "Expected time usage",
    "text": "Expected time usage\nAs per the rules for the European Credit Transfer System (ECTS) points, 1 credit equals 28 hours. Therefore, for a 5-person group working for 3 weeks, the expected total project hours is ~150. Setup your collaborative project and this will be more than sufficient to create a full bio data science project. Note, for lab 13, you workload will be a ~10min. presentation therefore the hours for this lab is included.",
    "crumbs": [
      "Course Elements",
      "Project Description"
    ]
  },
  {
    "objectID": "project_description.html#project-description",
    "href": "project_description.html#project-description",
    "title": "Project Description",
    "section": "Project Description",
    "text": "Project Description\n\nAim\nThe aim with the Project module of the course is to allow you to independently work with the course elements, you have been exposed to during the first 9 weeks of teaching. Here, you will synthesise the entire bio data science cycle, thereby internalising the course elements. Moreover, you are to:\n\nUse the tools you have learned in the course and “design and execute a bio data science project focusing on collaborative coding and reproducibility, incl. independently using online resources to seek information about application and technical details of state-of-the-art data science tools”\n\nRecall the “Data Science Cycle”",
    "crumbs": [
      "Course Elements",
      "Project Description"
    ]
  },
  {
    "objectID": "project_description.html#project-requirements",
    "href": "project_description.html#project-requirements",
    "title": "Project Description",
    "section": "Project Requirements",
    "text": "Project Requirements\n\nLocation\nThe project must be placed on the course GitHub organisation and named e.g. group_03_project, replacing 03, with your group number.\n\n\nGitHub README\n\nIMPORTANT: First header in the README has to be “Project Contributors” and then please state the student ids and matching GitHub usernames, so we know who-is-who\nBe sure to include a direct link to your presentation, e.g. the direct link to the lecture in lab 3 is: https://raw.githack.com/r4bds/r4bds.github.io/main/lecture_lab03.html\nSince we are not putting data on GitHub, use the README to supply information on data retrieval\n\n\n\nOrganisation\nYour project must strictly adhere to the organisation illustrated below:\n\n\n\n\n\nWhere directories left-to-right, starting with data and corresponding files are:\n\n\n\ndata: Your data\n\n\n\n_raw: Directory inside data, containing your raw data file(s), which must never be edited\n\nraw_data.xlsx: This need not necessarily be an Excel-file, but should be seen as a placeholder for whichever raw data, your project is based on\n\n01_dat_load.tsv: Your loaded data, e.g. combining multiple sheets from an excel file\n02_dat_clean.tsv: Your data cleaned per your specifications and in tidy format\n03_dat_aug.tsv: Your data with added variables, e.g. like computing a BMI\n\n\n\n\n\nR: Your code\n\n\nNB! Every .qmd-file MUST be end-to-end executable independent from environment variables create in other .qmd-files\n\n01_load.qmd: Loads your data, e.g. combining multiple sheets from an excel file\n02_clean.qmd: Cleans your data per your specifications and converts to tidy format\n03_augment.qmd: Adds new variables, like e.g. computing a BMI\n04_describe.qmd: Descriptive statistics, how many in each group, etc.\n05_analysis_1.qmd: Here goes your first analysis\n06_analysis_2.qmd: Here goes your second analysis and so on\n99_proj_func.qmd: DRY: Don’t Repeat Yourself, repeated code goes into a function\n00_all.qmd: The master document, capable of running the entire project in one go\n\n\n\n\n\nresults: Your results\n\n\n\n*.html: The output from your *.qmds in your R-directory\nkey_plots.png: Whichever of all the nice plots you made, which should end up in your final presentation\n\n\n\n\n\ndoc: Your documents\n\n\n\npresentation.qmd: Your final Quarto Presentation of your project\npresentation.html: Self-contained HTML5 presentation (just as the course slides)\n\n\nImportant: This entails, that the entire project as put on GitHub, can be cloned and then executed end-to-end. Since, we are not putting data on GitHub, if possible included programmatic retrieval of data and/or data instructions on how to easily retrieve the data, e.g. via login, in your GitHub repository README file\n\nBut How???\nYes, we are not putting the data on GitHub, so either include programmatic retrieval of the data or state in your GitHub README file, how you retrieved the data forming the base of the project.\n\n\n\nCode\nYour project must strictly adhere to the Course Code Styling Guide\n\n\nData\nFirst and foremost, you must find a data set you can work with in the project!\n\nIt must be based on a bio data set\nStart out as “dirty” i.e. a completely clean / tidy and analysis-ready data set will not allow you to demonstrate, that you have met the course learning objectives\nIt is advisable, that the data is of limited size, so you do not risk time waste due to long runtimes\nNote, you should demonstrate ability to extract biological insights, but at the same time mind that the focus should be on demonstrating that you master the data science toolbox according to course aim and learning objectives\nRemember, the process is the product!\nNaturally, you cannot reuse the data we have worked with during the exercise labs\n\n\n\nPresentation\n\nA 10-slides-in-10-mins presentation, possibly followed by a few questions\nFollow the IMRAD standard scientific structure:\n\nIntroduction\nMaterials and Methods\nResults (And)\nDiscussion\n\nWith a technical focus, but minding to communicate which-ever biological insights you arrived at\nShould not include all your code, but rather focus on the broader picture and include data summaries and visualisations\nCreated using Quarto Presentation, just as the course slides are\nNOTE: This final presentation in HTML-format must be zipped and uploaded to DTU Learn before deadline, so we can check the GitHub version is identical",
    "crumbs": [
      "Course Elements",
      "Project Description"
    ]
  },
  {
    "objectID": "project_description.html#project-supervision",
    "href": "project_description.html#project-supervision",
    "title": "Project Description",
    "section": "Project Supervision",
    "text": "Project Supervision\n\nAs a point of reference, this project is part of the overall assessment and you will therefore have limited access to supervision\nThink of the project, as a long take-home-assignment\nI highly encourage the use of Piazza for questions, which will be monitored by the teaching team\nAlso, a rather comprehensive list of Project FAQ have been compiled, so be sure to check that out!\nI recommend using your course Quarto documents for reference\nAlso, perhaps you can get input on the Posit Community Pages\n\nIf any aspects of the above is not clear, please reach out to the teaching team!",
    "crumbs": [
      "Course Elements",
      "Project Description"
    ]
  },
  {
    "objectID": "project_faq.html",
    "href": "project_faq.html",
    "title": "Project FAQ",
    "section": "",
    "text": "Text in the Quarto Docs\nMake sure you are familiar with the rules concerning the use of Artificial Intelligence (generative AI) in teaching and exams at DTU",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#text-in-the-quarto-docs",
    "href": "project_faq.html#text-in-the-quarto-docs",
    "title": "Project FAQ",
    "section": "",
    "text": "How much text should we add beyond code?\n\nThe emphasis for the communication of the course is on the presentation. Hence, think of the report text as “What information would I like to have / need in order to follow the code and project?”\n\n\n\nWhat should the narrative be?\n\nDo not think of the text as constituting a report in a classical sense, as you have likely done in other courses. Rather, the text constitutes the underpinnings of a technical report, i.e. the aim is to support the code and clarify what is going on enabling the reader to easily follow the code",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#data",
    "href": "project_faq.html#data",
    "title": "Project FAQ",
    "section": "Data",
    "text": "Data\n\nIs this data set ok to use?\n\nWhere can I find data? Rethink the question: Discuss in your group what are your interests and then find data related to your problem, there are literally terabytes of publicly available data, i.e. don’t just google “data”, be specific\nIn order for you to demonstrate that you master the entire bio data science cycle, you must choose a bio data set, which requires cleaning and tidying\nIt would be good, if the data set requires joining, if not, consider artificially splitting it, to demonstrate that you master joining\nOne approach could be to work on reproducing results from a paper. Here, you can even consider improving the data visualisations or adding something extra\nDo not use a data set from one of the course exercises. Doing so would not allow you to demonstrate that you are independently capable of performing a bio data science analysis\nYou are 100% free to choose bio data from any resource as long as it meets the above requirements\nBe sure to understand that the purpose of the project is for you to synthesise an end-to-end Bio Data Science project demonstrating, that you have met the learning objectives of the course, as stated at the individual labs and on the DTU course base. Hence and importantly, your data must allow you to do so!\nMy data is secret, what do I do? It is common for e.g. PhD-students to bring their own data to the projects, which is perfectly fine. It is however important to note, that the project has to be end-to-end executable and it is the groups responsibility to find a solution, where the data is easy accessible, e.g. with a login.\n\n\n\nHow should we handle NAs in the data?\n\nDepends, does your data set come with a readme, which allows you to make a decision regarding NAs?\nBe careful not to simply drop all NAs as you might drop observations, where columns of interest have complete data\nWhether to remove NAs or not cannot be universally answered, as it is specific to the challenge / data at hand. If it’s meaningful for you to keep NAs, then do so, if not, then remove them.\n\n\n\nHow should we handle binary variables?\n\nIn case your variable contains categories, i.e. values, where the question “Is one larger than the other” is nonsensical, then encode as factors\n\n\n\nHow do we handle nonsensical data points?\n\nIf you believe the data points are nonsensical, e.g. manual typing errors, then exclude them, but be very clear about which data (groups of) points were excluded and why\n\n\n\nCan we subset the data?\n\nYes, if you are particularly interested in a subset of the data, then that is perfectly fine. Just be aware, that any choice you take with the data, you must be able to account for why you took that choice\nYou should NOT just sample 100 observations as was done in the exercises to reduce runtime. However, if your data set is very large, consider down-sampling either randomly or by some metric of association. For the latter e.g. perform a test and identify the top X observations, save those to file and continue from there\nHow big is too big? Well, that depends, can you work with it smoothly and is RStudio responsive? A hard cut is not really meaningful, so instead try to evaluate if it is feasible to work with the data smoothly on a “normal” laptop\nFor large data, try appending “.gz” to your files, when you write them using write_tsv(), this will invoke gzip-compression",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#inputoutput-files",
    "href": "project_faq.html#inputoutput-files",
    "title": "Project FAQ",
    "section": "Input/output files",
    "text": "Input/output files\n\nHow can we get a better overview of the data file flow?\n\nConsider creating a flow chart of “your data journey”. It will force you to think about how files are connected and how input/output flows, check out e.g. draw.io for this\n\n\n\nHow can we write a file with e.g. factor encoding or a nested tibble?\n\nThis can only be done by writing an R-object. However, this is for future reference - in this course if at all possible, stick to flat text files, e.g. .tsv or .csv\n\n\n\nHow do we handle different naming conventions?\n\nDO NOT do a manual search and replace in Excel. That defeats the whole purpose of this course\nFix names using a programmatic approach. The stringr-package is extremely useful in this context\n\n\n\nWhere should we place external files?\n\nConsider creating an “images” folder in the “doc” folder, from where you can input images to your presentation",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#modelling",
    "href": "project_faq.html#modelling",
    "title": "Project FAQ",
    "section": "Modelling",
    "text": "Modelling\n\nWhat should we include in the modelling part?\n\nWe have worked with fitting a linear regression and we have done a PCA. You could do something along those lines or something of similar complexity\nThis is not a modelling course. We briefly visited modelling to bridge the process of going from the raw data to analysis ready and then communication via data visualisation. Therefore, do not start fitting a full fledged machine/deep learning model, it is a time-void, which you cannot “afford” to get sucked into\nAlso, mind that the focus of this course is to make you realise that even though you have been trained in delivering results, then the process of arriving at the results in a reproducible manner is equally important - The process is the product!\n\n\n\nDo we HAVE to do a PCA?\n\nDoes it make sense to do a PCA-analysis in your project? If you have group labels and you want to visualise to see if there is a separation, then a PCA is a good first step\n\n\n\nOur model is not “performing very well”. What should we do?\n\nLeave it as is. The focus of this course is not on the results, but on the reproducible process of arriving and communicating said results",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#coding",
    "href": "project_faq.html#coding",
    "title": "Project FAQ",
    "section": "Coding",
    "text": "Coding\n\nWhat goes in “augment”?\n\nIf you google “Augment“, you will get “make (something) greater by adding to it; increase”, in other words, when you add e.g. variables to your data, which was not there initially, e.g. like we did, when we calculated the BMI from existing information on weight and height\nYou should think of the augment script as the place, where you create your “database” for everything that happens afterwards, i.e. all your analyses and ideas\nCreate that “database” and then load it into your downstream scripts\nAugment == Adding something\nBe aware, that if you downstream discover a need to create a variable, you will need to go back to the augment and update\n\n\n\nWhat goes in “describe”?\n\nDescriptive statistics, e.g. how many in each group in your data, etc.\n“Describe” is for clarifying the content of the data, the starting point of the analysis if you will\nShould contain some key insights before you begin the analysis\nDescribe your data in plots and perhaps some tables if relevant\n\n\n\nCan we mix base R and tidyverse?\n\nNo, you might as well get used to it - Tidyverse all the way! (This is a tidyverse course)\nIf you use base, like e.g. my_data$my_var &lt;- 1:4 or my_data$my_var[1] or similar, then you will get points deducted in the evaluation of your project\nAgain, the process is the product and it matters much if you are using the course correct dialect of R. What you choose to do after the course is your choice\nUsing base R, where course content contains describes the tidyverse alternative is no different from using python and I suppose you wouldn’e expect to be able to hand in python code as the final product in an R course\n\n\n\nWhat if we can’t make it work in tidyverse but only in base R?\n\nIf you can make it work in base R, you can make it work in tidyverse\n\n\n\nIs it okay to use e.g. the base function sum()?\n\nYes, think of it this way: R has as core functionality to do statistics. Tidyverse is for performing the data manipulation surrounding these calculations. Therefore, using functions such as sum(), mean(), sd(), etc. is naturally fine\nE.g. recall, when we did the group_by() \\(\\rightarrow\\) summarise()-workflow, we used the descriptive statistics functions\nAlso, we used the lm()-function for fitting a linear regression.\nTidyverse aims to replace the inconsistent and low legibility of base R with respect to data manipulation and visualisation - It does not replace the core functionality of doing statistics\n\n\n\nHow do we know if there is a tidyverse function we should use rather than base?\n\nIdentify the general area of what you’re working with. E.g. if strings, then go to your RStudio session and find the console and type stringr:: and hit tab, then you can look through the functions\nLook and search in the R4DS2e-book\nAsk on the Posit community pages\nAlso generally base will use e.g. read.table(), where tidyverse will use read_table(), i.e. a period vs. an underscore\n\n\n\nCan we use package X for analysis/visualisation?\n\nIf the package performs a lot of the work, which you are to demonstrate that you can do, then absolutely no. E.g. DO NOT use packages, where you call a plot-my-data-function and the ggplot-magic happens that will NOT allow you to demonstrate that you have met the course learning objectives\nFor an example of and-then-magic-happens, see e.g. the corrplot-package\nBasically, any package, which automatically does “things”, that are part of the learning objectives, will not allow you to demonstrate, that you have met the learning objectives and is thus NOT to be used! You HAVE to show that you can do these things manually\nBut why??? Because using “magic”-packages restricts you to the functionality of those packages, whereas if you understand how to do the details yourself, then you are free as a bird to create anything\n\n\n\nCan we install package X for our project?\n\nIf you need a specific package, please request it at the slack channel for the project and exam. The problem is that with that many groups, then each installing some number of packages, will create an unnecessary load on our storage due to redundancy\n\n\n\nHow do we run the entire project incl. the presentation?\n\nMake sure to include a programmatic call to render a file, i.e. in your run-all qmd file, when you include sub-documents and run the run-all file, the sub-files will not be generated, hence you need to include a programmatic call in your run-all file, which will also render the seperate sub-documents\nNote, you can include one Quarto document in another\n\n\n\nShould we create a Shiny app?\n\nThe project deliverables are the GitHub repo and your presentation\nRemember, you do this project not for me as “your teacher”, but for you to internalize the knowledge you have been exposed to during the initial 10 weeks of teaching\nIf you find Shiny interesting/fun, then by all means, please do create an app\nNote, Shiny apps are optional and will not give extra credit - Be careful with your time!\n\n\n\nShould we create a package?\n\nIf you find Rpackages interesting/fun, then by all means, please do create an Rpackage\nNote, Rpackages are optional and will not give extra credit - Be careful with your time!\n\n\n\nWhen should we create functions?\n\nRemember DRY (Don’t-Repeat-Yourself), so generally, if you do something more than once, it is a function\nHowever, we do not focus much on functions in this course, so don’t put too much effort into creating functions\nPlace functions in 99_proj_functions.R as illustrated in the overview\nDO NOT hide your code away in functions, so that your main script just becomes 10 function calls. For this process-oriented course, show the code in your main qmd-documents, i.e. 01_, 02_, …\n\n\n\nCan we use loops?\n\nNo, you should instead embrace functional programming and use functions from the purrr-package. Revisit lab 6, if needed\n\n\n\nCan we directly copy/paste a code chunk from an online source?\n\nNo, that would be plagiarism. Understand the steps in the chunk and make the code your own\nI acknowledge that for coding this is not completely black and white, but please refrain from a direct copy/paste\n\n\n\nCan we use chatGPT or similar for coding?\n\nNo, of course not, the coding has to be done by the students, not an AI\nHow would you know if we used an AI? I may not be able to tell or I may be, I wouldn’t recommend taking the risk\nOnce through this course, AIs can be a powerful allied, but it requires skills and experience to use an AI productively, otherwise you will end up in “traps”",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#coding-style",
    "href": "project_faq.html#coding-style",
    "title": "Project FAQ",
    "section": "Coding style",
    "text": "Coding style\n\nHow should we comment on our code?\n\nRemember, the point of writing verbose tidyverse code is that the code-becomes-the-comments. Think of it this way: The pipeline is the text on a page in a book, so before your pipeline, put a header/title on what is happening below, just as a title/header in a book\nThe title/header will be what is generally going on, e.g. “Normalise all gene expression values using standard score approach” and the pipeline will be how that is actually done\nThe pipe “%&gt;%” is pronounced as “then”, when you “read” your code\n\n\n\nHow should we style our code?\n\nStrictly adhere to the Course Style Guide as introduced in the course\nMake sure that all scripts are styled the same way and be consistent in your coding style\nYour code should not be &gt;80 chars wide, follow the vertical line in your editor\n\n\n\nHow should we style our plots?\n\nBe concise, “less is more”\nMake some nice and relevant plots. Show us that you’ve learned data visualisation. Remember legends, titles etc.\nDo not put 3 messages in 1 plot. Instead put each message into a different plot\nBe very careful with matching the text size in your plots to that of your presentation (trial-and-error)",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#github",
    "href": "project_faq.html#github",
    "title": "Project FAQ",
    "section": "GitHub",
    "text": "GitHub\n\nHow should we start our project up on GitHub?\n\nUse the “Captain” approach as outlined in the exercises\n\n\n\nWhere should we place the project?\n\nThe project must be placed on the course GitHub rforbiodatascienceXX, where XX is the course year\n\n\n\nShould our GitHub be public or private?\n\nPublic, show the world what you can do!\n\n\n\nShould we keep all the data on GitHub?\n\nNo, GitHub is meant for code, not data, see the project organisation\n\n\n\nWhat goes in the “doc” folder?\n\nThe project organisation chart is a generic figure. You should not hand in a report. Your deliverable/product outcome for the project period is: 1) Your GitHub repository and 2) A presentation. Therefore, the “doc” folder would in this case contain your presentation\n\n\n\nWhy are there multiple “qmd-reports” in the “results” folder?\n\nIn case you have a computationally intensive part of the project it can be an advantage to split into sub-reports and then collect in a master report (think e.g. large LaTeX reports)\nOverview, even if your report is not computationally intensive, then it may be long and splitting into sub-reports facilitate better overview\n\n\n\nShould we include a README on GitHub?\n\nThat would be a good way to briefly introduce what the contents of this repo is and something one would usually do\n\n\n\nShould everything be on Github, also plots we don’t present?\n\nYes, everything!",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#examhand-in-deliverables",
    "href": "project_faq.html#examhand-in-deliverables",
    "title": "Project FAQ",
    "section": "Exam/hand-in deliverables",
    "text": "Exam/hand-in deliverables\n\nWhat does “Independently identify and adapt relevant novel state-of-the-art data science tools” mean?\n\nThis is the exact result of completing the project work and my reason for not doing hand-holding in the project period. You are training your competences in that exact learning objective, so that once you leave this course, you will be able to do just that. This is absolute key as a modern bio data scientist. As an example, I programmed in a programming language called Perl, when I did my bioinformatics PhD, in fact the majority of my fellow phd students did that. I spend a lot of time getting really good at Perl… Today, I never use it… As in ever-never! Technology is under constants development and those with the capabilities stated in this LO will have a competitive edge and remain forefront in their working life\n\n\n\nWhat is the final product?\n\nA GitHub repository organised according to principles of reproducible data analysis\nA Quarto HTML-presentation following the IMRAD structure as elaborated in the project description\nThis final presentation in HTML-format should be uploaded to DTU Learn\n\n\n\nDo we need to hand in a report?\n\nNo, no report is required!",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_faq.html#presentation",
    "href": "project_faq.html#presentation",
    "title": "Project FAQ",
    "section": "Presentation",
    "text": "Presentation\n\nShould we include code in our presentation?\n\nYour presentation should follow the standard scientific IMRAD-structure, i.e. introduction, materials-and-methods, results, and discussion.\nInclude certain decisions you took with the data and your reasoning herfore.\nIf you found a particular challenging problem in coding, to which you found an elegant solution, include that in your presentation as an example\nThe presentation is your chance to practice communicating insights to stakeholders\nYour audience will be same-level bioinformaticians\nPerhaps consider a graphical representation of your process going from raw to analysis-ready data\n\n\n\nShould we include tables or plots?\n\nWhat makes sense? If you only have two numerical values, then perhaps a table is fine. If you have 100 observations, then a plot is likely better\nRemember, we are as humans evolutionary encoded to interpret visual information, not numbers\n\n\n\nHow do we add plots to the Quarto presentation?\n\nOutput the plot as a png file using the function ggsave and include that png in your presentation. Think dynamically, we don’t want to type anything manually in our presentation\n\n\n\nHow do we get the presentation in wide-/full screen?\n\nHit w and f while the HTML presentation is loaded\n\n\n\nShould we split our presentation into sub-presentations?\n\nNo, this is not necessary given the extend/size of this project. Just create one qmd-file, which outputs an HTML-presentation\n\n\n\nWe are working on re-creating paper results. Should we re-create the exact same plots?\n\nIdeally, you re-create the plots and then you create your own improved version of the visualisation\n\n\n\nWhere should we “place” our Shiny app in our presentation?\n\nDemo it briefly at the end, but stay within the assigned per-presentation time\n\n\n\nHow can we illustrate our data handling?\n\nConsider creating a flow chart, what are input files and how are they connected to final output?\n\n\n\nHow much code should we include in the presentation?\n\nThe exam deliverables are two-fold. Code is the GitHub repo and your ability to communicate biological insights is in the presentation. Therefore, code could be included in the presentation if you faced and solved a particularly challenging problem in a clever way\n\n\n\nHow do we include data numbers in the presentation?\n\nYou could from your script output a results table with relevant numbers as a .tsv file and then read that into the presentation and extract the numbers\nRemember, think dynamical reporting, what if your input data changes and you manually entered the numbers in your presentation? Then you wouldn’t be much farther than a powerpoint\n\n\n\nWhat goes into the materials and methods section?\n\nMaterials: What data did you use and where did you get it from?\nMethods: Which modelling did you use? Think of the methods section as a recipe for how to go from raw to results =&gt; Flow chart?\n\n\n\nShould we interpret our results?\n\nYes, you have to think about it as a “normal” project presentation\n\n\n\nWhat about all the stuff we tried, which did not work?\n\nIn the presentation, you should focus on what worked and what results you arrived at\nExclude dead-ends from the presentation, but… Leave them in the repo, perhaps with an initial comment signifying that the following turned out to be a dead-end\nThink about this - Scenario: You’re working in a company and you and your team of 3 spend 6 months on a project, which turns out to be a dead-end. For future reference: Would the company be interested in knowing that this project was a dead end or should you delete everything and never speak of it again?",
    "crumbs": [
      "Course Elements",
      "Project FAQ"
    ]
  },
  {
    "objectID": "project_checklist.html",
    "href": "project_checklist.html",
    "title": "Project Checklist",
    "section": "",
    "text": "Make sure you are familiar with the rules concerning the use of Artificial Intelligence (generative AI) in teaching and exams at DTU\n\nIMPORTANT: Ask yourselves:\n\n\n“Have we remembered to look through the FAQ and description”\n\nThis is essential, this is where all the information on the project is collected\n\n\n\n“Have we included a project README on the GitHub Repository?”\n\nIMPORTANT: First header in the README has to be “Project Contributors” and then please state the student ids and matching GitHub usernames, so we know who-is-who\nBe sure to include a direct link to your presentation, e.g. the direct link to the lecture in lab 3 is: https://raw.githack.com/r4bds/r4bds.github.io/main/lecture_lab03.html\nSince we’re not putting data on GitHub, you need to let us know how to get the data\n\n\n\n“Does our presentation follow the IMRAD structure?”\n\nIs the presentation created as one qmd-file and output to a HTML?\nAre the 10 presentation slides clear and concise? (Title slides does not count)\nDid we include slide numbers in the presentation?\nAre we doing good data communication via good visualisations?\nDo we present a clear overview of the data process incl. any decisions made, e.g. using a flow chart?\nAre we clearly communicating a biological insight?\nAre we following standard guidelines? Sources, references, etc.\n\n\n\n\n\n\n\n\n“Does our project include all components of the data science cycle?”\n\n\n\n\n\n\n\n“Are we aware of and have included learning objectives as appropriate in the project?”\nCheck your project against the course learning objectives, as defined on the DTU course base\n\n\n“Is our project-GitHub organised as instructed and can it run end-to-end via a”doit”?”\n\nA “doit” is a script, which acts as a wrapper executing other scripts. In the project organisation, what would be the 00_all.qmd-file\n\n(Note, this is a generic representation, e.g. you do not need exactly 3 key plots nor exactly 2 analyses)\n\n\n\n\n\n\n\n“Does ALL our code in ALL our files follow the Course Style Guide?”\n\nRecall what we discussed in the course on styling\nSee the Course Style Guide\nE.g.:\n\n\n\n\n\n\n\n\n“Are we using base-R, where we should use tidyverse-R?”\n\nE.g. think about the following:\n\n\n\n\n\n\n\n\n“Can we explain and justify the data decisions in the project?”\n\nIMPORTANT: In essence it does not matter which decision you took, what really matters is your ability to explain and justify why you took that decision, e.g. decided on a particular path in your analysis",
    "crumbs": [
      "Course Elements",
      "Project Checklist"
    ]
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Exam",
    "section": "",
    "text": "Design\nMake sure you are familiar with the rules concerning the use of Artificial Intelligence (generative AI) in teaching and exams at DTU\nThe exam consists of 3 components:\nSee course description at DTU course base for further description",
    "crumbs": [
      "Course Elements",
      "Exam"
    ]
  },
  {
    "objectID": "exam.html#design",
    "href": "exam.html#design",
    "title": "Exam",
    "section": "",
    "text": "A group project, where all members are responsible for all parts of the project. This is handed in as a code base on the course GitHub repository\nAn oral group presentation of the project, where all group members, as per DTU rules on oral exams, must be physically present\nA two hour multiple choice quiz (MCQ) exam, where general course learning objectives are examined",
    "crumbs": [
      "Course Elements",
      "Exam"
    ]
  },
  {
    "objectID": "exam.html#deadlines-and-dates",
    "href": "exam.html#deadlines-and-dates",
    "title": "Exam",
    "section": "Deadlines and Dates",
    "text": "Deadlines and Dates\n\nProject code base and presentation must be completed at the latest 23:59 on the day before lab 13. Note, you cannot edit anything on the GitHub repository after this deadline!\nThe oral group presentation will be on lab 13 of the course, see DTU Learn calender for dates\nThe MCQ is placed according to the DTU exam schedule",
    "crumbs": [
      "Course Elements",
      "Exam"
    ]
  },
  {
    "objectID": "exam.html#content",
    "href": "exam.html#content",
    "title": "Exam",
    "section": "Content",
    "text": "Content\n\nThe Group Project\n\nBe sure that everyone understands ALL code in the project as all group members are responsible for ALL code\nThe aim of the project is to cover the entire course cycle, so be sure to align your project\nBe sure to read Project Description\nQuestions? Make sure to consult the Project FAQ\nFinal check? Be sure to consult the Project Checklist\n\n\n\nThe Presentation\n\nThe format is 10-slides-in-10-mins on the clock\nEveryone in the group must be physically present and present a part of the project\nExpect a few overall questions regarding the decisions you have made throughout your project\nIMPORTANT: The final presentation in HTML-format must be zipped and uploaded to DTU Learn before deadline, so we can check the GitHub version is identical\n\n\n\nThe MCQ\n\nA 2-hour individual multiple choice exam\nIn 2023 the exam contained 80 questions, expect a similar number of questions\nAll aids are allowed, but no open internet\nEach question will have 4 possible answers and only 1 is the right one and you can only choose one answer per question\n\nIf any aspects of the above is not clear, please reach out to the teaching team!",
    "crumbs": [
      "Course Elements",
      "Exam"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Note: These references primarily consist of papers that describe the datasets utilised in the course. Thus, these papers are not required reading for the course.\n\n\nGravier, Eléonore, Gaëlle Pierron, Anne Vincent‐Salomon, Nadège Gruel,\nVirginie Raynal, Alexia Savignoni, Yann De Rycke, et al. 2010. “A\nPrognostic DNA Signature for T1T2 Node‐negative Breast Cancer\nPatients.” Genes, Chromosomes and Cancer 49 (12):\n1125–34. https://doi.org/10.1002/gcc.20820.\n\n\nJackson, AS, PR Stanforth, J Gagnon, T Rankinen, AS Leon, DC Rao, JS\nSkinner, C Bouchard, and JH Wilmore. 2002. “The Effect of Sex, Age\nand Race on Estimating Percentage Body Fat from Body Mass Index: The\nHeritage Family Study.” International Journal of Obesity\n26 (6): 789–96. https://doi.org/10.1038/sj.ijo.0802006.\n\n\nMcCarthy, Mary K., and Jason B. Weinberg. 2015. “The\nImmunoproteasome and Viral Infection: A Complex Regulator of\nInflammation.” Frontiers in Microbiology 6 (January). https://doi.org/10.3389/fmicb.2015.00021.\n\n\nNoble, William S. 2009. “How Does Multiple Testing Correction\nWork?” Nature Biotechnology 27 (12): 1135–37. https://doi.org/10.1038/nbt1209-1135.\n\n\nNolan, Sean, Marissa Vignali, Mark Klinger, Jennifer N. Dines, Ian M.\nKaplan, Emily Svejnoha, Tracy Craft, et al. 2020. “A Large-Scale\nDatabase of t-Cell Receptor Beta (TCRβ) Sequences and Binding\nAssociations from Natural and Synthetic Exposure to SARS-CoV-2.”\nAugust. https://doi.org/10.21203/rs.3.rs-51964/v1.\n\n\nSchorling, John B., Julienne Roach, Marjorie Siegel, Natalie Baturka,\nDawn E. Hunt, Thomas M. Guterbock, and Herbert L. Stewart. 1997.\n“A Trial of Church-Based Smoking Cessation Interventions for Rural\nAfrican Americans.” Preventive Medicine 26 (1): 92–101.\nhttps://doi.org/10.1006/pmed.1996.9988.\n\n\nTorondel, Belen, Jeroen H. J. Ensink, Ozan Gundogdu, Umer Zeeshan Ijaz,\nJulian Parkhill, Faraji Abdelahi, Viet-Anh Nguyen, et al. 2016.\n“Assessment of the Influence of Intrinsic Environmental and\nGeographical Factors on the Bacterial Ecology of Pit Latrines.”\nMicrobial Biotechnology 9 (2): 209–23. https://doi.org/10.1111/1751-7915.12334.\n\n\nWillems, James P., J Terry Sanders, Dawn E. Hunt, and John B. Schorling.\n1997. “Prevalence of Coronary Heart Disease Risk Factors Among\nRural Blacks: A Community-Based Study.” Southern Medical\nJournal 90 (8): 814–20. https://doi.org/10.1097/00007611-199708000-00008.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "lab05.html#packages",
    "href": "lab05.html#packages",
    "title": "Lab 5: Data Wrangling II",
    "section": "",
    "text": "dplyr\nstringr\ntidyr\nforcats\npatchwork\nggseqlogo\ntable1",
    "crumbs": [
      "Course Labs",
      "Lab 5: Data Wrangling II"
    ]
  },
  {
    "objectID": "lab06.html#packages",
    "href": "lab06.html#packages",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "",
    "text": "broom\npurrr",
    "crumbs": [
      "Course Labs",
      "Lab 6: Applying Functional Programming with Purrr to Models"
    ]
  },
  {
    "objectID": "lab07.html#packages",
    "href": "lab07.html#packages",
    "title": "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio",
    "section": "",
    "text": "usethis\ngitcreds\ngit (Actually not an R-package this time)",
    "crumbs": [
      "Course Labs",
      "Lab 7: Collaborative Bio Data Science using git and GitHub via RStudio"
    ]
  },
  {
    "objectID": "lab08.html#packages",
    "href": "lab08.html#packages",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "",
    "text": "devtools\nusethis\nroxygen2\ntestthat\ngert",
    "crumbs": [
      "Course Labs",
      "Lab 8: Creating a Simple R-package"
    ]
  },
  {
    "objectID": "lab09.html#packages",
    "href": "lab09.html#packages",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "",
    "text": "shiny\ngolem",
    "crumbs": [
      "Course Labs",
      "Lab 9 Creating a Simple Shiny Application"
    ]
  },
  {
    "objectID": "lab10.html#schedule",
    "href": "lab10.html#schedule",
    "title": "Lab 10 Project Startup & Industry Talks",
    "section": "",
    "text": "08.00 - 08.15: Recap of Lab 9\n08.15 - 08.45: Introduction to Project Period and Exam\n08.45 - 09.00: Break\n09.00 - 12.00: Mini Symposium",
    "crumbs": [
      "Course Labs",
      "Lab 10 Project Startup & Industry Talks"
    ]
  },
  {
    "objectID": "primer_logistic_regression.html",
    "href": "primer_logistic_regression.html",
    "title": "Primer on Logistic Regression in R",
    "section": "",
    "text": "Logistic Regression\ngravier_wide &lt;- gravier |&gt;\n  bind_cols() |&gt;\n  as_tibble()\ngravier_wide\n\n# A tibble: 168 × 2,906\n      g2E09    g7F07    g1A01   g3C09      g3H08    g1A08    g1B01   g1int1\n      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n 1 -0.00144 -0.00144 -0.0831  -0.0475  0.0158    -0.0336  -0.136    0.0180 \n 2 -0.0604   0.0129  -0.00144  0.0104  0.0316     0.108    0.0158   0.0800 \n 3  0.0398   0.0524  -0.0786   0.0635 -0.0395     0.0342   0.00288  0.0594 \n 4  0.0101   0.0314  -0.0218   0.0215  0.0868     0.0272  -0.0160   0.0759 \n 5  0.0496   0.0201   0.0370   0.0311  0.0207    -0.0174   0.111   -0.0469 \n 6 -0.0664   0.0468   0.00720 -0.370   0.00288    0.0243   0.0909   0.0482 \n 7 -0.00289 -0.0816  -0.0291  -0.0249 -0.0174     0.0172  -0.170   -0.00578\n 8 -0.198   -0.0499  -0.0634  -0.0298  0.0300     0.00144 -0.0529  -0.0401 \n 9  0.00288  0.0201   0.0272   0.0174 -0.0000789 -0.0634   0.0370   0.0314 \n10 -0.0574  -0.0574  -0.0831  -0.0897 -0.101     -0.144   -0.167    0.0172 \n# ℹ 158 more rows\n# ℹ 2,898 more variables: g1E11 &lt;dbl&gt;, g8G02 &lt;dbl&gt;, g1H04 &lt;dbl&gt;, g1C01 &lt;dbl&gt;,\n#   g1F11 &lt;dbl&gt;, g3F05 &lt;dbl&gt;, g3B09 &lt;dbl&gt;, g1int2 &lt;dbl&gt;, g2C01 &lt;dbl&gt;,\n#   g1A05 &lt;dbl&gt;, g1E01 &lt;dbl&gt;, g1B05 &lt;dbl&gt;, g3C05 &lt;dbl&gt;, g3A07 &lt;dbl&gt;,\n#   g1F01 &lt;dbl&gt;, g2D01 &lt;dbl&gt;, g1int3 &lt;dbl&gt;, g1int4 &lt;dbl&gt;, g1D05 &lt;dbl&gt;,\n#   g1E05 &lt;dbl&gt;, g1G05 &lt;dbl&gt;, g1C05 &lt;dbl&gt;, g1G11 &lt;dbl&gt;, g2D08 &lt;dbl&gt;,\n#   g2E06 &lt;dbl&gt;, g3H09 &lt;dbl&gt;, g2F09 &lt;dbl&gt;, g3G06 &lt;dbl&gt;, g2G08 &lt;dbl&gt;, …\nWhat if we wanted to understand how a continous variable like e.g. gene_expression_level of a given gene, determined whether the plant would survive or sucomb to heat shock at a given temperature? In that case, we would be interested in understanding survived yes/no as a function of gene_expression_level:\n\\[survived_{yes/no} \\sim gene\\_expresssion\\_level\\]\nMoreover, we are interested in estimating the probability of survival given a certain gene_expression_level.",
    "crumbs": [
      "Primers",
      "Primer on Logistic Regression in `R`"
    ]
  },
  {
    "objectID": "primer_functions.html",
    "href": "primer_functions.html",
    "title": "Primer on Functions in R",
    "section": "",
    "text": "The Basics\nA function in its simplest form takes some “input” and does “something” and then returns the “result”:\nf &lt;- function(input){\n  # Do \"something\" to get the result\n  return(result)\n}\nSo the function “acts” upon the “input” and thereby creates the result. Since we are working with R for Bio Data Science, let us use DNA as an example of input and then choose transcription as the act upon the input, whereby messenger RNA would be the output. This would look like so:\ntranscribe_dna &lt;- function(DNA){\n\n  # First we initiate\n  mRNA &lt;- \"\"\n  \n  # Then we elongate by going through all\n  # of the nucleotides in the input DNA\n  for( i in 1:nchar(DNA) ){\n    \n    # To elongate, first we specify\n    # the i'th nucleotide in the input DNA\n    nucleotide &lt;- substr(DNA, i, i)\n    \n    # Then we define the as of yet unknown\n    # result of the transcription\n    transcribed_nucleotide &lt;- \"\"\n    \n    # And then we see if the input nucleotide\n    # is \"a\", \"c\", \"g\" or \"t\" and perform the \n    # appropriate transcription:\n    if( nucleotide == \"a\" ){\n      transcribed_nucleotide &lt;- \"u\"\n    } else if(nucleotide == \"c\"){\n      transcribed_nucleotide &lt;- \"g\"\n    } else if(nucleotide == \"g\"){\n      transcribed_nucleotide &lt;- \"c\"\n    } else if(nucleotide == \"t\"){\n      transcribed_nucleotide &lt;- \"a\"\n    }\n    \n    # Perform the elongation\n    mRNA &lt;- paste0(mRNA, transcribed_nucleotide)\n  }\n  \n  # And finally, we terminate\n  return(mRNA)\n}",
    "crumbs": [
      "Primers",
      "Primer on Functions in `R`"
    ]
  },
  {
    "objectID": "primer_r_packages.html#how-to-work-with-an-r-package",
    "href": "primer_r_packages.html#how-to-work-with-an-r-package",
    "title": "Primer on R package development",
    "section": "",
    "text": "Dependencies\nThe one strict rule is Never use library(\"package\") within a package! \nInstead, add the packages your are going to use to the DESCRIPTION file and in the function descriptions. This is done by running usethis::use_package(\"packageName\") in the console and adding @import package (OK) or @importFrom package function1 function2 ... (Best). Using the functions in your package is then done with package::function() (e.g., dplyr::mutate()) or omitting the package::.\nThis way, it is easy to read what functions are from your package, and your package namespace does not get cluttered. Read more in the Namespace section.\nIt should also be a goal to make your package depend on as few other packages as possible. The user will need to install all packages your package depends on, but then also every package those depends on - that list quickly becomes quite long if you are not careful.\n\n\nFunctions\nA package is typically a collections of functions.\nThese functions are stored in .R files in the R folder. A good starting point is to create an .R file for each function. But, as the package becomes bigger, it often makes sense to combine related functions into bigger files.\nYou can quickly create a new .R file with usethis::use_r(\"function_name\"). Or do it manually, as you are used to.\n\n\nDocumenting functions\nTry running ?mean in the Console.\nIf you have every wondered how to write a manual like the one that pops up, please click here and read - if not, consider reading it anyway, as you will use it later.\nWhen you have made a function, or have at least defined one, you should describe what it does and how to use it. The manual you write for your function is the function documentation, and it should describe the purpose of the function and how to use it.\nYou can read extensively about it here, but I will give you the most essential information to get you started.\nThe R package roxygen2 makes this easy as 1-2-3. It is part of devtools and is already installed. It uses #' comments above the function. @ tags lets you add specific details to the function documentation.\nCreate an roxygen skeleton by clicking somewhere in your function. Go to the ‘Code’ tab in the top of your window and select ‘Insert Roxygen Skeleton’.\nThis will look something like this:\n\n#' Title\n#'\n#' @param foo \n#' @param bar \n#'\n#' @return\n#' @export\n#'\n#' @examples\nmyFunction &lt;- function(foo, bar){\n  # Do stuff with foo and bar\n  foobar &lt;- (foo * bar) / (foo + bar)\n  return(foobar)\n}\n\nThis allows you to add the most basic descriptions. To begin with, the Title, @param, and @export are the most important, you may remove the other tags for now. A more detailed example is given here. There, you can also read about documenting datasets and other object types - even the package itself.\n\n\nNamespace\nYour package namespace can quickly become very cluttered, if you are not careful.\nTherefore, follow these rules:\n\nOnly @export the functions the users will use. Removing the tag makes the function internal and hides it from your package namespace. It can still be freely used within your package and accessed outside your package with package:::internal_function()\nMake your code explicit with package::function().\n\nThis step is not mandatory, but makes reading the code easier.\n\nAdd your dependencies in the DESCRIPTION file with usethis::use_package(\"packageName\")\nOnly very rarely use the @import tag. Aim to use the @importFrom tags in your function descriptions instead.\n\nYou can read more extensively about namespace here.\n\n\nTesting\nTesting is essential\n\nto ensure your package runs smoothly and that no bugs are introduced when you make a seemingly minor change. It is handled with the testthat package, which is also installed with devtools.\nI will not go into too much detail here, but know that testing is an important, but often neglected, part of building a package. You can read more about it here.\nEvery time you run the usethis::use_r() function to create a new script, the function encourages you to create a test alongside the new function. I recommend you follow that advise.\nYou create a test by running usethis::use_test(\"function name\").\nThe function creates a new folder tests and creates a test script for the function. The good R package creator writes a few tests for every function.\nThe exercises will ask you to make a simple test for every function, introducing you to the concept.\n\n\nThe Package Workflow\nWhen creating a package, it is important to test your work along the way.\nYou can do that in many ways, but I recommend the following workflow:\n\nWrite a function / make a change\n\nIf it is a new function, document it\n\nSave your files: rstudioapi::documentSaveAll()\nCreate package documentation: devtools::document()\n\n\nIf at this point, you get a warning that NAMESPACE already exists, delete it and try again.\n\n\nLoad package: devtools::load_all()\nYour package is now loaded, and you can test that it works as intended.\n\nOptionally, you can save the three lines of code in dev/load.R and run the lines with source(\"dev/load.R\"). If you do, add the dev folder to the .Rbuildignore file.",
    "crumbs": [
      "Primers",
      "Primer on R package development"
    ]
  },
  {
    "objectID": "guide_working_locally.html#download-course-materials",
    "href": "guide_working_locally.html#download-course-materials",
    "title": "Guide for Working Locally",
    "section": "",
    "text": "Go to the R for Bio Data Science RStudio Cloud Server\nLogin using your credentials\nIn the file pane, navigate to your projects-directory, where (if you followed course instructions) you will find your r_for_bio_data_science-directory\nTick the box to the left of your r_for_bio_data_science-directory\nClick the gear cog, which says More\nClick Export...\nYou should now be prompted to download r_for_bio_data_science.zip\nClick Download",
    "crumbs": [
      "Guides",
      "Guide for Working Locally"
    ]
  },
  {
    "objectID": "guide_building_r4ds2e_locally.html#clone-repository",
    "href": "guide_building_r4ds2e_locally.html#clone-repository",
    "title": "Guide for Building the R4DS2e Book Locally",
    "section": "",
    "text": "In the upper right corner of RStudio, click the current project\nChoose New Project...\nChoose Version Control\nChoose Git\nUnder Repository URL:, enter https://github.com/hadley/r4ds\nUnder Project directory name:, enter r4ds\nUnder Create project as subdirectory of: uge the Browse...-button to navigate to where you want to place this project\nClick Create Project\n\n\n\nIn the RStudio Files pane of your new project, find and open the file index.qmd\nClick the render-button above the index.qmd-file you just opened\nIt probably will not run on the first try! Make sure to check the errors, e.g. Error in library(ggthemes) : there is no package called 'ggthemes' and then run e.g. install.packages(\"ggthemes\")\nAgain, click the render-button above the index.qmd-file (Be aware that RStudio will show you the file, where the missing package is mentioned, so you will have to make sure, that you click the index.qmd-file tab) and redo installation of any missing packages (For Restart R error, see below)\nOnce all missing packages have been installed, the rendered book will appear in the Viewer pane. Click the small white and lightblue icon with and arrow\nThe book will now open locally in your browser on e.g. http://localhost:7829/index.html",
    "crumbs": [
      "Guides",
      "Guide for Building the R4DS2e Book Locally"
    ]
  },
  {
    "objectID": "lab02.html#packages",
    "href": "lab02.html#packages",
    "title": "Lab 2: Data Visualisation I",
    "section": "",
    "text": "ggplot2",
    "crumbs": [
      "Course Labs",
      "Lab 2: Data Visualisation I"
    ]
  },
  {
    "objectID": "primer_git_cli.html",
    "href": "primer_git_cli.html",
    "title": "git command line vs. RStudio GUI",
    "section": "",
    "text": "GitHub credentials\nIf you are handy with the terminal, it is a good idea to learn git’s command-line interface. When using git from the terminal, you can pair it with any code editor or programming language without worrying about where the git buttons are or if they behave slightly differently between different code editors. The error messages are also much easier to understand and you get the general advantages of the terminal: it is faster and automatable.\nOf course, both the GUI and the Terminal are powerful, and it’s completely normal to use a combination of both, but even if you prefer only the GUI, it is still useful to know the terminal version for when you use a code editor different than RStudio.\nThis primer will explain the basics of the git command-line interface by comparing it to the RStudio Git tab.\nWhen pushing to a GitHub repository, or when pulling from a private one, GitHub needs to authenticate that you are the actual owner of your account. In Lab 7 we used Personal Access Tokens, but when working purely from the Linux terminal, it is most common to use SSH keys.\nFirst, create an ssh key by running ssh-keygen (leave everything empty by pressing enter a few times) and then cd into ~/.ssh.\nYou will see 1 or more pairs of files named &lt;whatever&gt; and &lt;whatever&gt;.pub. Copy the contents the one ending in .pub (if there are multiple, any of them will work). Then add it to your GitHub account by following these instructions.",
    "crumbs": [
      "Primers",
      "`git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#creating-a-repository",
    "href": "primer_git_cli.html#creating-a-repository",
    "title": "Primer on git command line vs. RStudio GUI",
    "section": "",
    "text": "Converting an existing folder into a git repository\nIn the terminal, cd into the directory with code you want to turn into a git repository and run git init. From then you can use all of git’s functionality.\nIn the git repository hosting service of your choice, you will find instructions on how to pair a newly created repo with your local git repo. On GitHub, simply follow the instructions under …or push an existing repository from the command line that will appear in your newly created repo.\n\n\nCreating a repository from scratch\nWhen creating an empty repository from",
    "crumbs": [
      "Primers",
      "Primer on `git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#creating-a-repository-and-connecting-it-to-github",
    "href": "primer_git_cli.html#creating-a-repository-and-connecting-it-to-github",
    "title": "git command line vs. RStudio GUI",
    "section": "Creating a repository and connecting it to GitHub",
    "text": "Creating a repository and connecting it to GitHub\nThese instructions are valid regardless of whether you want to create a completely empty repository or whether you want to convert an existing RStudio project (or any folder with code) into a git repository. To create an empty one, simply create the folder or RStudio project and go from there.\n\nTerminal\ncd into the directory with code you want to turn into a git repository and run git init. From then onward you can use all of git’s functionality. You should notice that a hidden folder named .git has been created.\nOn whatever git repository hosting service you use (e.g. GitHub), you will find instructions on how to pair a newly created repo with your local git repo. On GitHub, simply follow the instructions under …or push an existing repository from the command line that will appear in your newly created repo (you must have created at least 1 commit).\n\n\nRStudio\n\nClick on your .Rproj file under the Files tab.\nClick on Git/SVN\nOn Version control system:, select git\nWhen asked if you want to initialize a git repository, click on Yes (if it asks you to restart RStudio, say yes again)\n\nAt this point, you can create commits and branches as you like, or go back to any previous commit. However the local repo is not connected to GitHub yet, so you will not be able to pull or push.\nTo the best of our knowledge, there is no way to do this from the RStudio GUI, but can you always just follow the Terminal instructions above (skipping the git init, which is what you just did) or in the case of creating a new project, simply create it as we did in Lab 7.",
    "crumbs": [
      "Primers",
      "`git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#pulling-and-pushing",
    "href": "primer_git_cli.html#pulling-and-pushing",
    "title": "git command line vs. RStudio GUI",
    "section": "Pulling and pushing",
    "text": "Pulling and pushing\nThe equivalent of the  Pull button is git pull, while the  Push button is git push.",
    "crumbs": [
      "Primers",
      "`git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#checking-the-repository-status",
    "href": "primer_git_cli.html#checking-the-repository-status",
    "title": "git command line vs. RStudio GUI",
    "section": "Checking the repository status",
    "text": "Checking the repository status\nThe git status command will show you a summary of any changed and/or new files that are not committed. In RStudio, the equivalent is simply looking at the Status column of the Git tab.",
    "crumbs": [
      "Primers",
      "`git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#creating-commits",
    "href": "primer_git_cli.html#creating-commits",
    "title": "git command line vs. RStudio GUI",
    "section": "Creating commits",
    "text": "Creating commits\n\nAdding to the staging area\nTo add a file to the staging area, you do:\ngit add &lt;file_path&gt; # you can add as many paths as you want\nThis is the equivalent of ticking the check boxes of the Staged column in the Git tab of Rstudio.\n\n\nReviewing and commiting the contents of the staging area\nWhen you click the Diff or Commit button in the RStudio Git tab, you will get a nice visualization with deleted (or modified) lines of code in red, and new lines in green. On the top right of that, you can then add a commit message and click Commit.\nIn the terminal you can get the nice visualization with:\ngit diff --staged\nAnd once you have reviewed it, you can commit by running:\ngit commit -m \"&lt;your_commit_message&gt;\"",
    "crumbs": [
      "Primers",
      "`git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#branching",
    "href": "primer_git_cli.html#branching",
    "title": "git command line vs. RStudio GUI",
    "section": "Branching",
    "text": "Branching\nTo create a new branch in RStudio, you click the new branch button () in the Git tab and switch between branches with the little dropdown to the right of the new branch button.\nIn the terminal, you create the branch with:\ngit branch &lt;your_branch_name&gt;\nYou can verify that it was created by listing existing branches with git branch (simply running it without a branch name). Note that it will only list branches in your local repo, which will not necessarily contain all branches from GitHub. Add the --all option to also show branches in the remote repository (i.e., the GitHub repo).\nYou can switch to the newly created branch (or any other branch) with:\ngit checkout &lt;your_branch_name&gt;\nAlternatively, you can create the branch and switch to it at once by adding the -b option to checkout:\ngit checkout -b &lt;your_branch_name&gt;\n\nMerging branches\nIn Lab 7, we only did branch merging via a Pull request, but this is a feature of GitHub which doesn’t exist in git itself. Under the hood, what GitHub is doing is simply run a git merge when you approve a Pull request.\nWith just git, you first git checkout to the branch you are going to merge into. For example, if you are merging the branch new_feature into main, then at this point you must switch to main and run:\ngit merge new_feature",
    "crumbs": [
      "Primers",
      "`git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#time-travelling",
    "href": "primer_git_cli.html#time-travelling",
    "title": "git command line vs. RStudio GUI",
    "section": "Time travelling",
    "text": "Time travelling\n\nLooking into the past\nThe window that pops up when clicking the History button on the Git tab has a lot of information, which can be obtained from the terminal with a few different commands.\nThe git log command will show you a list of all the existing commits. The long “random” code on each commit is code is called the commit hash and is a unique identifier for each commit.\nYou can view the exact changes that were made in that commit by doing:\ngit show &lt;commit_hash&gt;\nOr see all the (cumulative) changes between 2 commits with:\ngit diff &lt;commit_hash_1&gt; &lt;commit_hash_2&gt;\n\n\nWhen this baby hits 88 miles per hour…\nThe same checkout command that we use to change branches can be used to “revert” the repository to any previously commited version:\ngit checkout &lt;commit_hash&gt;\nAfter running that, all your files will be in the exact state that it was when you did that commit.\nOnce you are done investigating why you have a bug that didn’t exist before your colleague started messing with the code, you can git log again to find the most recent commit and checkout back to the future.\nAs far as we know, there is no way of reverting back to a previous commit from the RStudio GUI, it can only be done via the terminal.",
    "crumbs": [
      "Primers",
      "`git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#setting-git-options",
    "href": "primer_git_cli.html#setting-git-options",
    "title": "git command line vs. RStudio GUI",
    "section": "Setting git options",
    "text": "Setting git options\nThe equivalent of the usethis::use_git_config(&lt;option_name&gt; = &lt;value&gt;) function that we used in Lab 7 is:\ngit config --global &lt;option_name&gt; &lt;value&gt;\nThe --global flag will make the option persist across all your git repositories, but you can remove it to apply the configuration to only the current repo. You can undo it with:\ngit config unset --global &lt;option_name&gt;",
    "crumbs": [
      "Primers",
      "`git` command line vs. RStudio GUI"
    ]
  },
  {
    "objectID": "primer_git_cli.html#github-credentials",
    "href": "primer_git_cli.html#github-credentials",
    "title": "Primer on git command line vs. RStudio GUI",
    "section": "GitHub credentials",
    "text": "GitHub credentials\nWhen pushing to a GitHub repository, or when pulling from a private one, GitHub need to authenticate that you are the actual owner of your account. In Lab 7 we used Personal Access Tokens, but when working purely from the Linux terminal, it is most common to use SSH keys.\nFirst, create an ssh key by running ssh-keygen (leave everything empty by pressing enter a few times) and then cd into ~./.ssh.\nYou will see 1 or more pairs of files named whatever and whatever.pub. Copy the contents the one ending in .pub (if there are multiple, any of them will work). Then add it to your GitHub account by following these instructions.",
    "crumbs": [
      "Primers",
      "Primer on `git` command line vs. RStudio GUI"
    ]
  }
]