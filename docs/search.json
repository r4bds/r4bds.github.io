[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Bio Data Science",
    "section": "",
    "text": "A tidy approach to wrangling, exploring, visualising and communicating bio data with an emphasis on doing collaborative and reproducible bioinformatics projects \nLeon Eyrich Jessen & the TA team\n\nWelcome to R for Bio Data Science\nSo, you signed up for the 22100/22160 bioinformatics study line course - Congratulations! That was your first step towards getting a set of bio data science skills, which will serve you through your future career regardless of your path!\nInspirational quotes can be cliche, however this one hits the nail on the head:\n\n‚ÄúThink about the readability of your code. Every project your work on is fundamentally collaborative. Even if you are not working with any other person, you are always working with future you and you really do not want to be in a situation where future you has no idea what past you was thinking, because past you will not respond to any emails!‚Äù Hadley Wickham\n\nBio Data Science in intrinsically collaborative (even if it‚Äôs just you working) and intrinsically interdisciplinary, so collaborative-, reproducibility- and communication- skills are key. In this course, you will learn how to do modern project oriented collaborative bio data science in tidyverse R - Welcome!"
  },
  {
    "objectID": "prologue.html",
    "href": "prologue.html",
    "title": "Prologue",
    "section": "",
    "text": "The course is designed as a semi-flipped classroom, with an emphasis on active learning. This means that during the 4h classes, the first hour will be dedicated to reviewing key points from last week and then a brief introduction to the topic of the day followed by a break. Hereafter, the students will work hands-on in groups on computational exercises using cloud computing infrastructure. The exercises will rely on relevant bioinformatics data from publicly available databases and gradually build the students toolbox with an emphasis on collaborative project work. This part will take up the first 9 labs. Each lab is defined by a set of specified learning objectives, it is essential that students continuously make sure, that they are on track with these LOs.\nThe fist hour of the 10th lab is dedicated to introducing the project part of the course and the subsequent 3h are dedicated to a mini symposium on ‚ÄúApplication of R for Bio Data Science in Industry‚Äù. Here students will get a change to get insights into how the course topics are implemented in industry and get a glimpse into what options are available upon completing their education. This hybrid event typically attracts ~250 participants and have featured talks from major national and international Pharma/biotech companies, such as: Novo Nordisk, Lundbeck, Chr. Hansen, Bristol Meyer Squibb, a.o.\nIn the project part of the course, students will form groups of 4-5 students based on common interests. Hereafter, the students will seek out and select a data set, which will form the foundation for the project work. In the project work, the students will go through the entire data science cycle and produce the code base for a complete bioinformatics project. This entails a complete synthesis of all components of the exercise labs and supports the collaborative aspect. The groups must then condense the project into a presentation, thereby addressing communicative competencies as an essential part of being a modern bio data scientist."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "This section contain essential information for getting up and running for the classes"
  },
  {
    "objectID": "getting_started.html#when-and-where",
    "href": "getting_started.html#when-and-where",
    "title": "Getting Started",
    "section": "When and Where",
    "text": "When and Where\nTeaching sessions will be E3A, Tuesday mornings 8 - 12 in building 358, room 060a (exercises: Also room 045) and the general schedule will be:\n\n08.00 - 08.30 Recap of key points from last weeks exercises\n08.30 - 09.00 Introduction to theme of the day\n09.00 - 12.00 Exercises"
  },
  {
    "objectID": "getting_started.html#logging-onto-cloud-server",
    "href": "getting_started.html#logging-onto-cloud-server",
    "title": "Getting Started",
    "section": "Logging onto Cloud Server",
    "text": "Logging onto Cloud Server\nFirst, make sure you have a working DTU account, either as a student id or employee initials (e.g.¬†PhD-students or postdocs). In this course we will be using a cloud server infrastructure to perform our work. Click below to access the cloud server:\n\nR for Bio Data Science Cloud Server\n\nNote, that there is a 24h time out on the sessions, meaning that if you logon and forget to sign out, your session will be terminated after 24h."
  },
  {
    "objectID": "getting_started.html#setting-up-a-github-account",
    "href": "getting_started.html#setting-up-a-github-account",
    "title": "Getting Started",
    "section": "Setting up a GitHub account",
    "text": "Setting up a GitHub account\nPrior to class, please go to GitHub and setup and account, shouldn‚Äôt take long. During the registration process you can set up a student account, which will give you additional benefits, including GitHub Pro. In order to get it, you need to use your DTU email when setting account and select Apply for your GitHub student benefits when asked during the registration process. You‚Äôll be then asked to apply for GitHub Student Developer Pack, which will require uploading your student id photo. The process of confirming a student account may take up to a few days (however, it can be almost instantaneous). In the meantime you can already use your free account. Free account should be sufficient for this class, so if you don‚Äôt want to set up a school account, you don‚Äôt need to. Please state your GitHub username in this google sheet."
  },
  {
    "objectID": "getting_started.html#class-communication",
    "href": "getting_started.html#class-communication",
    "title": "Getting Started",
    "section": "Class Communication",
    "text": "Class Communication\nThis course has grown from ~35 students in 2020 to ~150 students in 2023, therefore this term we will be using Piazza for class discussion. The system is highly catered to getting you help fast and efficiently from classmates, the TAs, and myself. Rather than emailing questions to the teaching staff, I encourage you to post your questions on Piazza. If you have any problems or feedback for the developers, email team@piazza.com.\nFind our class signup link at: https://piazza.com/dtu.dk/fall2023/22100 and see the Intro for Students video."
  },
  {
    "objectID": "getting_started.html#group-formation",
    "href": "getting_started.html#group-formation",
    "title": "Getting Started",
    "section": "Group Formation",
    "text": "Group Formation\nThe backbone of this course is modern collaborative data science and as such ‚Äúactive participation in the group work ‚Ä¶ [is an] indispensable prerequisites for exam participation‚Äù as stated in the course base. Furthermore, It is important that course participants prioritise to be present during classes as the course design is based on student-student interaction in an active learning environment.\nTherefore, students will work in groups of 4-5 students. Group formation sheet can be found here"
  },
  {
    "objectID": "lab00.html",
    "href": "lab00.html",
    "title": "Course Labs",
    "section": "",
    "text": "This chapter provides a complete overview of the course curriculum"
  },
  {
    "objectID": "lab01.html",
    "href": "lab01.html",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "",
    "text": "Base"
  },
  {
    "objectID": "lab01.html#schedule",
    "href": "lab01.html#schedule",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Arrival and 5 min. pre-course anonymous survey: Click here to answer\n08.15 - 08.45: Lecture: Course Introduction\n08.45 - 09.00: Break\n09.00 - 11.15: Exercises\n11.15 - 11.30: Break\n11.30 - 12.00: Lecture: Reproducibility in Modern Bio Data Science"
  },
  {
    "objectID": "lab01.html#learning-materials",
    "href": "lab01.html#learning-materials",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nRead the full course description here: 22100 / 22160\nAnswer this brief anonymous R for Bio Data Science Pre-course Questionnaire\nRead course site sections: Welcome to R for Bio Data Science, Prologue and Getting Started and perform any small tasks mentioned\nR4DS2e Book: Welcome, Preface to the second edition, Chapter 1, Chapter 3, Chapter 29 (Don‚Äôt do the exercises)\nVideo: RStudio for the Total Beginner\nWeb: Read the detailed course description\nPaper: Ten Simple Rules for Effective Statistical Practice\nPaper: A Quick Guide to Organizing Computational Biology Projects"
  },
  {
    "objectID": "lab01.html#learning-objectives",
    "href": "lab01.html#learning-objectives",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nMaster the very basics of R\nNavigate the RStudio IDE\nCreate, edit and run a basic Quarto document\nExplain why reproducible data analysis is important, as well as identify relevant challenges and explain replicability versus reproducibility\nDescribe the components of a reproducible data analysis"
  },
  {
    "objectID": "lab01.html#sec-exercises",
    "href": "lab01.html#sec-exercises",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Exercises",
    "text": "Exercises\nToday, we will focused on getting you started and up and running with the first elements of the course, namely the RStudio IDE (Integrated Developer Environment) and Quarto. If the relationship between R and RStudio is unclear, think of it this way: Consider a car, in that case, R would be the engine and RStudio would be the rest of the car. Neither is particularly useful, but together they form a functioning unit. Before you continue, make sure you in fact did watch the ‚ÄúRStudio for the Total Beginner‚Äù video (See the Learning Materials for todays session).\n\nCloud server and the RStudio IDE\nGo to the R for Bio Data Science Cloud Server and follow the login procedure. Upon login, you will see this:\n\n\n\n\n\nThis is the RStudio IDE. It allows you to consolidate all features needed to develop R code for analysis. Now, click Tools \\(\\rightarrow\\) Global Options... \\(\\rightarrow\\) Pane Layout and you will see this:\n\n\n\n\n\nThis outlines the four panes you have in your RStudio IDE and allow you rearrange them as you please. Now, re-arrange them, so that they look like this:\n\n\n\n\n\nClick Apply \\(\\rightarrow\\) OK and you should see this:\n\n\n\n\n\n\n\nFirst steps\n\nThe Console\nNow, in the console, as you saw in the video, you can type commands like:\n\n2+2\n1:100\n3*3\nsample(1:9)\nX <- matrix(sample(1:9), nrow = 3, ncol = 3)\nX\nsum(X)\nmean(X)\n?sum\nsum\nnucleotides <- c(\"a\", \"c\", \"g\", \"t\")\nnucleotides\nsample(nucleotides, size = 100, replace = TRUE)\ntable(sample(nucleotides, size = 100, replace = TRUE))\npaste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\")\nreplicate(n = 10, expr = paste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\"))\ndf <- data.frame(id = paste0(\"seq\", 1:10), seq = replicate(n = 10, expr = paste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\")))\ndf\nstr(df)\nls()\n\nTake some time and play around with these commands and other things you can come up with. Use the ?function to get help on what that function does. Be sure to discuss what you observe in the console. Do not worry too much on the details for now, we are just getting started. But as you hopefully can see, R is very flexible and basically the message is: ‚ÄúIf you can think it, you can build it in R‚Äù.\n\nGo to Chapter 3 in R4DS2e and do the exercises\n\n\n\nThe Terminal\nNotice how in the console pane, you also get a Terminal, click and enter:\n\nls\nmkdir tmp\ntouch tmp/test.txt\nls tmp\nrm tmp/test.txt\nrmdir tmp\nls\necho $SHELL\n\nBasically, here you have access to a full terminal, which can prove immensely useful! Note, you may or may not be familiar with the concept of a terminal. Simply think of it as a way to interact with the computer using text command, rather than clicking on icons etc. Click back to the console.\n\n\nThe Source\nThe source is where you will write scripts. A script is a series of commands to be executed sequentially, i.e.¬†first line 1, then line 2 and so on. Right now, you should have a open script called Untitled1. If not, you can create a new script by clicking white paper with a round green plus sign in the upper left corner.\nTaking inspiration from the examples above, try to write a series of commands and include a print()-statement at the very end. Click File \\(\\rightarrow\\) Save and save the file as e.g.¬†my_first_script.R. Now, go to the console and type in the command source(\"my_first_script.R\"). Congratulations! You have now written your very first reproducible R-program!\n\n\n\nThe Whole Shebang\nEnough playing around, let us embark on our modern Bio Data Science in R journey.\n\nIn the Files pane, click New Folder and create a folder called projects\nIn the upper right corner, click where it says Project: (None) and then click New Project...\nClick New Directory and then New Project\nIn the Directory name:, enter e.g.¬†r_for_bio_data_science\nClick the Browse... button and select your newly created projects directory and then click Choose\nClick Create Project and wait a bit for it to get created\n\n\nOn Working in Projects\nProjects allow you to create fully transferable bio data science projects, meaning that the root of the project will be where the .Rproj file is located. You can confirm this by entering getwd() in the console. This means that under no circumstance should ever not work in a project nor should ever use absolute paths. Every single path you state in your project must be relative to the project root.\nBut why? Imagine you have create a project, where you have indeed used absolute paths. Now you want to share that project with a colleague. Said colleague gets your project and tests the reproducibility by running the project end-to-end. But it completely fails because you have hardcoded your paths to be absolute, meaning that all files and project ressources locations points to locations on your laptop.\nProjects are a must and allows you to create reproducible encapsulated bio data science projects. Note, the concept of reproducibility is absolute central to this course and must be considered in all aspect of the life cycle of a project!\n\n\nQuarto\nWhile .R-scripts are a perfectly, there is another Skywalker:\n\nIn the upper left corner, again, click the white paper with the round green plus, but this time select Quarto Document\nEnter a Title:, e.g.¬†‚ÄúLab 1 Exercises‚Äù and enter your name below in the box Author:\nClick Create\nImportant: Save your Quarto document! Click File \\(\\rightarrow\\) Save and name it e.g.¬†lab_01_exercises.qmd\nMinimise the Environment-pane\n\nYou should now see something like this:\n\n\n\n\n\nTry clicking the Render button just above the quarto-document. This will create the HTML5 output file. Note! You may get a Connection Refused message. If so, don‚Äôt worry, just close the page to return to the cloud server and find the generated .html file, left-click and select View in Web Browser.\nIf you have previously worked with Rmarkdown, then many features of Quarto will be familiar. Think of Quarto as the complete rethink of Rmarkdown, i.e.¬†based on all the experience gained, what would the optimal way of constructing an open-source scientific and technical publishing system?\nPerhaps you have previously encountered Jupyter notebooks, Quarto is similar. The basic idea is to have one document covering the entire project cycle.\nProceed R for Data Science (2e), chapter 29 and do the exercises.\n\n\n\nBio Data Science with a Virtual AI Assistant\nI am pretty sure you all know of chatGPT by now, so let us address the elephant in the room!\n\nGetting started\n\nGo to the ChatGPT site\nCreate a user and login\n\n\nLet us get acquainted\nNow, at the bottom it says ‚ÄúSend a message‚Äù, let us ask 3 simple question and see if we can find out what is what. Type the following questions in the prompt and read the answers:\n\nExplain in simple terms what you are\nExplain in simple terms how you work\nExplain in simple terms how you can be used to generate value as a virtual AI assistant, when doing Bio Data Science for R\n\n\n\nMoving onto R\nIn the upper left corner, it says + New chat, click it to start a new session.\nAgain, type the following questions in the prompt and read the answers:\n\nR\nWhat is R?\nGive a few simple examples\n\nIf you do get some code examples, try to copy/paste into the console in RStudio and see if they run\n\n\nPrompt Engineering\nStart a new chat and enter:\n\nGive a few simple examples\n\nCompare the response with the one from before, is it the same or different and why so?\nDiscuss in your group what is ‚ÄúPrompt Engineering‚Äù and how does it relate to the above few tests you did? (Bonus info: Prompt engineer is already a job and people are making money off of selling prompts)\n\n\n‚Ä¶a bit more\nStart a new chat and enter:\n\nTell me about DNA\n\nCheck if it gives you correct information?\nNow, write:\n\nGive a few fun examples on how to get started with the R programming language using DNA\n\nCopy/paste the code into the Console, do the examples all run?\n\nEarlier you were told to play around with some code snippets. Perhaps you didn‚Äôt fully catch what was going on? If so, try to type in e.g.:\n\nI'm new to R, please explain in simple terms, what the following code does:\n\n\"\nnucleotides <- c(\"a\", \"c\", \"g\", \"t\")\nreplicate(n = 10, expr = paste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\"))\n\"\n\n\nSummary\nWhile chatGPT can be a powerful tool for code productivity, it comes with a major caveat: When it fails, it fails with confidence. This means that it will be equally confident whether it is right or wrong! The optimal yield is when you are working on a problem with a tool, where you already have a good knowledge of the problem and the tool. Here, you can see if you are heading in the right direction or if you‚Äôre being send on a wild goose chase.\nTherefore, in the beginning of the course, for your own sake, please refrain from using chatGPT!\nInstead solve the exercises based on the materials you have prepared, talk to you group members and discuss the challenges and check your understanding. Then later on, when you have gained initial experience, we can explore using chatGPT to augment your bio data science workflow to enhance productivity"
  },
  {
    "objectID": "lab02.html",
    "href": "lab02.html",
    "title": "Lab 2: Data Visualisation I",
    "section": "",
    "text": "ggplot2"
  },
  {
    "objectID": "lab02.html#schedule",
    "href": "lab02.html#schedule",
    "title": "Lab 2: Data Visualisation I",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Pre-course Survey Walk-through\n08.15 - 08.30: Recap: RStudio Cloud, RStudio and R - The Very Basics (Live session)\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab02.html#learning-materials",
    "href": "lab02.html#learning-materials",
    "title": "Lab 2: Data Visualisation I",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nBook: R4DS2e Chapter 2 Data Visualisation\nPaper: ‚ÄúA Layered Grammar of Graphics‚Äù by Hadley Wickham\nVideo: The best stats you‚Äôve ever seen\nVideo: The SDGs aren‚Äôt the same old same old\nVideo: EMBL Keynote Lecture - ‚ÄúData visualization and data science‚Äù by Hadley Wickham"
  },
  {
    "objectID": "lab02.html#learning-objectives",
    "href": "lab02.html#learning-objectives",
    "title": "Lab 2: Data Visualisation I",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nExplains the basic theory of data visualisation\nDecipher the components of a simple ggplot\nUse ggplot to do basic data visualisation"
  },
  {
    "objectID": "lab02.html#sec-exercises",
    "href": "lab02.html#sec-exercises",
    "title": "Lab 2: Data Visualisation I",
    "section": "Exercises",
    "text": "Exercises"
  },
  {
    "objectID": "lab02.html#prelude",
    "href": "lab02.html#prelude",
    "title": "Lab 2: Data Visualisation I",
    "section": "Prelude",
    "text": "Prelude\nDiscuss these 4 visualisations with your group members\n\nWhat is problematic?\nWhat could be done to rectify?\n\n\n\n\nClick here for visualisation 1\n\n\nNote, AMR = Antimicrobial Resistance\n\n\n\n\n\nClick here for visualisation 2\n\n\n\n\n\n\n\nClick here for visualisation 3\n\n\n\n\n\n\n\nClick here for visualisation 4"
  },
  {
    "objectID": "lab02.html#getting-started",
    "href": "lab02.html#getting-started",
    "title": "Lab 2: Data Visualisation I",
    "section": "Getting Started",
    "text": "Getting Started\nFirst of all, make sure to read every line in these exercises carefully!\nIf you get stuck with Quarto, revisit R4DS2e, chapter 29 or take a look at the Comprehensive guide to using Quarto\n\nGo to the R for Bio Data Science RStudio Cloud Server session from last time and login and choose the project you created.\nCreate a new Quarto Document for todays exercises, e.g.¬†lab02_exercises.qmd\n\nRecall the layout of the IDE (Integrated Development Environment)\n\n\n\n\n\nThen, before we start, we need to fetch some data to work on.\n\nSee if you can figure out how to create a new folder called ‚Äúdata‚Äù, make sure to place it the same place as your my_project_name.Rproj file.\n\nThen, without further ado, run each of the following lines separately in your console:\n\ntarget_url <- \"https://github.com/ramhiser/datamicroarray/raw/master/data/gravier.RData\"\noutput_file <- \"data/gravier.RData\"\ncurl::curl_download(url = target_url,\n                    destfile = output_file)\n\n\nUsing the files pane, check the folder you created to see if you managed to retrieve the file.\n\nRecall the syntax for a new code chunk:\n  ```{r}\n  #| echo: true\n  #| eval: true\n  # Here goes the code... Note how this part does not get executed because of the initial hashtag, this is called a code-comment\n  1 + 1\n  my_vector <- c(1, 2, 3)\n  my_mean <- mean(my_vector)\n  print(my_mean)\n  ```\nIMPORTANT! You are mixing code and text in a Quarto Document! Anything within a ‚Äúchunk‚Äù as defined above will be evaluated as code, whereas anything outside the chunks is markdown. You can use shortcuts to insert new code chunks:\n\nMac: CMD + OPTION + i\nWindows: CTRL + ALT + i\n\nNote, this might not work, depending on your browser. In that case you can insert a new code chunk using  or You can change the shortcuts via ‚ÄúTools‚Äù > ‚ÄúModify Keyboard shortcuts‚Ä¶‚Äù > Filter for ‚ÄúInsert Chunk‚Äù and then choose the desired shortcut. E.g. change the shortcut for code chunks to Shift+Cmd+i or similar.\n\nAdd a new code chunk and use the load()-function to load the data you retrieved.\n\n\n\n\nClick here for a hint\n\n\nRemember, you can use ?load to get help on how the function works and remember your project root path is defined by the location of your .Rproj file, i.e.¬†the path. A path is simply where R can find your file, e.g.¬†/home/projects/r_for_bio_data_science/ or similar depending on your particular setup.\n\nNow, in the console, run the ls()-command and confirm, that you did indeed load the gravier data.\n\nRead the information about the gravier-data here\n\nNow, in your Quarto Document, add a new code chunk like so\n\nlibrary(\"tidyverse\")\n\nThis will load our data science toolbox, including ggplot."
  },
  {
    "objectID": "lab02.html#create-data",
    "href": "lab02.html#create-data",
    "title": "Lab 2: Data Visualisation I",
    "section": "Create data",
    "text": "Create data\nBefore we can visualise the data, we need to wrangle it a bit. Nevermind the details here, we will get to that later. Just create a new chunk, copy/paste the below code and run it:\n\nset.seed(676571)\ncancer_data=mutate(as_tibble(pluck(gravier,\"x\")),y=pluck(gravier,\"y\"),pt_id=1:length(pluck(gravier, \"y\")),age=round(rnorm(length(pluck(gravier,\"y\")),mean=55,sd=10),1))\ncancer_data=rename(cancer_data,event_label=y)\ncancer_data$age_group=cut(cancer_data$age,breaks=seq(10,100,by=10))\ncancer_data=relocate(cancer_data,c(pt_id,age,age_group,pt_id,event_label))\n\nNow we have the data set as an tibble, which is an augmented data frame (we will also get to that later):\n\ncancer_data\n\n# A tibble: 168 √ó 2,909\n   pt_id   age age_group event_label    g2E09    g7F07    g1A01   g3C09    g3H08\n   <int> <dbl> <fct>     <fct>          <dbl>    <dbl>    <dbl>   <dbl>    <dbl>\n 1     1  34.2 (30,40]   good        -0.00144 -0.00144 -0.0831  -0.0475  1.58e-2\n 2     2  47   (40,50]   good        -0.0604   0.0129  -0.00144  0.0104  3.16e-2\n 3     3  60.3 (60,70]   good         0.0398   0.0524  -0.0786   0.0635 -3.95e-2\n 4     4  57.8 (50,60]   good         0.0101   0.0314  -0.0218   0.0215  8.68e-2\n 5     5  54.9 (50,60]   good         0.0496   0.0201   0.0370   0.0311  2.07e-2\n 6     6  58.8 (50,60]   good        -0.0664   0.0468   0.00720 -0.370   2.88e-3\n 7     7  52.9 (50,60]   good        -0.00289 -0.0816  -0.0291  -0.0249 -1.74e-2\n 8     8  74.5 (70,80]   good        -0.198   -0.0499  -0.0634  -0.0298  3.00e-2\n 9     9  47.6 (40,50]   good         0.00288  0.0201   0.0272   0.0174 -7.89e-5\n10    10  55.8 (50,60]   good        -0.0574  -0.0574  -0.0831  -0.0897 -1.01e-1\n# ‚Ñπ 158 more rows\n# ‚Ñπ 2,900 more variables: g1A08 <dbl>, g1B01 <dbl>, g1int1 <dbl>, g1E11 <dbl>,\n#   g8G02 <dbl>, g1H04 <dbl>, g1C01 <dbl>, g1F11 <dbl>, g3F05 <dbl>,\n#   g3B09 <dbl>, g1int2 <dbl>, g2C01 <dbl>, g1A05 <dbl>, g1E01 <dbl>,\n#   g1B05 <dbl>, g3C05 <dbl>, g3A07 <dbl>, g1F01 <dbl>, g2D01 <dbl>,\n#   g1int3 <dbl>, g1int4 <dbl>, g1D05 <dbl>, g1E05 <dbl>, g1G05 <dbl>,\n#   g1C05 <dbl>, g1G11 <dbl>, g2D08 <dbl>, g2E06 <dbl>, g3H09 <dbl>, ‚Ä¶\n\n\n\nQ1: What is this data?\n\n\n\n\nClick here for a hint\n\n\nWhere did the data come from?\n\n\nQ2: How many rows and columns are there in the data set in total?\n\n\n\n\nClick here for a hint\n\n\nDo you think you are the first person in the world to try to find out how many rows and columns are in a data set in R?\n\n\nQ3: Which are the variables and which are the observations in relation to rows and columns?"
  },
  {
    "objectID": "lab02.html#ggplot---the-very-basics",
    "href": "lab02.html#ggplot---the-very-basics",
    "title": "Lab 2: Data Visualisation I",
    "section": "ggplot - The Very Basics",
    "text": "ggplot - The Very Basics\n\nGeneral Syntax\nThe general syntax for a basic ggplot is:\n\nggplot(data = my_data,\n       mapping = aes(x = variable_1_name,\n                     y = variable_2_name)) +\n  geom_something() +\n  labs()\n\nNote the + for adding layers to the plot\n\nggplot the plotting function\nmy_data the data you want to plot\naes() the mappings of your data to the plot\nx data for the x-axis\ny data for the y-axis\ngeom_something() the representation of your data\nlabs() the x-/y-labels, title, etc.\n\nNow:\n\nReivisit this illustration and discuss in your group what is what:\n\n\nA very handy ggplot cheat-sheet can be found here\n\n\nBasic Plots\nRemember to write notes in your rmarkdown document. You will likely revisit these basic plots in future exercises.\nPrimer: Plotting 2 x 20 random normally distributed numbers, can be done like so:\n\nggplot(data = tibble(x = rnorm(20),\n                     y = rnorm(20)),\n       mapping = aes(x = x,\n                     y = y)) +\n  geom_point()\n\n\n\n\nUsing this small primer, the materials you read for today and the cancer_data you created, in separate code-chunks, create a:\n\nT1: scatterplot of one variable against another\nT2: linegraph of one variable against another\nT3: boxplot of one variable (Hint: Set x = \"my_gene\" in aes())\nT4: histogram of one variable\nT5: densitogram of one variable\n\nRemember to write notes to yourself, so you know what you did and if there is something in particular you want to remember.\n\nQ4: Do all geoms require both x and y?\n\n\n\nExtending Basic Plots\n\nT6: Pick your favourite gene and create a boxplot of expression levels stratified on the variable event_label\nT7: Like T6, but with densitograms GROUP ASSIGNMENT\nT8: Pick your favourite gene and create a boxplot of expression levels stratified on the variable age_group\n\nThen, add stratification on event_label\nThen, add transparency to the boxes\nThen, add some labels\n\nT9: Pick your favourite gene and create a scatter-plot of expression levels versus age\n\nThen, add stratification on event_label\nThen, add a smoothing line\nThen, add some labels\n\nT10: Pick your favourite two genes and create a scatter-plot of their expression levels\n\nThen, add stratification on event_label\nThen, add a smoothing line\nThen, show split into seperate panes based on the variable age_group\nThen, add some labels\nChange the event_label title of the legend\n\nT11: Recreate the following plot\n\n\n\n\n\n\n\nQ5: Using your biological knowledge, what is your interpretation of the plot?\nT12: Recreate the following plot\n\n\n\n\n\n\n\nQ6: Using your biological knowledge, what is your interpretation of the plot?\nT13: If you arrive here and there is still time left for the exercises, you are probably already familiar with ggplot - Use what time is left to challenge yourself to further explore the cancer_data and create some nice data visualisations - Show me what you come up with!"
  },
  {
    "objectID": "lab02.html#further-ressources-for-data-visualisation",
    "href": "lab02.html#further-ressources-for-data-visualisation",
    "title": "Lab 2: Data Visualisation I",
    "section": "Further ressources for data visualisation",
    "text": "Further ressources for data visualisation\n\nA very handy ggplot cheat-sheet can be found here\nSo which plot to choose? Check this handy guide\nExplore ways of plotting here"
  },
  {
    "objectID": "lab03.html",
    "href": "lab03.html",
    "title": "Lab 3: Data Visualisation II",
    "section": "",
    "text": "ggplot2\npatchwork\nscales\nggridges"
  },
  {
    "objectID": "lab03.html#schedule",
    "href": "lab03.html#schedule",
    "title": "Lab 3: Data Visualisation II",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.45: Recap of Lab 2 and Lecture\n08.45 - 09.00: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab03.html#learning-materials",
    "href": "lab03.html#learning-materials",
    "title": "Lab 3: Data Visualisation II",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: ‚ÄúVisualize‚Äù, chapters 10, 11 and 12\nVideo: William Chase | The Glamour of Graphics | RStudio (2020)\nWeb: Patchwork - Getting started\nWeb: Scales - Getting started"
  },
  {
    "objectID": "lab03.html#learning-objectives",
    "href": "lab03.html#learning-objectives",
    "title": "Lab 3: Data Visualisation II",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUse more advanced ggplot features\nCustomise the data visualisation\nCombine multiple plots into one pane\nLook at a more advanced ggplot and decipher the components used"
  },
  {
    "objectID": "lab03.html#sec-exercises",
    "href": "lab03.html#sec-exercises",
    "title": "Lab 3: Data Visualisation II",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nRead the steps of this exercises carefully, while completing them\n\nIntroduction\nSome authors are kind enough to supply the data they used for their paper, e.g.:\n\n‚ÄúAssessment of the influence of intrinsic environmental and geographical factors on the bacterial ecology of pit latrines‚Äù\n\nWhere the supporting data can be found here:\n\nhttp://userweb.eng.gla.ac.uk/umer.ijaz/bioinformatics/ecological.html\n\n\n\nGetting Started\nAgain, go to the R for Bio Data Science RStudio Cloud Server session from last time and login and choose the project you created\n\nCreate a new Quarto Document for todays exercises, e.g.¬†lab03_exercises.qmd\nNB! The Quarto document MUST be placed together with your .Rproj file (defining, the project root - look in your Files-tab) and also there, the data-folder should be placed!\nREMEMBER paths are important! Also, R is case-sensitive, i.e.¬†‚Äúdata‚Äù is not the same as ‚ÄúData‚Äù\n\nSee Paths and Projects\n\n\nGetting the data\nAdd a new code chunk and add the following code (Never mind the details, we will get back to this), remember you can use headers to nicely section your quarto Document.\n\nbase_url <- \"http://userweb.eng.gla.ac.uk/umer.ijaz/bioinformatics/ecological/\"\n\nSPE <- read_csv(file = str_c(base_url, \"SPE_pitlatrine.csv\"))\nwrite_csv(x = SPE,\n          file = \"data/SPE_pitlatrine.csv\")\n\nENV <- read_csv(file = str_c(base_url, \"ENV_pitlatrine.csv\"))\nwrite_csv(x = ENV,\n          file = \"data/ENV_pitlatrine.csv\")\n\nAdd the chunk settings #| echo: true and #| eval: true, then run the block and change the latter to #| eval: false.\n\nDiscuss in your group, what this means and why we do it\n\n\n\n\nClick here for hint\n\n\nFrom where do we retrieve the data and to where do we write it and what happens if we run the chunk more than one time?\n\n\n\nWrangling the data\n\nWhat is data wrangling?\n\nBefore we continue with plotting, we want to unify the data, so here again you will run some code, where the details are not important right now.\nBut‚Ä¶ Make sure, that you have run library(\"tidyverse\") somewhere in your Quarto document - Perhaps under an initial header saying ‚ÄúLoad Libraries‚Äù or similar?\n\nSPE |> \n  pivot_longer(cols = -Taxa,\n               names_to = \"Samples\",\n               values_to = \"OTU_Count\")  |> \n  full_join(ENV, by = \"Samples\") |> \n  mutate(site = case_when(str_detect(Samples, \"^T\") ~ \"Tanzania\",\n                          str_detect(Samples, \"^V\") ~ \"Vietnam\")) |>  \n  write_tsv(file = \"data/SPE_ENV.tsv\")\n\nChange the chunk settings as before"
  },
  {
    "objectID": "lab03.html#data-visualisation-ii",
    "href": "lab03.html#data-visualisation-ii",
    "title": "Lab 3: Data Visualisation II",
    "section": "Data Visualisation II",
    "text": "Data Visualisation II\n\nRead the data\n\nSPE_ENV <- read_tsv(file = \"data/SPE_ENV.tsv\")\nSPE_ENV\n\n# A tibble: 4,212 √ó 15\n   Taxa   Samples OTU_Count    pH  Temp    TS    VS   VFA  CODt  CODs perCODsbyt\n   <chr>  <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>      <dbl>\n 1 Acido‚Ä¶ T_2_1           0  7.82  25.1  14.5 71.3   71     874   311         36\n 2 Acido‚Ä¶ T_2_10          0  9.08  24.2  37.8 31.5    2     102     9          9\n 3 Acido‚Ä¶ T_2_12          0  8.84  25.1  71.1  5.94   1      35     4         10\n 4 Acido‚Ä¶ T_2_2           0  6.49  29.6  13.9 64.9    3.7   389   180         46\n 5 Acido‚Ä¶ T_2_3           0  6.46  27.9  29.4 26.8   27.5   161    35         22\n 6 Acido‚Ä¶ T_2_6           0  7.69  28.7  65.5  7.03   1.5    57     3          6\n 7 Acido‚Ä¶ T_2_7           0  7.48  29.8  36.0 34.1    1.1   107     9          8\n 8 Acido‚Ä¶ T_2_9           0  7.6   25    46.9 19.6    1.1    62     8         13\n 9 Acido‚Ä¶ T_3_2           0  7.55  28.8  12.6 51.8   30.9   384    57         15\n10 Acido‚Ä¶ T_3_3           0  7.68  28.9  14.6 48.1   24.2   372    57         15\n# ‚Ñπ 4,202 more rows\n# ‚Ñπ 4 more variables: NH4 <dbl>, Prot <dbl>, Carbo <dbl>, site <chr>\n\n\n\n\nIMPORTANT INSTRUCTIONS - READ!\nFor these exercises, you will have to identify what you see in the plot!\nFor each plot, complete the following steps\n\nLook at this overview of the components of a ggplot (see below)\nLook at the plot you are to recreate and discuss in the group:\n\nWhat is the data? Take a look at it and understand what is in the data\nWhat are the mappings? I.e. what variables are on the x-/y-axis?\nAre there any colour-/fill-mappings?\nWhat are the geoms used?\nAre there any modifications to theme?\n\n\n\n\n\nClick here for hint\n\n\n\nConsult the Data visualization with ggplot2 cheatsheet\nCheck which options you have available\nConsult the chapters in the book you read, see preparation materials for labs 2 and 3\n\n\n\n\n\nTASKS\n\nTask 1 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 2 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 3 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 4 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 5 - Recreate the following plots\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nSame data, but a transformation happened, changing the representation of the data. Look carefully at the axes.\n\n\n\nTask 6 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nSee if you can find something online on geom_smooth()\n\n\n\nTask 7 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nThink about fill and then see if you can find something online on geom_tile(), scale_fill_gradient2 and how to ggplot rotate axis labels\n\n\n\nTask 8 - Recreate the following plot\nStart by running this code in a new chunk (ignore details for now)\n\ntargets <- c(\"Methanobacteria\", \"Clostridia\", \"Actinobacteria\",\n            \"Sphingobacteria\", \"Anaerolineae\")\nSPE_ENV_targets <- SPE_ENV |>\n  filter(Taxa %in% targets)\n\nand then use the created dataset SPE_ENV_targets to recreate this plot:\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nHere we need to use geom_density_ridges(), but which package contains this? Also we are using a colour scale called viridis, but how do we add this? Also, perhaps there are more themes we can use than just theme_classic()?\n\n\n\nTask 9 - GROUP ASSIGNMENT\nFor this assignment you and your group are to apply what you have learned in the two data visualisation labs. The task is to create a really nice plot using one of two datasets, the cancer_data or the SPE_ENV\nTry to play around with some custom colouring. There is a nice tool to aid in choosing colours for visualisations here\nBe sure to read the assignment instructions before submitting your solution."
  },
  {
    "objectID": "lab04.html",
    "href": "lab04.html",
    "title": "Lab 4: Data Wrangling I",
    "section": "",
    "text": "dplyr\nreadr\ntibble"
  },
  {
    "objectID": "lab04.html#schedule",
    "href": "lab04.html#schedule",
    "title": "Lab 4: Data Wrangling I",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap of Lab 3\n08.15 - 08.30: Assignment 2 walk-through\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab04.html#learning-materials",
    "href": "lab04.html#learning-materials",
    "title": "Lab 4: Data Wrangling I",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nR4DS2e book: Chapter 4, chapter 5, chapter 8, chapter 9\nWeb: What is data wrangling? Intro, Motivation, Outline, Setup ‚Äì Pt. 1 Data Wrangling Introduction\nWeb: (NB! STOP at 7:45, i.e.¬†skip tidyr) Tidy Data and tidyr ‚Äì Pt 2 Intro to Data Wrangling with R and the Tidyverse\nWeb: Data Manipulation Tools: dplyr ‚Äì Pt 3 Intro to the Grammar of Data Manipulation with R"
  },
  {
    "objectID": "lab04.html#learning-objectives",
    "href": "lab04.html#learning-objectives",
    "title": "Lab 4: Data Wrangling I",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUnderstand and apply the 6 basic dplyr verbs: filter(), arrange(), select(), mutate(), summarise() and group_by()\nConstruct and apply logical statements in context with dplyr pipelines\nUnderstand and apply the additional verbs count(), drop_na(), View()\nCombine dplyr verbs to form a data manipulation pipeline using the pipe |> operator\nDecipher the components and functions hereof in a dplyr pipeline"
  },
  {
    "objectID": "lab04.html#sec-exercises",
    "href": "lab04.html#sec-exercises",
    "title": "Lab 4: Data Wrangling I",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nImportant: As we progress, avoid using black-box do-everything-with-one-command R-packages like e.g.¬†ggpubr - I want you to learn the underlying technical bio data science! ‚Ä¶and why is that? Because if you use these type of packages, you will be limited to their functionality, whereas if you truly understand ggplot - The sky is the limit! Basically, it the clich√© that ‚ÄúIf you give a man a fish, you feed him for a day. If you teach a man to fish, you feed him for a lifetime‚Äù\n\nGetting Started\nUse this link to go to the R for Bio Data Science RStudio Cloud Server\nFirst things first:\n\nCreate a new Quarto Document for todays exercises, e.g.¬†lab04_exercises.qmd\n\nNote that: - The Quarto document MUST be placed together with your .Rproj file (defining, the project root - look in your Files-pane) and also there, the data-folder should be placed!\nUnderstanding how paths and projects work is important. In case this is no entirely clear, a new chapter has been added on Paths and Projects.\n\n\nBrief refresh of Quarto so far\nRecall the syntax for a new code chunk, where all your R code goes any text and notes must be outside the chunk tags:\n\n```{r}\n# Here the code goes\n1 + 1\nx <- c(1, 2, 3)\nmean(x)\n```\n\n[1] 2\n[1] 2\n\n\nOutside the code-chunks, go our markdown, e.g.:\n  # Header level 1\n  ## Header level 2\n  ### Header level 3\n  *A text in italics*\n  **A text in bold**\n  Normal text describing and explaining\nNow, in your new Quarto document, create a new Header level 2, e.g.:\n  ## Load Libraries\nand under your new header, add a new code chunk, like so\n\n```{r}\n#| message: false\nlibrary(\"tidyverse\")\n```\n\nAnd run the chunk. This will load our data science toolbox, including dplyr (and ggplot). But wait, what does the #| message: false-part do? ü§∑Ô∏è ü§î\nTry to structure your Rmarkdown document as your course notes, i.e.¬†add notes while solving the exercises aiming at creating a revisitable reference for your final project work\nBonus info: We are engineers, so of course, we love equations, we can include standard \\(\\LaTeX\\) syntax, e.g.:\n  $E(x) = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} x_{i}$\nTry it!\n\n\nA few handy short cuts\nInsert new code chunk:\n\nMac: CTRL + OPTION + i\nWindows: CTRL + OPTION + i\n\nRender my Quarto document\n\nMac: CMD + SHIFT + k\nWin: CTRL + SHIFT + k\n\nRun line in chunk\n\nMac: CMD + ENTER\nWin: CTRL + ENTER\n\nRun entire chunk\n\nMac: CMD + SHIFT + ENTER\nWin: CTRL + SHIFT + ENTER\n\nInsert the pipe symbol |>\n\nMac: CMD + SHIFT + m\nWin: CTRL + SHIFT + m\n\nNote, if you‚Äôre trying this out and you see %>% instead of |>, then go to Tools, Global Options..., Code and check the box Use native pipeoperator"
  },
  {
    "objectID": "lab04.html#a-few-initial-questions",
    "href": "lab04.html#a-few-initial-questions",
    "title": "Lab 4: Data Wrangling I",
    "section": "A few initial questions",
    "text": "A few initial questions\nFirst, if you don‚Äôt feel completely comfortable with the group_by |> summarise-workflow, then no worries - Feel free to visit this short R Tutorial: Grouping and summarizing\nThen, in your groups, discuss the following primer questions. Note, when asked for ‚Äúwhat is the output‚Äù, do not run the code in the console, instead try to talk and think about it and write your answers and notes in your Quarto document for the day:\nFirst, in a new chunk, run tibble(x = c(4, 3, 5, 1, 2)), so you understand what it does, then - Discuss in your group, what is the output of, remember first talk, then check understanding by running code:\n\nQ1: tibble(x = c(4, 3, 5, 1, 2)) |> filter(x > 2)?\nQ2: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(x)?\nQ3: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(desc(x))?\nQ4: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(desc(desc(x)))?\nQ5: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(x)?\nQ6: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(y)?\nQ7: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(-x)?\nQ8: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(-x, -y)?\nQ9: tibble(x = c(4, 3, 5, 1, 2)) |> mutate(x_dbl = 2*x)?\nQ10: tibble(x = c(4, 3, 5, 1, 2)) |> mutate(x_dbl = 2 * x, x_qdr = 2*x_dbl)?\nQ11: tibble(x = c(4, 3, 5, 1, 2)) |> summarise(x_mu = mean(x))?\nQ12: tibble(x = c(4, 3, 5, 1, 2)) |> summarise(x_max = max(x))?\nQ13: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, NA, 5, 1, 2)) |> group_by(lbl) |> summarise(x_mu = mean(x), x_max = max(x))?\nQ14: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, 3, 5, 1, 2)) |> group_by(lbl) |> summarise(n = n())?\nQ15: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, 3, 5, 1, 2)) |> count(lbl)?\n\nIn the following, return to these questions and your answers for reference on the dplyr verbs!"
  },
  {
    "objectID": "lab04.html#load-data",
    "href": "lab04.html#load-data",
    "title": "Lab 4: Data Wrangling I",
    "section": "Load data",
    "text": "Load data\nAgain, add a new header to your Quarto document, e.g.¬†## Load Data, then:\n\nGo to the Vanderbilt Biostatistics Datasets site\nFind Diabetes data and download the diabetes.csv file\nYou should have a data-folder, if not, then in the Files pane, click the New Folder button, enter folder name data and click ok\nNow, click on the folder you created\nClick the  Upload-button and navigate to the diabetes.csv file you downloaded\nClicking the two dots .. above the file you uploaded, look for  .., will take you one level up in your project path\nInsert a new code chunk in your Quarto document\nAdd and then run the following code\n\n\ndiabetes_data <- read_csv(file = \"data/diabetes.csv\")\ndiabetes_data\n\nThen realise that we could simply have run the following code to do the exact same thing (Yes, readr is pretty nifty):\n\n# Create the data directory programmatically\ndir_create(x = \"data\")\n\n# Retrieve the data directly\ndiabetes_data <- read_csv(file = \"https://hbiostat.org/data/repo/diabetes.csv\")\n\n# Write the data to disk\nwrite_csv(x = diabetes_data,\n          file = \"data/diabetes.csv\")\n\nJust remember the echo/eval trick from last session to avoid retrieving online data each time you render your Quarto document"
  },
  {
    "objectID": "lab04.html#work-with-the-diabetes-data-set",
    "href": "lab04.html#work-with-the-diabetes-data-set",
    "title": "Lab 4: Data Wrangling I",
    "section": "Work with the diabetes data set",
    "text": "Work with the diabetes data set\nUse the pipe |> to use the View()-function to inspect the data set. Note, if you click the -button, you will get a spreadsheet-like view of the data, allowing you to get an overview.\n\nQ1: How many observations and how many variables?\nQ2: Is this a tidy data set? Which three rules must be satisfied?\nQ3: When you run the chunk, then underneath each column name is stated <chr> and <dbl> what is that?\n\nBefore we continue\n\nT1: Change the height, weight, waist and hip from inches/pounds to the metric system (cm/kg), rounding to 1 decimal\n\n\n\n\nLet us try to take a closer look at the data by various subsetting (How many‚Ä¶ is equal to the number of rows in the subset of the data you created):\n\nQ4: How many weigh less than 100kg?\nQ5: How many weigh more than 100kg?\nQ6: How many weigh more than 100kg and are less than 1.6m tall?\nQ7: How many women are taller than 1.8m?\nQ8: How many men are taller than 1.8m?\nQ9: How many women in Louisa are older than 30?\nQ10: How many men in Buckingham are younger than 30 and taller than 1.9m?\nT2: Make a scatter plot of weight versus height and colour by sex for inhabitants of Louisa above the age of 40\nT3: Make a boxplot of height versus location stratified on sex for people above the age of 50\n\nSorting columns can aid in getting an overview of variable ranges (don‚Äôt use the summary function yet for this one)\n\nQ11: How old is the youngest person?\nQ12: How old is the oldest person?\nQ13: Of all the 20-year olds, what is the height of the tallest?\nQ14: Of all the 20-year olds, what is the height of the shortest?\n\nChoosing specific columns can be used to work with a subset of the data for a specific purpose\n\nQ15: How many columns (variables) starts_with a ‚Äúb‚Äù?\nQ16: How many columns (variables) contains the word ‚Äúeight‚Äù?\n\nCreating new variables is an integral part of data manipulation\n\nT4: Create a new variable, where you calculate the BMI\n\n\n\n\n\nT5: Create a BMI_class variable\n\nTake a look at the following code snippet to get you started:\n\ntibble(x = rnorm(10)) |> \n  mutate(trichotomised = case_when(\n    x < -1 ~ \"Less than -1\",\n    -1 <= x & x < 1 ~ \"larger than or equal to -1 and smaller than 1\",\n    1 <= x ~ \"Larger than or equal to 1\"))\n\nand then go read about BMI classification here and discuss in your group how to extract classifications from the Definition/Introduction section\nNote, the cut()-function could be used here, but you should try to use case_when() as illustrated in the example chunk above.\n\n\n\nOnce you have created the variable, you will need to convert it to a categorical variable, in R, these are called a factor and you can set the levels like so:\n\ndiabetes_data <- diabetes_data |>\n  mutate(BMI_class = factor(BMI_class,\n                            levels =  c(\"my 1st category\", \"my 2nd category\",\n                                        \"my 3rd category\", \"my nth category\")))\n\nThis is very important for plotting, as this will determine the order in which the categories appear on the plot!\n\nT6: Create a boxplot of hdl versus BMI_class\nQ17: What do you see?\nT7: Create a BFP (Body fat percentage) variable\n\n\n\n\nClick here for hint\n\n\nBFP can be calculated usin the below equation source:\n\\[BFP = 1.39 \\cdot BMI + 0.16 \\cdot age - 10.34 \\cdot sex - 9\\]\nWhere \\(sex\\) is defined as being \\(0\\) for female and \\(1\\) for male.\n\n\n\n\n\nT8: Create a WHR (waist-to-hip ratio) variable\nQ18: Which correlate better with BMI, WHR or BFP? GROUP ASSIGNMENT\n\n\n\n\nClick here for hint\n\n\nIs there a certain plot-type, which can visualise if the relationship between two variables and give insights to if they are correlated? Can you perhaps use an R-function to compute the ‚Äúcorrelation coefficient‚Äù?. Do not use e.g.¬†ggpubr, use only tidyverse and base)\n\nNow, with this augmented data set, let us create some summary statistics\n\nQ19: How many women and men are there in the data set?\nQ20: How many women and men are there from Buckingham and Louisa respectively in the data set?\nQ21: How many are in each of the BMI_class groups?\nQ22: Given the code below, explain the difference between A and B?\n\n\n# A\ndiabetes_data |>\n  ggplot(aes(x = BMI_class)) +\n  geom_bar()\n\n# B\ndiabetes_data |>\n  count(BMI_class) |>\n  ggplot(aes(x = BMI_class, y = n)) +\n  geom_col()\n\n\nT9: For each BMI_class group, calculate the average weight and associated standard deviation\nQ23: What was the average age of the women living in Buckingham in the study?\n\nFinally, if you reach this point and there is still time left. Take some time to do some exploratory plots of the data set and see if you can find something interesting."
  },
  {
    "objectID": "lab05.html",
    "href": "lab05.html",
    "title": "Lab 5: Data Wrangling II",
    "section": "",
    "text": "dplyr\nstringr\ntidyr\nforcats\npatchwork\nggseqlogo\ntable1"
  },
  {
    "objectID": "lab05.html#schedule",
    "href": "lab05.html#schedule",
    "title": "Lab 5: Data Wrangling II",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.30: Recap of Lab 4\n08.30 - 08.35: Lecture\n08.35 - 08.45: Break\n08.45 - 12.00: Exercises"
  },
  {
    "objectID": "lab05.html#learning-materials",
    "href": "lab05.html#learning-materials",
    "title": "Lab 5: Data Wrangling II",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nR4DS2e book: Chapter 6: Data Tidying, Chapter 15: Strings, Chapter 17: Factors, Chapter 20: Joins\nVideo: Tidy Data and tidyr - NB! Start at 7:45 and please note: gather() is now pivot_longer() and spread() is now pivot_wider()\nVideo: Working with Two Datasets: Binds, Set Operations, and Joins\nVideo: stringr (Playlist with 7 short videos)"
  },
  {
    "objectID": "lab05.html#learning-objectives",
    "href": "lab05.html#learning-objectives",
    "title": "Lab 5: Data Wrangling II",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUnderstand and apply the various str_*()-functions for string manipulation\nUnderstand and apply the family of *_join()-functions for combining data sets\nUnderstand and apply pivot_wider() and pivot_longer()\nUse factors in context with plotting categorical data using ggplot"
  },
  {
    "objectID": "lab05.html#sec-exercises",
    "href": "lab05.html#sec-exercises",
    "title": "Lab 5: Data Wrangling II",
    "section": "Exercises",
    "text": "Exercises\n\nPrologue\nToday will not be easy! But please try to remember Hadley‚Äôs word-of-advise:\n\n‚ÄúThe bad news is, whenever you‚Äôre learning a new tool, for a long time, you‚Äôre going to suck! It‚Äôs gonna be very frustrating! But the good news is that that is typical and something that happens to everyone and it‚Äôs only temporary! Unfortunately, there is no way to going from knowing nothing about the subject to knowing something about a subject and being an expert in it without going through a period of great frustration and much suckiness! Keep pushing through!‚Äù - H. Wickham (dplyr tutorial at useR 2014, 4:10 - 4:48)"
  },
  {
    "objectID": "lab05.html#intro",
    "href": "lab05.html#intro",
    "title": "Lab 5: Data Wrangling II",
    "section": "Intro",
    "text": "Intro\nWe are upping the game here, so expect to get stuck at some of the questions. Remember - Discuss with your group how to solve the task, revisit the materials you prepared for today and naturally, the TAs and I are happy to nudge you in the right direction. Finally, remember‚Ä¶ Have fun!\nRemember what you have worked on so far:\n\nRStudio\nQuarto\nggplot\nfilter\narrange\nselect\nmutate\ngroup_by\nsummarise\nThe pipe and creating pipelines\nstringr\njoining data\npivotting data\n\nThat‚Äôs quite a lot! Well done - You‚Äôve come quite far already! Remember to think about the above tools in the following as we will synthesise your learnings so far into an analysis!"
  },
  {
    "objectID": "lab05.html#sec-background",
    "href": "lab05.html#sec-background",
    "title": "Lab 5: Data Wrangling II",
    "section": "Background",
    "text": "Background\nIn the early 20s, the world was hit by the coronavirus disease 2019 (COVID-19) pandemic. The pandemic was caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). In Denmark the virus first confirmed case was on 27 February 2020.\nWhile initially very little was known about the SARS-CoV-2 virus, we did know the general pathology of vira. Briefly, the virus invades the cells and hijacks the intra-cellular machinery. Using the hijacked machinery, components for new virus particles are produced, eventually being packed into the viral envelope and released from the infected cell. Some of these components, viral proteins, is broken down into smaller fragments called peptides by the proteasome. These peptides are transported into the endoplasmatic reticulum by the Transporter Associated with antigen Processing (TAP) protein complex. Here, they are aided by chaperones bound to the Major Histocompatilibty Complex class I (MHCI) and then across the Golgi Aparatus they finally get displayed on the surface of the cells. Note, in humans, MHC is also called Human Leukocyte Antigen (HLA) and represents the most diverse genes. Each of us have a total of 6 HLA-alleles, 3 from the maternal and 3 from the paternal side. These are further divided into 3 classes HLA-A, HLA-B and HLA-C and the combination of these constitute the HLA-haplotype for an individual. Once the peptide is bound to the MHC Class I at the cell surface and exposed, the MHCI-peptide complex can be recognised by CD8+ Cytotoxic T-Lymphocytes (CTLs) via the T-cell Receptor (TCR). If a cell displays peptides of viral origin, the CTL gets activated and via a cascade induces apoptosis (programmed cell death) of the infected cell. The proces is summarised in the figure below.\n\n\n\n\n\nImage source: 10.3389/fmicb.2015.00021\nThe data we will be working with today contains data on sequenced T-cell receptors, viral antigens, HLA-haplotypes and clinical meta data for a cohort:\n\nA large-scale database of T-cell receptor beta (TCR\\(\\beta\\)) sequences and binding associations from natural and synthetic exposure to SARS-CoV-2"
  },
  {
    "objectID": "lab05.html#your-task-today",
    "href": "lab05.html#your-task-today",
    "title": "Lab 5: Data Wrangling II",
    "section": "Your Task Today",
    "text": "Your Task Today\nToday, we will emulate the situation, where you are working as a Bioinformatician / Bio Data Scientist and you have been given the data and the task of answering these two burning questions:\n\nWhat characterises the peptides binding to the HLAs?\nWhat characterises T-cell Receptors binding to the pMHC-complexes?\n\nGROUP ASSIGNMENT: Today, your assignment will be to create a micro-report on these 2 questions!\nMAKE SURE TO READ THE LAST SECTION ON THE ASSIGNMENT"
  },
  {
    "objectID": "lab05.html#getting-started",
    "href": "lab05.html#getting-started",
    "title": "Lab 5: Data Wrangling II",
    "section": "Getting Started",
    "text": "Getting Started\n\nClick here to go to the course RStudio cloud server and login\nMake sure you are in your r_for_bio_data_science-project, you can verify this in the upper right corner\nIn the same place as your r_for_bio_data_science.Rproj-file and existing data-folder, create a new folder and name it doc\nGo to the aforementioned manuscript. Download the PDF and upload it to your new doc-folder\nOpen the PDF and find the link to the data\nGo to the data site (Note, you may have to create and account to download, shouldn‚Äôt take too long) . Find and download the file ImmuneCODE-MIRA-Release002.1.zip (CAREFUL, do not download the superseded files)\nUnpack the downloaded file\nFind the files peptide-detail-ci.csv and subject-metadata.csv and compress to .zip-files\nUpload the compressed peptide-detail-ci.csv.zip- and subject-metadata.csv.zip-files to your data-folder in your RStudio Cloud session\nFinally, once again, create a new Quarto document for today‚Äôs exercises, containing the sections:\n\nBackground\nAim\nLoad Libraries\nLoad Data\nData Description\nAnalysis"
  },
  {
    "objectID": "lab05.html#creating-the-micro-report",
    "href": "lab05.html#creating-the-micro-report",
    "title": "Lab 5: Data Wrangling II",
    "section": "Creating the Micro-Report",
    "text": "Creating the Micro-Report\n\nBackground\nFeel free to copy paste the one stated in the background-section above\n\n\nAim\nState the aim of the micro-report, i.e.¬†what are the questions you are addressing?\n\n\nLoad Libraries\n\n\n\nLoad the libraries needed\n\n\nLoad Data\nRead the two data sets into variables peptide_data and meta_data.\n\n\n\nClick here for hint\n\n\nThink about which Tidyverse package deals with reading data and what are the file types we want to read here?\n\n\n\n\n\n\nData Description\nIt is customary to include a description of the data, helping the reader if the report, i.e.¬†your stakeholder, to get an easy overview\n\nThe Subject Meta Data\nLet‚Äôs take a look at the meta data:\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 30\n   Experiment Subject `Cell Type` `Target Type` Cohort          Age Gender Race \n   <chr>        <dbl> <chr>       <chr>         <chr>         <dbl> <chr>  <chr>\n 1 eXL32         4423 naive_CD8   C19_cI        Healthy (No ‚Ä¶    37 F      White\n 2 eLH43         3565 PBMC        C19_cI        COVID-19-Con‚Ä¶    57 M      <NA> \n 3 eJL154          83 PBMC        C19_cI        COVID-19-Exp‚Ä¶    35 F      Nati‚Ä¶\n 4 eOX49        10943 naive_CD8   C19_cI        Healthy (No ‚Ä¶    21 M      White\n 5 eLH42         1588 PBMC        C19_cI        COVID-19-Con‚Ä¶    63 M      <NA> \n 6 eLH54         1326 PBMC        C19_cI        COVID-19-Con‚Ä¶    NA <NA>   <NA> \n 7 eJL152        7842 PBMC        C19_cI        COVID-19-Con‚Ä¶    41 F      <NA> \n 8 ePD76        11011 naive_CD8   C19_cI        Healthy (No ‚Ä¶    33 M      White\n 9 eMR13         2059 PBMC        C19_cI        COVID-19-Con‚Ä¶    NA <NA>   <NA> \n10 ePD86           92 PBMC        C19_cI        COVID-19-Con‚Ä¶    58 M      White\n# ‚Ñπ 22 more variables: `HLA-A...9` <chr>, `HLA-A...10` <chr>,\n#   `HLA-B...11` <chr>, `HLA-B...12` <chr>, `HLA-C...13` <chr>,\n#   `HLA-C...14` <chr>, DPA1...15 <chr>, DPA1...16 <chr>, DPB1...17 <chr>,\n#   DPB1...18 <chr>, DQA1...19 <chr>, DQA1...20 <chr>, DQB1...21 <chr>,\n#   DQB1...22 <chr>, DRB1...23 <chr>, DRB1...24 <chr>, DRB3...25 <chr>,\n#   DRB3...26 <chr>, DRB4...27 <chr>, DRB4...28 <chr>, DRB5...29 <chr>,\n#   DRB5...30 <chr>\n\n\n\nQ1: How many observations of how many variables are in the data?\nQ2: Are there groupings in the variables, i.e.¬†do certain variables ‚Äúgo together‚Äù somehow?\nT1: Re-create this plot\n\nRead this first:\n\nThink about: What is on the x-axis? What is on the y-axis? And also, it looks like we need to do some counting. Recall, that we can stick together a dplyr-pipeline with a call to ggplot, so here we will have to count of Cohort and Gender before plotting\n\n\n\n\n\n\nDoes your plot look different somehow? Consider peeking at the hint‚Ä¶\n\n\n\nClick here for hint\n\n\nPerhaps not everyone agrees on how to denote NAs in data. I have seen -99, -11, _ and so on‚Ä¶ Perhaps this can be dealt with in the instance we read the data from the file? I.e. in the actual function call to your read-function. Recall, how can we get information on the parameters of a ?function\n\n\nT2: Re-create this plot\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nPerhaps there is a function, which can cut continuous observations into a set of bins?\n\n\nSTOP! Make sure you handled how NAs are denoted in the data before proceeding, see hint below T1\n\nT3: Look at the data and create yet another plot as you see fit. Also skip the redundant variables Subject, Cell Type and Target Type\n\n\n\n\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 27\n   Experiment Cohort      Age Gender Race  `HLA-A...9` `HLA-A...10` `HLA-B...11`\n   <chr>      <chr>     <dbl> <chr>  <chr> <chr>       <chr>        <chr>       \n 1 eXL36      Healthy ‚Ä¶    37 F      White A*01:01     A*02:01      B*15:01     \n 2 eLH46      COVID-19‚Ä¶    57 F      White A*02:01:01  A*26:01:01   B*38:01:01  \n 3 eMR16      COVID-19‚Ä¶    NA <NA>   <NA>  A*02:01:01  A*02:01:01   B*13:02:01  \n 4 eQD131     COVID-19‚Ä¶    NA <NA>   <NA>  A*02:01:01  A*32:01:01   B*15:01:01  \n 5 eDH107     COVID-19‚Ä¶    72 F      <NA>  A*03:01:01  A*03:01:01   B*15:01:01  \n 6 ePD76      Healthy ‚Ä¶    33 M      White A*02:01     A*03:01      B*35:01     \n 7 eJL151     COVID-19‚Ä¶    79 F      <NA>  A*24:02:01  A*68:01:01   B*15:01:01  \n 8 eLH53      COVID-19‚Ä¶    42 M      White A*01:01:01  A*11:01:01   B*55:01:01  \n 9 eOX52      Healthy ‚Ä¶    33 M      White A*02:01     A*24:02      B*15:17     \n10 eHO135     COVID-19‚Ä¶    33 M      White A*03:01:01  A*31:01:02   B*07:02:01  \n# ‚Ñπ 19 more variables: `HLA-B...12` <chr>, `HLA-C...13` <chr>,\n#   `HLA-C...14` <chr>, DPA1...15 <chr>, DPA1...16 <chr>, DPB1...17 <chr>,\n#   DPB1...18 <chr>, DQA1...19 <chr>, DQA1...20 <chr>, DQB1...21 <chr>,\n#   DQB1...22 <chr>, DRB1...23 <chr>, DRB1...24 <chr>, DRB3...25 <chr>,\n#   DRB3...26 <chr>, DRB4...27 <chr>, DRB4...28 <chr>, DRB5...29 <chr>,\n#   DRB5...30 <chr>\n\n\nNow, a classic way of describing a cohort, i.e.¬†the group of subjects used for the study, is the so-called table1 and while we could build this ourselves, this one time, in the interest of exercise focus and time, we are going to ‚Äúcheat‚Äù and use an R-package, like so:\nNB!: This may look a bit odd initially, but if you render your document, you should be all good!\n\nlibrary(\"table1\") # <= Yes, this should normally go at the beginning!\nmeta_data |>\n  mutate(Gender = factor(Gender),\n         Cohort = factor(Cohort)) |>\n  table1(x = formula(~ Gender + Age + Race | Cohort),\n         data = _)\n\n\n\n\n\n\nCOVID-19-Acute(N=4)\nCOVID-19-B-Non-Acute(N=8)\nCOVID-19-Convalescent(N=90)\nCOVID-19-Exposed(N=3)\nHealthy (No known exposure)(N=39)\nOverall(N=144)\n\n\n\n\nGender\n\n\n\n\n\n\n\n\nF\n1 (25.0%)\n4 (50.0%)\n33 (36.7%)\n1 (33.3%)\n17 (43.6%)\n56 (38.9%)\n\n\nM\n2 (50.0%)\n3 (37.5%)\n36 (40.0%)\n0 (0%)\n21 (53.8%)\n62 (43.1%)\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n21 (23.3%)\n2 (66.7%)\n1 (2.6%)\n26 (18.1%)\n\n\nAge\n\n\n\n\n\n\n\n\nMean (SD)\n50.7 (17.0)\n43.7 (7.74)\n51.5 (15.3)\n35.0 (NA)\n33.3 (9.93)\n44.9 (15.7)\n\n\nMedian [Min, Max]\n52.0 [33.0, 67.0]\n42.0 [33.0, 53.0]\n53.0 [21.0, 79.0]\n35.0 [35.0, 35.0]\n31.0 [21.0, 62.0]\n42.0 [21.0, 79.0]\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n21 (23.3%)\n2 (66.7%)\n0 (0%)\n25 (17.4%)\n\n\nRace\n\n\n\n\n\n\n\n\nAfrican American\n1 (25.0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (2.6%)\n2 (1.4%)\n\n\nWhite\n2 (50.0%)\n7 (87.5%)\n13 (14.4%)\n0 (0%)\n28 (71.8%)\n50 (34.7%)\n\n\nAsian\n0 (0%)\n0 (0%)\n3 (3.3%)\n0 (0%)\n2 (5.1%)\n5 (3.5%)\n\n\nHispanic or Latino/a\n0 (0%)\n0 (0%)\n1 (1.1%)\n0 (0%)\n0 (0%)\n1 (0.7%)\n\n\nNative Hawaiian or Other Pacific Islander\n0 (0%)\n0 (0%)\n0 (0%)\n1 (33.3%)\n0 (0%)\n1 (0.7%)\n\n\nBlack or African American\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n3 (7.7%)\n3 (2.1%)\n\n\nMixed Race\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (2.6%)\n1 (0.7%)\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n73 (81.1%)\n2 (66.7%)\n4 (10.3%)\n81 (56.3%)\n\n\n\n\n\n\nNote how good this looks! If you have ever done a ‚ÄúTable 1‚Äù before, you know how painful they can be and especially if something changes in your cohort - Dynamic reporting to the rescue!\nLastly, before we proceed, the meta_data contains HLA data for both class I and class II (see background), but here we are only interested in class I, recall these are denoted HLA-A, HLA-B and HLA-C, so make sure to remove any non-class I, i.e.¬†the one after, denoted D-something.\n\nT4: Create a new version of the meta_data, which with respect to allele-data only contains information on class I and also fix the odd naming, e.g.¬†HLA-A...9 becomes A1 oand HLA-A...10 becomes A2 and so on for B1, B2, C1 and C2 (Think: How can we rename variables? And here, just do it ‚Äúmanually‚Äù per variable). Remember to assign this new data to the same meta_data-variable\n\n\n\n\nClick here for hint\n\n\nWhich tidyverse function subsets variables? Perhaps there is a function, which somehow matches a set of variables? And perhaps for the initiated this is compatible with regular expressions (If you don‚Äôt know what this means - No worries! If you do, see if you utilise this to simplify your variable selection)\n\n\n\n\nBefore we proceed, this is the data we will carry on with:\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 11\n   Experiment Cohort        Age Gender Race  A1    A2    B1    B2    C1    C2   \n   <chr>      <chr>       <dbl> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 eLH46      COVID-19-C‚Ä¶    57 F      White A*02‚Ä¶ A*26‚Ä¶ B*38‚Ä¶ B*51‚Ä¶ C*12‚Ä¶ C*15‚Ä¶\n 2 eHO132     COVID-19-C‚Ä¶    65 F      White A*02‚Ä¶ A*24‚Ä¶ B*14‚Ä¶ B*35‚Ä¶ C*04‚Ä¶ C*08‚Ä¶\n 3 eAV88      Healthy (N‚Ä¶    24 M      White A*02‚Ä¶ A*03‚Ä¶ B*27‚Ä¶ B*40‚Ä¶ C*03‚Ä¶ C*07‚Ä¶\n 4 ePD87      COVID-19-C‚Ä¶    47 M      White A*03‚Ä¶ A*24‚Ä¶ B*07‚Ä¶ B*08‚Ä¶ C*07‚Ä¶ C*07‚Ä¶\n 5 eHO126     COVID-19-C‚Ä¶    37 F      <NA>  A*01‚Ä¶ A*24‚Ä¶ B*07‚Ä¶ B*57‚Ä¶ C*06‚Ä¶ C*07‚Ä¶\n 6 eQD116     COVID-19-C‚Ä¶    66 F      <NA>  A*03‚Ä¶ A*11‚Ä¶ B*35‚Ä¶ B*35‚Ä¶ C*04‚Ä¶ C*04‚Ä¶\n 7 eXL36      Healthy (N‚Ä¶    37 F      White A*01‚Ä¶ A*02‚Ä¶ B*15‚Ä¶ B*40‚Ä¶ C*03‚Ä¶ C*04‚Ä¶\n 8 eJL158     COVID-19-A‚Ä¶    33 M      White A*02‚Ä¶ A*24‚Ä¶ B*15‚Ä¶ B*40‚Ä¶ C*03‚Ä¶ C*15‚Ä¶\n 9 eEE217     Healthy (N‚Ä¶    32 F      White A*02‚Ä¶ A*02‚Ä¶ B*15‚Ä¶ B*44‚Ä¶ C*04‚Ä¶ C*05‚Ä¶\n10 eHO131     COVID-19-C‚Ä¶    58 F      <NA>  A*02‚Ä¶ A*02‚Ä¶ B*15‚Ä¶ B*51‚Ä¶ C*03‚Ä¶ C*15‚Ä¶\n\n\nNow, we have a beautiful tidy-dataset, recall that this entails, that each row is an observation, each column is a variable and each cell holds one value.\n\n\n\nThe Peptide Details Data\nLet‚Äôs start with simply having a look see:\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   `TCR BioIdentity`            TCR Nucleotide Seque‚Ä¶¬π Experiment `ORF Coverage`\n   <chr>                        <chr>                  <chr>      <chr>         \n 1 CASSVWETSGSYEQFF+TCRBV09-01‚Ä¶ TCTCTGGAGCTGGGGGACTCA‚Ä¶ eEE228     membrane glyc‚Ä¶\n 2 CASSLGRGGREQYF+TCRBV28-01+T‚Ä¶ CTGGAGTCCGCCAGCACCAAC‚Ä¶ eEE226     ORF1ab        \n 3 CASSYTAGGSQPQHF+TCRBV06-06+‚Ä¶ GAGTTGGCTGCTCCCTCCCAG‚Ä¶ eHO134     ORF1ab        \n 4 CASSQGQGLSYEQYF+TCRBV19-01+‚Ä¶ ACATCGGCCCAAAAGAACCCG‚Ä¶ ePD83      ORF3a         \n 5 CASSLVPRELSNQPQHF+TCRBV05-0‚Ä¶ TTGGAGCTGGGGGACTCGGCC‚Ä¶ eOX46      surface glyco‚Ä¶\n 6 CASSQIDRGGNNEQFF+TCRBV04-01‚Ä¶ GCCCTGCAGCCAGAAGACTCA‚Ä¶ eEE226     surface glyco‚Ä¶\n 7 CASSLGSIAYEQYF+TCRBV11-02+T‚Ä¶ ATCCAGCCTGCAAAGCTTGAG‚Ä¶ eAV91      ORF1ab        \n 8 CASSLGGAQHYGYTF+TCRBV12-X+T‚Ä¶ CAGCCCTCAGAACCCAGGGAC‚Ä¶ eEE226     ORF1ab        \n 9 CASSYSPKDEAFF+TCRBV27-01+TC‚Ä¶ ATCCTGGAGTCGCCCAGCCCC‚Ä¶ eXL31      ORF1ab        \n10 CASSFQGSYEQYF+TCRBV19-01+TC‚Ä¶ ACTGTGACATCGGCCCAAAAG‚Ä¶ eEE226     ORF1ab        \n# ‚Ñπ abbreviated name: ¬π‚Äã`TCR Nucleotide Sequence`\n# ‚Ñπ 3 more variables: `Amino Acids` <chr>, `Start Index in Genome` <dbl>,\n#   `End Index in Genome` <dbl>\n\n\n\nQ3: How many observations of how many variables are in the data?\n\nThis is a rather big data set, so let us start with two ‚Äútricks‚Äù to handle this, first:\n\nWrite the data back into your data-folder, using the filename peptide-detail-ci.csv.gz, note the appending of .gz, which is automatically recognised and results in gz-compression\nNow, check in your data folder, that you have two files peptide-detail-ci.csv and peptide-detail-ci.csv.gz, delete the former\nAdjust your reading-the-data-code in the ‚ÄúLoad Data‚Äù-section, to now read in the peptide-detail-ci.csv.gz-file\n\n\n\n\nClick here for hint\n\n\nJust as you can read a file, you can of course also write a file. Note the filetype we want to write here is csv. If you in the console type e.g.¬†readr::wr and then hit the tab-button, you will see the different functions for writing different filetypes\n\nThen:\n\nT5: As before, let‚Äôs immediately subset the peptide_data to the variables of interest: TCR BioIdentity, Experiment and Amino Acids. Remember to assign this new data to the same peptide_data-variable to avoid cluttering your environment with redundant variables. Bonus: Did you know you can click the Environment pane and see which variables you have?\n\n\n\n\nOnce again, before we proceed, this is the data we will carry on with:\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 3\n   Experiment `TCR BioIdentity`                        `Amino Acids`            \n   <chr>      <chr>                                    <chr>                    \n 1 eAV93      CASSSSTGTGVYGYTF+TCRBV07-09+TCRBJ01-02   NPANNAAIV,NPANNAAIVL,RNP‚Ä¶\n 2 eOX43      CASSLLALGEQFF+TCRBV07-X+TCRBJ02-01       DFLEYHDVR,EDFLEYHDVR,LEY‚Ä¶\n 3 ePD76      CASSSYSRTDNEQFF+TCRBV06-05+TCRBJ02-01    AFLLFLVLI,FLAFLLFLV,FYLC‚Ä¶\n 4 eXL30      CASSLVGAANEKLFF+TCRBV07-02+TCRBJ01-04    AFLLFLVLI,FLAFLLFLV,FYLC‚Ä¶\n 5 eOX46      CSVEGFWTLASYNEQFF+TCRBV29-01+TCRBJ02-01  MVMCGGSLYV,VMCGGSLYV     \n 6 eJL161     CASSAGAGHGYTF+TCRBV09-01+TCRBJ01-02      HTTDPSFLGRY              \n 7 eXL30      CASSPLGQGMNTEAFF+TCRBV19-01+TCRBJ01-01   AEAELAKNVSL,AELAKNVSLDNVL\n 8 eQD136     CASSLSGTGELFF+TCRBV05-01+TCRBJ02-02      QYIKWPWYI,YEQYIKWPW,YEQY‚Ä¶\n 9 eEE224     CASSQTSGRPSGANVLTF+TCRBV28-01+TCRBJ02-06 ASQSIIAYTM,RSVASQSII,SII‚Ä¶\n10 eEE226     CASSLGLAGGKDTQYF+TCRBV07-02+TCRBJ02-03   AFPFTIYSL,GYINVFAFPF,INV‚Ä¶\n\n\n\nQ4: Is this tidy data? Why/why not?\nT6: See if you can find a way to create the below data, from the above\n\n\n\n\n\npeptide_data |> \n  sample_n(size = 10)\n\n# A tibble: 10 √ó 5\n   Experiment CDR3b            V_gene     J_gene     `Amino Acids`              \n   <chr>      <chr>            <chr>      <chr>      <chr>                      \n 1 eEE240     CAWRATLGARNEKLFF TCRBV30-01 TCRBJ01-04 AFLLFLVLI,FLAFLLFLV,FYLCFL‚Ä¶\n 2 eHO141     CASSFMKEHNEQFF   TCRBV28-01 TCRBJ02-01 FAYANRNRF,LQFAYANRNR,YANRN‚Ä¶\n 3 eOX46      CASSPPDRVVEQYF   TCRBV18-01 TCRBJ02-07 APAHISTI,LIVNSVLLFL,LLFLAF‚Ä¶\n 4 eAV88      CASSLFSGRPGETQYF TCRBV28-01 TCRBJ02-05 FVDGVPFVV                  \n 5 eAV93      CASSDWGGRMETQYF  TCRBV04-02 TCRBJ02-05 KSWMESEFRV,SWMESEFRVY,WMES‚Ä¶\n 6 eDH107     CASSIRTKDEQYF    TCRBV19-01 TCRBJ02-07 AYKTFPPTEPK,KTFPPTEPK      \n 7 eQD128     CASSLSGGSGNSPLHF TCRBV27-01 TCRBJ01-06 AFLLFLVLI,FLAFLLFLV,FYLCFL‚Ä¶\n 8 eXL30      CASSQWVYSEAFF    TCRBV04-02 TCRBJ01-01 FVCNLLLLFV,LLFVTVYSHL,TVYS‚Ä¶\n 9 eEE240     CASSYSSGSGNEQFF  TCRBV06-05 TCRBJ02-01 QYIKWPWYI,YEQYIKWPW,YEQYIK‚Ä¶\n10 eHO134     CASSPGPREKLFF    TCRBV27-01 TCRBJ01-04 HTTDPSFLGRY                \n\n\n\n\n\nClick here for hint\n\n\nFirst: Compare the two datasets and identify what happened? Did any variables ‚Äúdissappear‚Äù and did any ‚Äúappear‚Äù? Ok, so this is a bit tricky, but perhaps there is a function to separate a composit (untidy) column into a set of new variables based on a separator? But what is a separator? Just like when you read a file with Comma Separated Values, a separator denotes how a composite string is divided into fields. So look for such a repeated values, which seem to indeed separate such fields. Also, be aware, that character, which can mean more than one thing, may need to be ‚Äúescaped‚Äù using an initial two backslashed, i.e.¬†‚Äú\\x‚Äù, where x denotes the character needing to be ‚Äúescaped‚Äù\n\n\nT7: Add a variable, which counts how many peptides are in each observation of Amino Acids\n\n\n\n\n\n\n\nClick here for hint\n\n\nWe have been working with the stringr-package, perhaps the contains a function to somehow count the number of occurrences of a given character in a string? Again, remember you can type e.g.¬†stringr::str_ and then hit the tab-button to see relevant functions\n\n\npeptide_data |> \n  sample_n(size = 10)\n\n# A tibble: 10 √ó 6\n   Experiment CDR3b               V_gene         J_gene `Amino Acids` n_peptides\n   <chr>      <chr>               <chr>          <chr>  <chr>              <dbl>\n 1 eEE224     CASSLDLTDTQYF       TCRBV03-01/03‚Ä¶ TCRBJ‚Ä¶ APAHISTI,LIV‚Ä¶          4\n 2 eXL31      CASSPLNVETQYF       TCRBV18-01     TCRBJ‚Ä¶ FVCNLLLLFV,L‚Ä¶          3\n 3 eQD110     CASSYAGLAGEQFF      TCRBV06-X      TCRBJ‚Ä¶ KAYNVTQAF              1\n 4 eHO130     CASSADVTGGFTGELFF   TCRBV02-01     TCRBJ‚Ä¶ AFLLFLVLI,FL‚Ä¶         11\n 5 eOX49      CASSVLIETAARRNTEAFF TCRBV09-01     TCRBJ‚Ä¶ VLWAHGFEL              1\n 6 eQD128     CASSLGGTEYEQYF      TCRBV05-01     TCRBJ‚Ä¶ AFLLFLVLI,FL‚Ä¶         11\n 7 eHO141     CASSSWGTNEKLFF      TCRBV12-X      TCRBJ‚Ä¶ APGQTGKIA,GQ‚Ä¶          5\n 8 eEE226     CASSAQTGEGEKLFF     TCRBV09-01     TCRBJ‚Ä¶ ILHCANFNV              1\n 9 eOX54      CSAIEGGTGELFF       TCRBV20-X      TCRBJ‚Ä¶ AFPFTIYSL,GY‚Ä¶          7\n10 eXL31      CASSGTDYNEQFF       TCRBV19-01     TCRBJ‚Ä¶ IMLIIFWFSL,M‚Ä¶          2\n\n\n\nT8: Re-create the following plot\n\n\n\n\n\n\n\nQ4: What is the maximum number of peptides assigned to one observation?\nT9: Using the str_c- and the seq-functions, re-create the below\n\n\n\n[1] \"peptide_1\" \"peptide_2\" \"peptide_3\" \"peptide_4\" \"peptide_5\"\n\n\n\n\n\nClick here for hint\n\n\nIf you‚Äôre uncertain on how a function works, try going into the console and in this case e.g.¬†type str_c(\"a\", \"b\") and seq(from = 1, to = 3) and see if you combine these?\n\n\nT10: Use, what you learned about separating in T6 and the vector-of-strings you created in T9 adjusted to the number from Q4 to create the below data\n\n\n\n\n\n\n\nClick here for hint\n\n\nIn the console, write ?separate and think about how you used it earlier. Perhaps you can not only specify a vector to separate into, but also specify a function, which returns a vector?\n\n\npeptide_data |> \n  sample_n(size = 10)\n\n# A tibble: 10 √ó 18\n   Experiment CDR3b        V_gene J_gene peptide_1 peptide_2 peptide_3 peptide_4\n   <chr>      <chr>        <chr>  <chr>  <chr>     <chr>     <chr>     <chr>    \n 1 eHO134     CASSPGLARHY‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ HTTDPSFL‚Ä¶ <NA>      <NA>      <NA>     \n 2 eEE226     CASSRSGGSTD‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ FLNGSCGSV <NA>      <NA>      <NA>     \n 3 eEE228     CASSIRSSYEQ‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ FFSNVTWFH FLPFFSNVT LPFFSNVTW LPFFSNVT‚Ä¶\n 4 eOX46      CASSGQLAFEV‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ FVCNLLLL‚Ä¶ LLFVTVYS‚Ä¶ TVYSHLLLV <NA>     \n 5 eEE226     CASSLGGAKTE‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ FKVSIWNL‚Ä¶ ILLIIMRT‚Ä¶ IMRTFKVSI KVSIWNLDY\n 6 eLH54      unproductive TCRBV‚Ä¶ TCRBJ‚Ä¶ FLNGSCGSV <NA>      <NA>      <NA>     \n 7 eXL31      CASSLSNGNEQ‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ QLMCQPILL QLMCQPIL‚Ä¶ <NA>      <NA>     \n 8 eLH47      CASSPQTGFER‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ LSPRWYFYY SPRWYFYYL <NA>      <NA>     \n 9 eAV88      CASSWDSLSGT‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ AFLLFLVLI FLAFLLFLV FYLCFLAFL FYLCFLAF‚Ä¶\n10 eOX52      CASSFAGEGYTF TCRBV‚Ä¶ TCRBJ‚Ä¶ AFPFTIYSL GYINVFAF‚Ä¶ INVFAFPF‚Ä¶ MGYINVFAF\n# ‚Ñπ 10 more variables: peptide_5 <chr>, peptide_6 <chr>, peptide_7 <chr>,\n#   peptide_8 <chr>, peptide_9 <chr>, peptide_10 <chr>, peptide_11 <chr>,\n#   peptide_12 <chr>, peptide_13 <chr>, n_peptides <dbl>\n\n\n\nQ5: Now, presumable you got a warning, discuss in your group why that is?\nQ6: With respect to peptide_n, discuss in your group, if this is wide- or long-data?\n\nNow, finally we will use the what we prepared for today, data-pivotting. There are two functions, namely pivot_wider() and pivot_longer(). Also, now, we will use a trick when developing ones data pipeline, while working with new functions, that on might not be completely comfortable with. You have seen the sample_n()-function several times above and we can use that to randomly sample n-observations from data. This we can utilise to work with a smaller data set in the development face and once we are ready, we can increase this n gradually to see if everything continues to work as anticipated.\n\nT11: Using the peptide_data, run a few sample_n()-calls with varying degree of n to make sure, that you get a feeling for what is going on\nT12: From the peptide_data data above, with peptide_1, peptide_2, etc. create this data set using one of the data-pivotting functions. Remember to start initially with sampling a smaller data set and then work on that first! Also, once you‚Äôre sure you‚Äôre good to go, reuse the peptide_data-variable as we don‚Äôt want huge redundant data sets floating around in our environment\n\n\n\n\n\n\n\nClick here for hint\n\n\nIf the pivotting is not clear at all, then do what I do, create some example data:\n\nmy_data <- tibble(\n  id = str_c(\"id_\", 1:10),\n  var_1 = round(rnorm(10),1),\n  var_2 = round(rnorm(10),1),\n  var_3 = round(rnorm(10),1))\n\n‚Ä¶and then play around with that. A small set like the one above is easy to handle, so perhaps start with that and then pivot back and forth a few times using the pivot_wider()-/pivot_longer()-functions. Use the View()-function to inspect and get a better overview of the results of pivotting.\n\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment CDR3b            V_gene     J_gene    n_peptides peptide_n peptide\n   <chr>      <chr>            <chr>      <chr>          <dbl> <chr>     <chr>  \n 1 eMR13      CASSDRGPNEKLFF   TCRBV27-01 TCRBJ01-‚Ä¶          1 peptide_4 <NA>   \n 2 eOX49      CASSLEGNIQYF     TCRBV07-09 TCRBJ02-‚Ä¶         11 peptide_4 FYLCFL‚Ä¶\n 3 eXL32      CSVATTGVSTDTQYF  TCRBV29-01 TCRBJ02-‚Ä¶         11 peptide_9 MIELSL‚Ä¶\n 4 eOX52      CACRYPDEAFF      TCRBV30-01 TCRBJ01-‚Ä¶          2 peptide_3 <NA>   \n 5 eEE240     CSATDTPEDTQYF    TCRBV20-X  TCRBJ02-‚Ä¶          1 peptide_1 KMQRML‚Ä¶\n 6 eEE226     CASSEPVYGEQYF    TCRBV02-01 TCRBJ02-‚Ä¶          6 peptide_4 LPFNDG‚Ä¶\n 7 eXL31      CSARVYDRMNTEAFF  TCRBV20-X  TCRBJ01-‚Ä¶          2 peptide_6 <NA>   \n 8 eEE224     CSVEDLTGRASYEQYF TCRBV29-01 TCRBJ02-‚Ä¶          4 peptide_‚Ä¶ <NA>   \n 9 eXL30      CASSVISGHPNTEAFF TCRBV27-01 TCRBJ01-‚Ä¶         11 peptide_‚Ä¶ <NA>   \n10 eEE226     CASSQEPGLGPETQYF TCRBV05-01 TCRBJ02-‚Ä¶         10 peptide_‚Ä¶ <NA>   \n\n\n\nQ7: You will see some NAs in the peptide-variable, discuss in your group from where these arise?\nQ8: How many rows and columns now and how does this compare with Q3? Discuss why/why not it is different?\nT13: Now, loose the redundant variables n_peptides and peptide_n and also get rid of the NAs in the peptide-column and make sure, that we only have unique observations, i.e.¬†there are no repeated rows/observations\n\n\n\n\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 5\n   Experiment CDR3b                 V_gene           J_gene     peptide      \n   <chr>      <chr>                 <chr>            <chr>      <chr>        \n 1 eEE228     CASSYTSGSTDTQYF       TCRBV06-05       TCRBJ02-03 AFLLFLVLI    \n 2 eXL31      CASSQGWGEQYF          TCRBV04-02       TCRBJ02-07 LLFLVLIML    \n 3 eXL30      CASIGTETYEQYF         TCRBV02-01       TCRBJ02-07 KEIIFLEGETL  \n 4 eXL30      CASSVAGTSDEQYF        TCRBV09-01       TCRBJ02-07 IELSLIDFYL   \n 5 eXL31      CASSLGAGGEFRPILQRTQYF X                TCRBJ02-05 AYSNNSIAIPTNF\n 6 eXL31      CSAEGGGTQYF           TCRBV20-01       TCRBJ02-05 LLFLVLIML    \n 7 eXL31      RASSFLHSFRPDTEAFF     TCRBV07-03       TCRBJ01-01 IELSLIDFYL   \n 8 ePD84      CAWNPWGQGRGEQFF       TCRBV30-01       TCRBJ02-01 FLWLLWPVTL   \n 9 eXL30      CASSLADLSYGYTF        TCRBV07-09       TCRBJ01-02 LTDEMIAQY    \n10 eAV88      CASSEMNTEAFF          TCRBV06-02/06-03 TCRBJ01-01 FKVSIWNLDY   \n\n\n\nQ8: Now how many rows and columns and is this data tidy? Discuss in your group why/why not?\n\nAgain, we turn to the stringr-package, as we need to make sure that the sequence data does indeed only contain valid characters. There are a total of 20 proteogenic amino acids, which we symbolise using ARNDCQEGHILKMFPSTWYV.\n\nT14: Use the str_detect()-function to filter the CDR3b and peptide variables using a pattern of [^ARNDCQEGHILKMFPSTWYV] and then play with the negate-parameter so see what happens\n\n\n\n\n\n\n\nClick here for hint\n\n\nAgain, try to play a bit around with the function in the console, type e.g.¬†str_detect(string = \"ARND\", pattern = \"A\") and str_detect(string = \"ARND\", pattern = \"C\") and then recall, that the filter-function requires a logical vector, i.e.¬†a vector of TRUE and FALSE to filter the rows\n\n\nT15: Add two new variables to the data, k_CDR3b and k_peptide each signifying the length of the respective sequences\n\n\n\n\n\n\n\nClick here for hint\n\n\nAgain, we‚Äôre working with strings, so perhaps there is a package of interest and perhaps in that package, there is a function, which can get the length of a string?\n\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment CDR3b            V_gene     J_gene     peptide   k_CDR3b k_peptide\n   <chr>      <chr>            <chr>      <chr>      <chr>       <int>     <int>\n 1 eXL30      CASSFPGQGQETQYF  TCRBV12-X  TCRBJ02-05 LPFNDGVY‚Ä¶      15        10\n 2 eOX46      CASSESGGAYEQYF   TCRBV10-02 TCRBJ02-07 SLIDFYLC‚Ä¶      14        10\n 3 eEE240     CSASTTSGGQETQYF  TCRBV20-01 TCRBJ02-05 SLIDFYLC‚Ä¶      15        10\n 4 eEE228     CASSLSIGSGQFF    TCRBV27-01 TCRBJ02-01 APAHISTI       13         8\n 5 eMR16      CASSPPIGTHQETQYF TCRBV11-03 TCRBJ02-05 YFLQSINF‚Ä¶      16        10\n 6 eEE240     CASSYLSGPSEWPQHF TCRBV21-01 TCRBJ01-05 MIELSLID‚Ä¶      16        10\n 7 eOX46      CASSEHAGGSYEQYF  TCRBV02-01 TCRBJ02-07 NVFAFPFT‚Ä¶      15        10\n 8 eOX43      CASSLAGEAYEQYF   TCRBV10-02 TCRBJ02-07 TVLSFCAFA      14         9\n 9 eOX49      CASSSTPLGQPQHF   TCRBV27-01 TCRBJ01-05 MIELSLID‚Ä¶      14        10\n10 eXL31      CASPPRLHDYNEQFF  TCRBV07-09 TCRBJ02-01 MIELSLID‚Ä¶      15        10\n\n\n\nT16: Re-create this plot\n\n\n\n\n\n\n\nQ9: What is the most predominant length of the CDR3b-sequences?\nT17: Re-create this plot\n\n\n\n\n\n\n\nQ10: What is the most predominant length of the peptide-sequences?\nQ11: Discuss in your group, if this data set is tidy or not?\n\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment CDR3b             V_gene          J_gene peptide k_CDR3b k_peptide\n   <chr>      <chr>             <chr>           <chr>  <chr>     <int>     <int>\n 1 eOX49      CASTTIVGQGYPYEQYF TCRBV28-01      TCRBJ‚Ä¶ FLWLLW‚Ä¶      17        10\n 2 eAV91      CASSQIGTLDTQYF    TCRBV05-05      TCRBJ‚Ä¶ KLNDLC‚Ä¶      14        10\n 3 eOX52      CASSLGVGEQFF      TCRBV07-06      TCRBJ‚Ä¶ NATRFA‚Ä¶      12         9\n 4 eAV93      CASSPGFLGTGIYEQYF TCRBV09-01      TCRBJ‚Ä¶ SLIDFY‚Ä¶      17        10\n 5 eLH47      CAIILIGQSDRNEQFF  TCRBV07-09      TCRBJ‚Ä¶ FAFACP‚Ä¶      16        13\n 6 eXL30      CASSQGTGIGQFF     TCRBV04-01      TCRBJ‚Ä¶ LIVNSV‚Ä¶      13        10\n 7 ePD85      CASSRGQGLNYEQYF   TCRBV19-01      TCRBJ‚Ä¶ YQIGGY‚Ä¶      15        10\n 8 ePD83      CASSMGLGESYEQYF   TCRBV19-01      TCRBJ‚Ä¶ YQIGGY‚Ä¶      15         9\n 9 eAV93      CASSPGTGGEQYF     TCRBV12-03/12-‚Ä¶ TCRBJ‚Ä¶ GVEHVT‚Ä¶      13        10\n10 eOX49      CASSSNTGAYGYTF    TCRBV03-01/03-‚Ä¶ TCRBJ‚Ä¶ LIVNSV‚Ä¶      14        10\n\n\n\n\nCreating one data set from two data sets\nBefore we move onto using the family of *_join-functions you prepared for today, we will just take a quick peek at the meta data again:\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 11\n   Experiment Cohort        Age Gender Race  A1    A2    B1    B2    C1    C2   \n   <chr>      <chr>       <dbl> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 eTH332     COVID-19-C‚Ä¶    NA <NA>   <NA>  \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 2 eQD112     COVID-19-C‚Ä¶    65 M      <NA>  \"A*2‚Ä¶ \"A*2‚Ä¶ \"B*0‚Ä¶ \"B*3‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 3 ePD84      Healthy (N‚Ä¶    29 F      Asian \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*1‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 4 ePD87      COVID-19-C‚Ä¶    47 M      White \"A*0‚Ä¶ \"A*2‚Ä¶ \"B*0‚Ä¶ \"B*0‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 5 ePD83      Healthy (N‚Ä¶    29 F      Asian \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*1‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 6 eMR16      COVID-19-C‚Ä¶    NA <NA>   <NA>  \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*1‚Ä¶ \"B*1‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 7 eHO128     COVID-19-C‚Ä¶    49 M      <NA>  \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*0‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 8 eJL157     COVID-19-C‚Ä¶    27 F      <NA>  \"A*1‚Ä¶ \"A*1‚Ä¶ \"B*0‚Ä¶ \"B*1‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 9 eQD136     COVID-19-C‚Ä¶    NA <NA>   <NA>  \"A*0‚Ä¶ \"A*6‚Ä¶ \"B*0‚Ä¶ \"B*1‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n10 ePD100     COVID-19-C‚Ä¶    66 M      <NA>  \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n\n\nRemember you can scroll in the data.\n\nQ12: Discuss in your group, if this data with respect to the A1-, A2-, B1-, B2-, C1- and C2-variables is a wide- or a long-data format?\n\nAs with the peptide_data, we will now have to use data-pivotting again. I.e.:\n\nT18: use either the pivot_wider- or pivot_longer-function to create the following data:\n\n\n\n\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment Cohort                        Age Gender Race  Gene  Allele    \n   <chr>      <chr>                       <dbl> <chr>  <chr> <chr> <chr>     \n 1 eQD114     COVID-19-Convalescent          73 M      <NA>  C1    C*07:01:01\n 2 eQD120     COVID-19-Convalescent          62 F      <NA>  C1    C*03:04:01\n 3 eEE226     Healthy (No known exposure)    21 F      White C2    C*07:02   \n 4 eHH174     Healthy (No known exposure)    31 F      White C2    C*15:02   \n 5 eMR16      COVID-19-Convalescent          NA <NA>   <NA>  C1    C*06:02:01\n 6 eJL151     COVID-19-Convalescent          79 F      <NA>  B2    B*40:01:02\n 7 eQD139     COVID-19-Convalescent          NA <NA>   <NA>  C1    C*01:02:01\n 8 eHO129     COVID-19-Convalescent          66 F      Asian C1    C*08:01:01\n 9 eOX49      Healthy (No known exposure)    21 M      White A2    A*26:01   \n10 eQD118     COVID-19-Convalescent          67 F      <NA>  B2    B*51:01:01\n\n\nRemember, what we are aiming for here, is to create one data set from two. So:\n\nQ13: Discuss in your group, which variable(s?) define the same observations between the peptide_data and the meta_data?\n\nOnce you have agreed upon Experiment, then use that knowledge to subset the meta_data to the variables-of-interest:\n\n\n\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 2\n   Experiment Allele      \n   <chr>      <chr>       \n 1 eMR23      \"\"          \n 2 eLH59      \"A*01:01:01\"\n 3 eQD129     \"A*02:01:01\"\n 4 eJL164     \"C*15:02:01\"\n 5 eQD125     \"C*08:01:01\"\n 6 eMR14      \"B*07:02:01\"\n 7 eMR25      \"\"          \n 8 eMR14      \"C*06:02:01\"\n 9 eJL147     \"A*11:01\"   \n10 eMR18      \"A*02:01:01\"\n\n\nUse the View()-function again, to look at the meta_data - Notice something? Some alleles are e.g.¬†A*11:01, whereas others are B*51:01:02. You can find information on why, by visiting Nomenclature for Factors of the HLA System.\nLong story short, we only want to include Field 1 (allele group) and Field 2 (Specific HLA protein). You have prepared the stringr-package for today. See if you can find a way to reduce e.g.¬†B*51:01:02 to B*51:01 and then create a new variable Allele_F_1_2 accordingly, while also removing the ...x (where x is a number) subscripts from the Gene-variable (It is an artifact from having the data in a wide format, where you cannot have two variables with the same name) and also, remove any NAs and \"\"s, denoting empty entries.\n\n\n\nClick here for hint\n\n\nThere are several ways this can be achieved, the easiest being to consider if perhaps a part of the string based on indices could be of interest. This term ‚Äúa part of a string‚Äù is called a substring, perhaps the stringr-package contains a function work with substring? In the console, type stringr:: and hit tab. This will display the functions available in the stringr-package. Scroll down and find the functionst starting with str_ and look for on, which might be relevant and remember you can use ?function_name to get more information on how a given function works.\n\n\n\n\n\nT19: Create the following data, according to specifications above:\n\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 3\n   Experiment Allele     Allele_F_1_2\n   <chr>      <chr>      <chr>       \n 1 eOX46      A*02:01    A*02:01     \n 2 eQD136     A*02:01:01 A*02:01     \n 3 eQD124     A*01:01:01 A*01:01     \n 4 eLH53      B*57:01:01 B*57:01     \n 5 eHH175     B*44:03    B*44:03     \n 6 eJL143     B*07:02    B*07:02     \n 7 eJL151     B*15:01:01 B*15:01     \n 8 eJL162     C*03:03:01 C*03:03     \n 9 eLH48      A*24:02:01 A*24:02     \n10 eMR12      A*02:01:01 A*02:01     \n\n\nThe asterix, i.e.¬†* is a rather annoying character because of ambiguity, so:\n\nT20: Clean the data a bit more, by removing the asterix and redundant variables:\n\n\n\n\n\nmeta_data |> \n  sample_n(size = 10)\n\n# A tibble: 10 √ó 2\n   Experiment Allele\n   <chr>      <chr> \n 1 eXL30      A02:01\n 2 eQD128     B39:01\n 3 eLH48      C08:02\n 4 eLH43      C04:09\n 5 eHO124     B44:27\n 6 eMR21      B44:03\n 7 eHO124     C07:04\n 8 eXL31      B44:03\n 9 eLH51      C07:02\n10 eLH48      B14:02\n\n\n\n\n\nClick here for hint 1\n\n\nAgain, the stringr-package may come in handy. Perhaps there is a function remove, one or more such pesky characters?\n\n\n\n\nClick here for hint 2\n\n\nGetting a weird error? Recall, that character ambiguity needs to be ‚Äúescaped‚Äù, you did this somehow earlier on‚Ä¶\n\nRecall the peptide_data?\n\npeptide_data |>\n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment CDR3b              V_gene         J_gene peptide k_CDR3b k_peptide\n   <chr>      <chr>              <chr>          <chr>  <chr>     <int>     <int>\n 1 eOX52      CASSLDLSNQPQHF     TCRBV06-05     TCRBJ‚Ä¶ AFPFTI‚Ä¶      14         9\n 2 eQD132     CASSTSYEQYF        TCRBV12-03/12‚Ä¶ TCRBJ‚Ä¶ INVFAF‚Ä¶      11        10\n 3 eXL37      CASSLISGANVLTF     TCRBV27-01     TCRBJ‚Ä¶ SLIDFY‚Ä¶      14        10\n 4 eXL31      CASSIALNTEAFF      TCRBV19-01     TCRBJ‚Ä¶ MIELSL‚Ä¶      13        10\n 5 eLH42      CASSRLAGGPGNEQFF   TCRBV28-01     TCRBJ‚Ä¶ LSPRWY‚Ä¶      16         9\n 6 eOX43      CASTAFGGKMDTEAFF   TCRBV05-01     TCRBJ‚Ä¶ VQELYS‚Ä¶      16        10\n 7 eQD128     CASSPNSYEQYF       TCRBV27-01     TCRBJ‚Ä¶ FLQSIN‚Ä¶      12         9\n 8 eOX46      CASSQSSGLHNPYNEQFF TCRBV19-01     TCRBJ‚Ä¶ KLSYGI‚Ä¶      18         9\n 9 eXL31      CASSQGQGVDTQYF     TCRBV03-01/03‚Ä¶ TCRBJ‚Ä¶ FLAFLL‚Ä¶      14         9\n10 eQD115     CASSRLTDANTEAFF    TCRBV07-09     TCRBJ‚Ä¶ PYRVVV‚Ä¶      15         9\n\n\n\nT21: Create a dplyr-pipeline, starting with the peptide_data, which joins it with the meta_data and remember to make sure that you get only unqiue observations of rows. Save this data into a new variable names peptide_meta_data (If you get a warning, discuss in your group what it means?)\n\n\n\n\n\n\n\nClick here for hint 1\n\n\nWhich family of functions do we use to join data? Also, perhaps here it would be prudent to start with working on a smaller data set, recall we could sample a number of rows yielding a smaller development data set\n\n\n\n\nClick here for hint 2\n\n\nYou should get a data set of around +3.000.000, take a moment to consider how that would have been to work with in Excel? Also, in case the servers are not liking this, you can consider subsetting the peptide_data prior to joining to e.g.¬†100,000 or 10,000 rows.\n\n\npeptide_meta_data |>\n  sample_n(10)\n\n# A tibble: 10 √ó 8\n   Experiment CDR3b            V_gene    J_gene peptide k_CDR3b k_peptide Allele\n   <chr>      <chr>            <chr>     <chr>  <chr>     <int>     <int> <chr> \n 1 eAV93      CASSPGTGDGGYTF   TCRBV05-‚Ä¶ TCRBJ‚Ä¶ WPVTLA‚Ä¶      14        10 B35:03\n 2 ePD83      CASSIGQGTTHTQYF  TCRBV19-‚Ä¶ TCRBJ‚Ä¶ SEHDYQ‚Ä¶      15        14 C07:06\n 3 eOX43      CASSQVLADSYEQYF  TCRBV14-‚Ä¶ TCRBJ‚Ä¶ FVCNLL‚Ä¶      15        10 B27:05\n 4 eQD129     CSASQTSRAQETQYF  TCRBV20-X TCRBJ‚Ä¶ FLAFLL‚Ä¶      15         9 C06:02\n 5 eXL27      CASSIAGNTEAFF    TCRBV19-‚Ä¶ TCRBJ‚Ä¶ QECVRG‚Ä¶      13        10 C07:04\n 6 eEE226     CASSYGQGLRYGYTF  TCRBV05-‚Ä¶ TCRBJ‚Ä¶ LLLDDF‚Ä¶      15         9 A01:01\n 7 eEE240     CASRPGQDTLRETQYF TCRBV28-‚Ä¶ TCRBJ‚Ä¶ LEYHDV‚Ä¶      16        10 C06:02\n 8 eEE228     CASSQQSNYGYTF    TCRBV04-‚Ä¶ TCRBJ‚Ä¶ TLATCE‚Ä¶      13        10 B35:03\n 9 eOX43      CSARDWTGGSSYEQYF TCRBV20-X TCRBJ‚Ä¶ GMEVTP‚Ä¶      16        11 C07:04\n10 eHO134     CASSYQSSSYEQYF   TCRBV05-‚Ä¶ TCRBJ‚Ä¶ KAYNVT‚Ä¶      14         9 B57:01\n\n\n\n\n\nAnalysis\nNow, that we have the data in a prepared and ready-to-analyse format, let us return to the two burning questions we had:\n\nWhat characterises the peptides binding to the HLAs?\nWhat characterises T-cell Receptors binding to the pMHC-complexes?\n\n\nPeptides binding to HLA\nAs we have touched upon multiple times, R is very flexible and naturally you can also create sequence logos. Finally, let us create a binding motif using the package ggseqlogo (More info here).\n\nT22: Subset the final peptide_meta_data-data to A02:01 and unique observations of peptides of length 9 and re-create the below sequence logo\n\n\n\n\nClick here for hint\n\n\nYou can pipe a vector of peptides into ggseqlogo, but perhaps you first need to pull that vector from the relevant variable in your tibble? Also, consider before that, that you‚Äôll need to make sure, you are only looking at peptides of length 9\n\n\n\n\n\n\n\n\n\n\n\nT23: Repeat for e.g.¬†B07:02 or another of your favorite alleles\n\nNow, let‚Äôs take a closer look at the sequence logo:\n\nQ14: Which positions in the peptide determines binding to HLA?\n\n\n\n\nClick here for hint\n\n\nRecall your Introduction to Bioinformatics course? And/or perhaps ask your fellow group members if they know?\n\n\n\nCDR3b-sequences binding to pMHC\n\nT24: Subset the peptide_meta_data, such that the length of the CDR3b is 15, the allele is A02:01 and the peptide is LLFLVLIML and re-create the below sequence logo of the CDR3b sequences:\n\n\n\n\n\n\n\n\n\n\n\nQ15: In your group, discuss what you see?\nT25: Play around with other combinations of k_CDR3b, Allele, and peptide and inspect how the logo changes\n\nDisclaimer: In this data set, we only get: A given CDR3b was found to recognise a given peptide in a given subject and that subject had a given haplotype - Something‚Äôs missing‚Ä¶ Perhaps if you have had immunology, then you can spot it? There is a trick to get around this missing information, but that‚Äôs beyond scope of what we‚Äôre working with here."
  },
  {
    "objectID": "lab05.html#epilogue",
    "href": "lab05.html#epilogue",
    "title": "Lab 5: Data Wrangling II",
    "section": "Epilogue",
    "text": "Epilogue\nThat‚Äôs it for today - I know this overwhelming now, but commit to it and you WILL be plenty rewarded! I hope today was at least a glimpse into the flexibility and capabilities of using tidyverse for applied Bio Data Science\n‚Ä¶also, noticed something? We spend maybe 80% of the time here on dealing with data-wrangling and then once we‚Äôre good to go, the analysis wasn‚Äôt that time consuming - That‚Äôs often the way it ends up going, you‚Äôll spend a lot of time on data handling and getting the tidyverse toolbox in your toolbelt, will allow you to be so much more effecient in your data wrangling, so you can get to the fun part as quick as possible!"
  },
  {
    "objectID": "lab05.html#sec-assignment",
    "href": "lab05.html#sec-assignment",
    "title": "Lab 5: Data Wrangling II",
    "section": "Today‚Äôs Assignment",
    "text": "Today‚Äôs Assignment\nAfter today, we are halfway through the labs of the course, so now is a good time to spend some time recalling what we have been over and practising writing a reproducible Quarto-report.\nYour group assignment today is to condense the exercises into a group micro-report! Talk together and figure out how to destill the exercises from today into one small end-to-end runable reproducible micro-report. DO NOT include ALL of the exercises, but rather include as few steps as possible to arrive at your results. Be vey consise!\nBut WHY? WHY are you not specifying exactly what we need to hand in? Because we are training taking independent decisions, which is crucial in applied bio data science, so take a look at the combined group code, select relevant sections and condense - If you don‚Äôt make it all the way through the exercises, then condense and present what you were able to arrive at! What do you think is central/important/indispensable? Also, these hand ins are NOT for us to evaluate you, but for you to train creating products and the get feedback on your progress!\nIMPORTANT: Remember to check the ASSIGNMENT GUIDELINES\n‚Ä¶and as always - Have fun!"
  },
  {
    "objectID": "lab06.html",
    "href": "lab06.html",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "",
    "text": "broom\npurrr"
  },
  {
    "objectID": "lab06.html#schedule",
    "href": "lab06.html#schedule",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.30: Recap of lab 5\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab06.html#learning-materials",
    "href": "lab06.html#learning-materials",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nQuestionnaire (brief, 5-10 min): Course Midway Evaluation\nR4DS Book (Note, this is intentionally 1.ed.): Chapter 22: Introduction, Chapter 23: Model Basics, Chapter 24: Model Building, Chapter 25: Many models\nVideo: Broom: Converting Statistical Models to Tidy Data Frames\nVideo: Alex Hayes | Solving the model representation problem with broom | RStudio (2019)\nVideo: ‚ÄúThe Joy of Functional Programming (for Data Science)‚Äù with Hadley Wickham"
  },
  {
    "objectID": "lab06.html#learning-objectives",
    "href": "lab06.html#learning-objectives",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUnderstand and apply simple purrr-functions for element-wise function application\nUnderstand and apply grouped supervised models to form nested model objects\nUnderstand and apply the broom-functions for tidying various model objects\nPerform a basic principal component analysis for dimension reduction of high dimensional data\nPerform a basic unsupervised k-means clustering of high dimensional data\n\n\nSTOP: The following is not yet completed!"
  },
  {
    "objectID": "lab06.html#sec-exercises",
    "href": "lab06.html#sec-exercises",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Exercises",
    "text": "Exercises"
  },
  {
    "objectID": "lab06.html#throwback",
    "href": "lab06.html#throwback",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Throwback‚Ä¶",
    "text": "Throwback‚Ä¶\nUsing the tibble() function, re-create the data from this visualisation and then create your version of how this data could be visualised in a more informative manner:\n\n\n\n\n\nAlso‚Ä¶ If you still need to be convinced of the flexibility of ggplot, try running this code:\n\nxy <- seq(from = -3,\n          to = 3, \n          by = 0.01)\nexpand.grid(x = xy,\n            y = xy) |>\n  ggplot(mapping = aes(\n    x = (1 - x - sin(y^2)),\n    y = (1 + y - cos(x^2)))) +\n  geom_point(alpha = 0.05,\n             shape = 20,\n             size = 0) +\n  theme_void() +\n  coord_polar()\n\nIf you are curious about what is going on here, try googling ‚ÄúGenerative art‚Äù‚Ä¶ Anyhoo‚Ä¶ Let us move on‚Ä¶"
  },
  {
    "objectID": "lab06.html#prologue",
    "href": "lab06.html#prologue",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Prologue",
    "text": "Prologue"
  },
  {
    "objectID": "lab06.html#data",
    "href": "lab06.html#data",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Data",
    "text": "Data\nSo far, we have worked on\n\nLab 2: Gene Expression Data, the gravier-data from A prognostic DNA signature for T1T2 node-negative breast cancer patients\nLab 3: Metagenomics, the pitlatrine-data from ‚ÄúAssessment of the influence of intrinsic environmental and geographical factors on the bacterial ecology of pit latrines‚Äù\nLab 4: Clinical data, the diabetes-data from Prevalence of coronary heart disease risk factors among rural blacks: a community-based study and A trial of church-based smoking cessation interventions for rural African Americans\nLab 5: High throughput data, the SARS-CoV-2-data A large-scale database of T-cell receptor beta (TCRŒ≤) sequences and binding associations from natural and synthetic exposure to SARS-CoV-2\n\nNow, inside your data directory in your RStudio Cloud project, create a new directory called _raw and using the RStudio G(raphical)U(ser)I(nterface), move any raw data into that directory.\n\nQ1: In your group, discuss, what is raw data?\n\nNow, make sure, that you have the raw versions of the 4 data sets.\n\nExample Work Flow: The Gravier Data\nAs inspiration, here a data work flow could look for the gravier-data\n\nGo to the GitHub site for the datamicroarray package\nClick the data-directory\nFind and click the gravier.RData-file\nIn the upper right corner, click the Download-button\nThis will download the gravier.RData-file to your local computer\nOnce again, start your rstudio.cloud and upload the gravier.RData-file to your _raw-directory\n\n\n# Load raw data from Lab 2\ngravier_raw <- read_rds(file = \"data/_raw/gravier.RData\")\n\n# Note: If this does not work, simply click the file\n# and \"Confirm Load data\" but then the data will be \"gravier\"\n\n# Clean data\ngravier_clean <- gravier_raw |>\n  bind_cols |>\n  as_tibble |>\n  relocate(y) |>\n  rename(outcome = y) |>\n  mutate(outcome = case_when(outcome == \"good\" ~ 0,\n                             outcome == \"poor\" ~ 1))\n\n# Write data\nwrite_tsv(gravier_clean,\n          file = \"data/01_gravier_clean.tsv.gz\")\n\n\nQ2: In your group, discuss, what each step of the above work flow does incl.¬†the specifics of the dplyr-pipeline and also why can bind_cols by very dangerous to use?\nQ3: What is the difference between using the file extensions *.tsv and *.tsv.gz?\nQ4: See if you can create a plain text file called gravier.README and information on how you retrieved the data - Discuss in your group, what is the purpose of this and in which directory would you place it?\n\nHint: Perhaps RStudio has the ability to create other file types than just .Rmd and .R?"
  },
  {
    "objectID": "lab06.html#modelling",
    "href": "lab06.html#modelling",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Modelling",
    "text": "Modelling\n\n\n\n\n\n\n\n\n\n\nPrimer\nFirst, the recent couple of years have seen an immense development in unifying the modelling interface in R, which is notoriously inconsistent. You may be familar with the caret-package, the developer of which has created tidymodels, which I really wish we had time to explore this in details. In the following we will work with some of the principles for tidying model object using broom, having object nested in tibbles and working with these using purrr.\n\n\nGetting started\nFor the data you have so nicely created, the outcome is either 0 or 1, meaning, that we will need a logistic regression to model the data. If you need a refresh on this, then go to this condensed primer, The general model syntax in R is as follows:\n\nmy_data |> \n  glm(y ~ x1 + x2 + ... + xn,\n      data = .,\n      family = binomial(link = \"logit\"))\n\nWhere y ~ x1 + x2 + ... xn is called a formula and is read like: ‚ÄúModelling \\(y\\) as a function of \\(x_1\\) plus \\(x_2\\) plus \\(...\\) plus \\(x_n\\)‚Äù.\n\n\nFirst steps‚Ä¶\nUsing the data you have created, choose your favourite gene, and make a model of the outcome modelled as a function of that one gene\n\nQ1: What are the coefficients for the intercept and your gene?\n\n\ngravier_data |>\n  glm(outcome ~ g2E09,\n      data = .,\n      family = binomial(link=\"logit\"))\n\n\nQ2: What is the p-value for your gene? Mine is:\n\n\ngravier_data |>\n  glm(outcome ~ g2E09,\n      data = .,\n      family = binomial(link=\"logit\")) |> \n  tidy |>\n  filter(term == \"g2E09\") |> \n  select(term, p.value)\n\n\n\nModels, models everywhere‚Ä¶\nNow, make one model for each of the ! In principle, you could do that, but that would require a lot code and a lot of hard coding. So do not do that for now, but let us instead see if we can come up with something just a tad more clever.\nCreate this long version of your data and save it in gravier_data_long:\n\ngravier_data_long = gravier_data |>\n  pivot_longer(cols = -outcome,\n               names_to = \"gene\",\n               values_to = \"log2_expr_level\")\ngravier_data_long\n\nUsing the group_by()-nest()-ungroup() work-flow, create this nested tibble:\n\ngravier_data_long_nested = gravier_data_long |>\n  group_by(gene) |>\n  nest |> \n  ungroup\ngravier_data_long_nested\n\nNote, this is conceptually a super-tricky data structure, so please do make sure to discuss in your group, what is going on here!\nThen use sample_n() to randomly select 100 genes for further analysis. Remember you may want to use the set.seed() function to create a reproducible random draw\n\nset.seed(934485)\ngravier_data_long_nested = gravier_data_long_nested |>\n  sample_n(100)\ngravier_data_long_nested\n\nNow, we want to fit a logisic regression model to each of the per-gene data sets in the nested tibble. We do this by using mutate(), map() and the ‚ÄúThe general model syntax‚Äù as defined previously. Remember you are mapping a model to each of the nested data sets, so you will need to include the variable names in your formula and also remember the receipe Hadley mentioned. This is quite tricky initially, so here is a bit of example code, to get you started. Here, we are fitting a standard ordinary least squares linear model on random data:\n\ntibble(x = rnorm(100),\n       y = rnorm(100),\n       g = sample(seq(1, 4),\n                  size = 100,\n                  replace = TRUE)) |>\n  group_by(g) |>\n  nest |>\n  ungroup |>\n  mutate(mu_group = map(data, ~lm(y ~ x, data = .x)))\n\nThink about this and as I have mentioned previously - Copy/paste bits and bops of the code into the console and play around with it until you understand what is going on.\nReturn to your gravier_data_long_nested-dataset and create the below, still saving it into your gravier_data_long_nested and remembering, that we here need a logistic regression\n\ngravier_data_long_nested = gravier_data_long_nested |>\n  mutate(mdl = map(data, ~glm(outcome ~ log2_expr_level,\n                              data = .x,\n                              family = binomial(link = \"logit\"))))\ngravier_data_long_nested\n\nNote, once again, this is conceptually a super-tricky data structure, not only do we have a nested tibble, but now we also have a nested model object - So please do make sure to discuss in your group, what is going on here!\nOk, now we have model for each of our 100 genes. Let us use the broom-package to extract some information from each of the models\nHint: broom has a very useful function, which helps ‚Äúcleaning up‚Äù model objects. Also, you may want to include conf.int = TRUE in your map()-call\n\ngravier_data_long_nested = gravier_data_long_nested |>\n  mutate(mdl_tidy = map(mdl, tidy, conf.int = TRUE)) |> # OR map(mdl, ~tidy(.x, conf.int = TRUE))\n  unnest(mdl_tidy)\ngravier_data_long_nested\n\nNow, we are only interested in the terms for the genes, so remove the (Intercept)-rows:\nHint: Check, that your dimensions should go from 200 to 100, corresponding to the 100 random genes you selected for further analysis!\n\ngravier_data_long_nested = gravier_data_long_nested |> \n  filter(str_detect(term, \"log2\"))\ngravier_data_long_nested\n\nAdd an indicator variable, denoing if a given term for a given gene is significant i.e.¬†\\(p < 0.05\\)\n\ngravier_data_long_nested = gravier_data_long_nested |> \n  mutate(identified_as = case_when(p.value < 0.05 ~ \"Significant\",\n                                   TRUE ~ \"Non-significant\"))\ngravier_data_long_nested\n\n\n\nVisualise Associations\nA Useful way of illustrating p-values for gene association is the so-called Manhattan-plot. For this we need to calculate the negative log10 of the p-values, so do that like so:\n\ngravier_data_long_nested = gravier_data_long_nested |> \n  mutate(neg_log10_p = -log10(p.value))\ngravier_data_long_nested\n\nNow, using your data visualisations skills, re-create this Manhattan-plot:\n\ngravier_data_long_nested |> \n  ggplot(aes(x = fct_reorder(gene, desc(neg_log10_p)),\n             y = neg_log10_p,\n             colour = identified_as)) + \n  geom_point() +\n  geom_hline(yintercept = -log10(0.05),\n             linetype = \"dashed\") +\n  theme_classic(base_family = \"Avenir\", base_size = 8) +\n  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1),\n        legend.position = \"bottom\") +\n  labs(x = \"Gene\", y = \"Minus log10(p)\")\n\nCongratulations! You have just done a what is equivalent of a GWAS! However, a more interesting visualisation is a confidence interval plot with effect directions, so let us create that and by us, I mean you\n\ngravier_data_long_nested |> \n  ggplot(aes(x = estimate,\n             y = fct_reorder(gene, desc(estimate)),\n             colour = identified_as)) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high, height = 0.2)) +\n  theme_classic(base_family = \"Avenir\", base_size = 8) +\n  theme(legend.position = \"bottom\") +\n  labs(y = \"\",\n       title = \"\",\n       caption = \"The 111 patients with no event after diagnosis were labelled good (0), and the 57 patients with early metastasis were labelled poor (1).\") \n\nIn the first plot, we coloured by whether or not a give gene had as signficant association - Notice something about how this colour maps to the this plot?\nNow, do the same plot, but now only including genes with a significant association:\n\ngravier_data_long_nested |> \n  filter(identified_as == \"Significant\") |> \n  ggplot(aes(x = estimate,\n             y = fct_reorder(gene, desc(estimate)))) +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high, height = 0.2)) +\n  theme_classic(base_family = \"Avenir\", base_size = 12) +\n  theme(legend.position = \"bottom\") +\n  labs(y = \"\",\n       title = \"\",\n       caption = \"The 111 patients with no event after diagnosis were labelled good (0), and the 57 patients with early metastasis were labelled poor (1).\") \n\nNote, we have not taken multiple testing into account. If you have the time, you could redo the above with adjust p-values. See ?p.adjust"
  },
  {
    "objectID": "lab06.html#pca",
    "href": "lab06.html#pca",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "PCA",
    "text": "PCA\nNow, let us play around with around with a little PCA, first add and run this little chunk, allowing us to continue our work on the 100 genes we previously selected:\n\ngravier_data_wide = gravier_data |>\n  select(outcome, pull(gravier_data_long_nested, gene))\n\nThen go and visit this blog post by Claus O. Wilke, Professor of Integrative Biology.\nYour task is to work through the blog post using your gravier_data_wide-dataset to crate a PCA-analysis GROUP ASSIGNMENT\nNOTE: THIS TIME, MAKE A MICRO-GROUP-REPORT AND HANDIN THE KNITTED PDF-FILE\nMy plots look like this, how about yours?\n\npca_fit = gravier_data_wide |> \n  select(-outcome) |> \n  prcomp(scale = TRUE)\n\n\npca_fit |>\n  augment(gravier_data_wide) |>\n  mutate(outcome = factor(outcome)) |> \n  ggplot(aes(x = .fittedPC1,\n             y = .fittedPC2,\n             colour = outcome)) + \n  geom_point(size = 1.5) +\n  theme_classic(base_family = \"Avenir\",\n                base_size = 12) +\n  theme(legend.position = \"bottom\")\n\n\n# define arrow style for plotting\narrow_style <- arrow(\n  angle = 20, ends = \"first\", type = \"closed\", length = grid::unit(8, \"pt\")\n)\n\n# plot rotation matrix\npca_fit |>\n  tidy(matrix = \"rotation\") |>\n  pivot_wider(names_from = \"PC\", names_prefix = \"PC\", values_from = \"value\") |>\n  ggplot(aes(PC1, PC2)) +\n  geom_segment(xend = 0, yend = 0, arrow = arrow_style) +\n  geom_text(\n    aes(label = column),\n    hjust = 1, nudge_x = -0.02, \n    color = \"#904C2F\"\n  ) +\n  #xlim(-1.25, .5) +\n  #ylim(-.5, 1) +\n  coord_fixed() + # fix aspect ratio to 1:1\n  theme_minimal(12)\n\n\npca_fit |>\n  tidy(matrix = \"eigenvalues\") |>\n  filter(PC <= 10) |> \n  ggplot(aes(PC, percent)) +\n  geom_col(fill = \"#56B4E9\", alpha = 0.8) +\n  scale_x_continuous(breaks = 1:9) +\n  scale_y_continuous(\n    labels = scales::percent_format(),\n    expand = expansion(mult = c(0, 0.01))\n  ) +\n  theme_minimal(12)"
  },
  {
    "objectID": "lab06.html#k-means",
    "href": "lab06.html#k-means",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "K-means",
    "text": "K-means\nAgain, use your gravier_data_wide and then go and visit this K-means clustering with tidy data principles post and work through the example."
  },
  {
    "objectID": "lab07.html",
    "href": "lab07.html",
    "title": "Lab 7: Scripting in a Reproducible and Collaborative Framework using GitHub via RStudio",
    "section": "",
    "text": "Package(s)\n\nusethis\ngit (Actually not an R-package this time)"
  },
  {
    "objectID": "lab07.html#schedule",
    "href": "lab07.html#schedule",
    "title": "Lab 7: Scripting in a Reproducible and Collaborative Framework using GitHub via RStudio",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap of midway evaluation and exercises from last lab\n08.15 - 08.30: Assignment walk-through\n08.30 - 09.00: Introduction to Lab\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab07.html#learning-materials",
    "href": "lab07.html#learning-materials",
    "title": "Lab 7: Scripting in a Reproducible and Collaborative Framework using GitHub via RStudio",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: Happy Git and GitHub for the useR ‚Äì Read chapters 1 (intro), 20 (basic terms), 22 (branching), 23 (remotes). Do not pay much attention to syntax of specific commands, because we are not going to use them during the exercises, focus on the idea.\nBook: Introduction to Data Science - Data Analysis and Prediction Algorithms with R by Rafael A. Irizarry: Chapter 40 Git and GitHub ‚Äì Some of the information here is redundant with the previous book, but very important thing is a visualization of basic git actions and screenshots of how to perform them using RStudio.\nVideo: RStudio and Git - an Overview (Part 1) ‚Äì Basic git concepts, for those who prefer listen rather than read. Books, however, contain more information.\nVideo: How to use Git and GitHub with R ‚Äì Basic operating on git in RStudio. Complementary to second book. You can skip to 2:50, we are not going to link to git manually either way."
  },
  {
    "objectID": "lab07.html#learning-objectives",
    "href": "lab07.html#learning-objectives",
    "title": "Lab 7: Scripting in a Reproducible and Collaborative Framework using GitHub via RStudio",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nExplain why reproducible data analysis is important\nIdentify associated relevant challenges\nExplain replicability versus reproducibility\nDescribe the components of a reproducible data analysis\nUse RStudio and GitHub (git) for collaborative bio data science projects"
  },
  {
    "objectID": "lab08.html#packages",
    "href": "lab08.html#packages",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Package(s)",
    "text": "Package(s)\n\ndevtools\nusethis\nroxygen2\ntestthat"
  },
  {
    "objectID": "lab08.html#schedule",
    "href": "lab08.html#schedule",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap of exercises from last lab\n08.15 - 08.30: Assignment walk-through\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab08.html#learning-materials",
    "href": "lab08.html#learning-materials",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nCheatsheet: Package development with devtools ‚Äì When in doubt, check here first\nWeb: R Packages: A Beginner‚Äôs Tutorial ‚Äì Read this before class\nWeb: Developing Packages with RStudio ‚Äì Creating an R Package with RStudio\nWeb: R package primer a minimal tutorial ‚Äì Brief but comprehensive R Package tutorial\nBook: R Packages - 1 Introduction ‚Äì Everything you need to know about R Packages"
  },
  {
    "objectID": "lab08.html#learning-objectives",
    "href": "lab08.html#learning-objectives",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nPrepare a simple R package for distributing documented functions\nExplain the terms Repository, Dependencies, and Namespace\nImplement testing in an R package\nCollaboratively work on an R package on GitHub"
  },
  {
    "objectID": "lab08.html#sec-exercises",
    "href": "lab08.html#sec-exercises",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Exercises",
    "text": "Exercises\nRead the steps of this exercises carefully while completing them\n\nIntroduction\nThe aim of these exercises is to set up a collaborative coding project using GitHub and collaborate on creating a simple R package that replicates the central dogma of molecular biology.\nThe exercises uses what you learned in Lab 7 and its exercises. But don‚Äôt worry too much, the setup steps will be given here as well.\n\n\n\nHow to work with an R package\nThere are a few things to know when creating a package before you jump in. These are not strict rules, but they make your life easier when bug-fixing and make the package much easier to use for the users. To learn about those, click the foldout sections in the following topics. If you feel confident in the realm of R packages, feel free to skip ahead, but don‚Äôt leave your group too far behind.\n\nDependencies\n\n\nThe one strict rule is Never use library(‚Äúpackage‚Äù) within a package! \n\nInstead, add the packages your are going to use to the DESCRIPTION file and in the function descriptions. This is done by running usethis::use_package(\"packageName\") in the console and adding @import package (OK) or @importFrom package function1 function2 ... (Best). Using the functions in your package is then done with package::function() (e.g., dplyr::mutate()) or omitting the package::.\nThis way, it is easy to read what functions are from your package, and your package namespace does not get cluttered. Read more in the Namespace section.\nIt should also be a goal to make your package depend on as few other packages as possible. The user will need to install all packages your package depends on, but then also every package those depends on - that list quickly become quite long if you are not careful.\n\n\n\nFunctions\n\n\nA package is typically a collections of functions.\n\nThese functions are stored in .R files in the R folder. A good starting point is to create an .R file for each function. But, as the package becomes bigger, it often makes sense to combine related functions into bigger files.\nYou can quickly create a new .R file with usethis::use_r(\"function_name\"). Or do it manually, as you are used to.\n\n\n\nDocumenting functions\nTry running ?mean in the Console.\n\n\nIf you have every wondered how to write a manual like the one that pops up, please click here and read - if not, consider reading it anyway, as you will use it later.\n\nWhen you have made a function, or have at least defined one, you should describe what it does and how to use it. The manual you write for your function is the function documentation, and it should describe the purpose of the function and how to use it.\nYou can read extensively about it here, but I will give you the most essential information to get you started.\nThe R package roxygen2 makes this easy as 1-2-3. It is part of devtools and is already installed. It uses #' comments above the function. @ tags lets you add specific details to the function documentation.\nCreate an roxygen skeleton by clicking somewhere in your function. Go to the ‚ÄòCode‚Äô tab in the top of your window and select ‚ÄòInsert Roxygen Skeleton‚Äô.\nThis will look something like this:\n\n#' Title\n#'\n#' @param foo \n#' @param bar \n#'\n#' @return\n#' @export\n#'\n#' @examples\nmyFunction <- function(foo, bar){\n  # Do stuff with foo and bar\n  foobar <- (foo * bar) / (foo + bar)\n  return(foobar)\n}\n\nThis allows you to add the most basic descriptions. To begin with, the Title, @param, and @export are the most important, you may remove the other tags for now. A more detailed example is given here. There, you can also read about documenting datasets and other object types - even the package itself.\n\n\n\nNamespace\n\n\nYour package namespace can quickly become very cluttered, if you are not careful.\n\nTherefore, follow these rules:\n\nOnly @export the functions the users will use. Removing the tag makes the function internal and hides it from your package namespace. It can still be freely used within your package and accessed outside your package with package:::internal_function()\nMake your code explicit with package::function().\n\nThis step is not mandatory, but makes reading the code easier.\n\nAdd your dependencies in the DESCRIPTION file with usethis::use_package(\"packageName\")\nOnly very rarely use the @import tag. Aim to use the @importFrom tags in your function descriptions instead.\n\nYou can read more extensively about namespace here.\n\n\n\nTesting\n\n\nTesting is essential\n\nto ensure your package runs smoothly and that no bugs are introduced when you make a seemingly minor change. It is handled with the testthat package, which is also installed with devtools.\nI will not go into too much detail here, but know that testing is an important, but often neglected, part of building a package. You can read more about it here.\nEvery time you run the usethis::use_r() function to create a new script, the function encourages you to create a test alongside the new function. I recommend you follow that advise.\nYou create a test by running usethis::use_test(\"function name\").\nThe function creates a new folder tests and creates a test script for the function. The good R package creator writes a few tests for every function.\nThe exercises will ask you to make a simple test for every function, introducing you to the concept.\n\n\n\nThe Package Workflow\nWhen creating a package, it is important to test your work along the way.\n\n\nYou can do that in many ways, but I recommend the following workflow:\n\n\nWrite a function / make a change\n\nIf it is a new function, document it\n\nSave your files: rstudioapi::documentSaveAll()\nCreate package documentation: devtools::document()\nLoad package: devtools::load_all()\nYour package is now loaded, and you can test that it works as intended.\n\nOptionally, you can save the three lines of code in dev/load.R and run the lines with source(\"dev/load.R\"). If you do, add the dev folder to the .Rbuildignore file.\n\n\n\n\n\nTASKS\nNow you know the basics of creating a package. And you are ready to begin.\n\nTask 1 - Setting up the R package\nIn the first task, one team member (you decide who) will initiate a package project and push it to Github. Then, when instructed, the remaining group members will connect to that repository, and the real package building will begin.\n\nTask 1.1 - Create R Package\n\nGo to R for Bio Data Science RStudio Cloud Server and login.\nClick Create a project in the top left and choose New Directory.\nSelect R Package and pick a fitting Package name.\n\nThe package you will create will replicate the central dogma of molecular biology. Let that inspire you\nLook up naming rules here\nThe most important rule: _, -, and ' are not allowed.\n\nClick ‚ÄúCreate project‚Äù.\n\nRStudio will now create an R project for you that contains all necessary files and folders.\n\nOpen the DESCRIPTION file and write a title for your package, a small description, and add authors.\n\n\nWhen adding authors, the format is:\n\n\nAuthor@R:\n  c(person(given = \"firstname\",\n           family = \"lastname\",\n           role = c(\"aut\", \"cre\"), # There must be a \"cre\", but there can only be one\n           email = \"your@email.com\"), \n    person(given = \"firstname\",\n           family = \"lastname\",\n           role = \"aut\",\n           email = \"your@email.com\"))\n\n\nCreate an MIT license usethis::use_mit_license(\"Group name\")\n\n\nOptional setup steps (generally good, but not essential for this course)\n\n\n\nIf the ‚ÄúGit‚Äù tab is not in the lower left panel:\n\n\n\nRun usethis::use_git() and reopen the project to have it appear\n\n\nTo use GitHub, you create an empty repository there and link the account with gert::git_remote_add(\"link\") and gert::git_push(\"origin\")\n\n\ngit push will now push your committed changes to the GitHub repository\n\n\n\nAdd a lifecycle badge: usethis::use_lifecycle_badge( \"Experimental\" )\n\n\nWrite vignettes: usethis::use_vignette(\"Vignette name\")\n\n\nAdd a Readme file usethis::use_readme_rmd( open = FALSE ). You will do that in the Group Assignment later\n\n\n\n\n\n\nTask 1.2 - Setup GitHub Repository for your group‚Äôs R package\nStill only the first team member\n\nGo to https://github.com/rforbiodatascience22\nClick the green New-button\nCreate a new repository called group_X_package. Remember to replace the X\nSelect rforbiodatascience22 as owner\nMake the repository Public\nClick the green Create repository-button\nIn this new repository, click the settings tab\nClick Collaborators and Team\nClick the green Add people-button\nInvite the remaining group members\nAll other team members should now have access to the repository, but do not create your own project yet.\n\n\n\nTask 1.3 - Connect your RStudio Server Project to the GitHub Repository\nStill only the first team member\n\nFind your PAT key or create a new\n\n\nHow to create a new one\n\n\n\nType in the console usethis::create_github_token(). You‚Äôre going to be redirected to GitHub website.\n\nIn case you did that step manually, remember to give permission to access your repositories.\n\nYou need to type your password. Don‚Äôt change the default settings of creating the token except for the description ‚Äì set ‚ÄòRStudio Cloud‚Äô or something similar. Then hit Generate token.\n\nCopy the generated token (should start with ghp_) and store it securely (e.g., in a password manager). Do not keep it in a plain file.\n\n\nGo back to the RStudio Server project\nType in the console gitcreds::gitcreds_set() and paste the PAT key\nStage all files (in the git window in the top right) by ticking all boxes under Staged\nStill in the console, run the following commands. Replace your group number and use your GitHub username and email. You run these to link your project to the GitHub repository you created. Remember to replace the X.\n\n\nusethis::use_git_remote(name = \"origin\", \"https://github.com/rforbiodatascience22/group_X_package.git\", overwrite = TRUE)\nusethis::use_git_config(user.name = \"USERNAME\", user.email = \"USEREMAIL@EXAMPLE.ORG\")\ngert::git_commit_all(\"Package setup\")\ngert::git_push(\"origin\")\n\nAll other team members\nAfter your teammate has pushed to GitHub.\n\nIn the RStudio Server, create a new project based on the GitHub Repository just created by your teammate.\n\nClick Create a project in the top left.\nChoose Version Control and then Git.\nPaste in the repository URL: \"https://github.com/rforbiodatascience22/group_X_package.git\". Remember to replace the X.\nGive the directory a fitting name and click Create Project.\n\nFind your PAT key or create a new\n\n\nHow to create a new one\n\n\n\nType in the console usethis::create_github_token(). You‚Äôre going to be redirected to GitHub website.\n\nIn case you did it manually, remember to give permission to access your repositories.\n\nYou need to type your password. Don‚Äôt change the default settings of creating the token except for the description ‚Äì set ‚ÄòRStudio Cloud‚Äô or something similar. Then hit Generate token.\n\nCopy the generated token (should start with ghp_) and store it securely (e.g.¬†in password manager). Do not keep it in a plain file.\n\n\nGo back to the RStudio Server project\nType in the console gitcreds::gitcreds_set() and paste the PAT key\nStill in the console, run the following commands. Replace your GitHub username and email.\n\n\nusethis::use_git_config(user.name = \"USERNAME\", user.email = \"USEREMAIL@EXAMPLE.ORG\")\n\nIf RStudio at any point asks you to log in to GitHub, redo step 4 and 5.\nNow you are ready to work on the R package.\n\n\n\nTask 2 - Build the package\n\nEach team member should build and implement their own function. The code will be given, but you will be asked to come up with a name for your function and many of the variables, so discuss in the team what naming convention you want to use. Do you want to use snake_case (common in tidyverse) or camelCase (common in Bioconductor)? It doesn‚Äôt really matter, but try to be consistent.\n\n\nTask 2.1 - Incorporate data\nIn this task you will include the following codon table in your package.\nThis task should be done by one team member. The rest should follow along.\n\nBelow is a standard codon table. Store it in an object with a name of your own choosing.\n\n\nc(\"UUU\" = \"F\", \"UCU\" = \"S\", \"UAU\" = \"Y\", \"UGU\" = \"C\",\n  \"UUC\" = \"F\", \"UCC\" = \"S\", \"UAC\" = \"Y\", \"UGC\" = \"C\",\n  \"UUA\" = \"L\", \"UCA\" = \"S\", \"UAA\" = \"_\", \"UGA\" = \"_\",\n  \"UUG\" = \"L\", \"UCG\" = \"S\", \"UAG\" = \"_\", \"UGG\" = \"W\",\n  \"CUU\" = \"L\", \"CCU\" = \"P\", \"CAU\" = \"H\", \"CGU\" = \"R\",\n  \"CUC\" = \"L\", \"CCC\" = \"P\", \"CAC\" = \"H\", \"CGC\" = \"R\",\n  \"CUA\" = \"L\", \"CCA\" = \"P\", \"CAA\" = \"Q\", \"CGA\" = \"R\",\n  \"CUG\" = \"L\", \"CCG\" = \"P\", \"CAG\" = \"Q\", \"CGG\" = \"R\",\n  \"AUU\" = \"I\", \"ACU\" = \"T\", \"AAU\" = \"N\", \"AGU\" = \"S\",\n  \"AUC\" = \"I\", \"ACC\" = \"T\", \"AAC\" = \"N\", \"AGC\" = \"S\",\n  \"AUA\" = \"I\", \"ACA\" = \"T\", \"AAA\" = \"K\", \"AGA\" = \"R\",\n  \"AUG\" = \"M\", \"ACG\" = \"T\", \"AAG\" = \"K\", \"AGG\" = \"R\",\n  \"GUU\" = \"V\", \"GCU\" = \"A\", \"GAU\" = \"D\", \"GGU\" = \"G\",\n  \"GUC\" = \"V\", \"GCC\" = \"A\", \"GAC\" = \"D\", \"GGC\" = \"G\",\n  \"GUA\" = \"V\", \"GCA\" = \"A\", \"GAA\" = \"E\", \"GGA\" = \"G\",\n  \"GUG\" = \"V\", \"GCG\" = \"A\", \"GAG\" = \"E\", \"GGG\" = \"G\")\n\n\nRun usethis::use_data(name_of_object, overwrite = TRUE, internal = TRUE)\nYou have now made the data available to our functions, but we also want to make it visible for our users.\n\nRun usethis::use_data(name_of_object, overwrite = TRUE).\n\nWrite a data manual (document the data).\n\nAll non-internal data should be documented in a data.R file in the R folder. Create it with usethis::use_r(\"data\")\nAdd the following scaffold and write a very brief description of the data (see an example here). Don‚Äôt spend a lot of time here.\n\n\n\n#' Title\n#' \n#' Description\n#' \n#' \n#' @source \\url{https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi?chapter=tgencodes#SG1}\n\"name_of_object\"\n\nNormally, you should also describe how the raw data was cleaned. You would do that in the file that opens after running usethis::use_data_raw( name = \"name_of_object\", open = TRUE ), but that is less relevant here, so we will skip that part.\nYour package now includes some data.\nRestart R, clean your Environment (small broom in the Environment tab), run the three lines of code from The Package Workflow section, and run ?name_of_object. Your manual should pop up. Try printing the object as well to see what it looks like print(name_of_object).\nPush your changes to GitHub. The other team members pull the changes to have the data available to you as well.\n\n\n\nTask 2.2 - Implement functions\nIn this task you will be working individually to implement a function each. If you are fewer in the team than the number of functions. The quickest to finish can implement the remaining, or separate them out as you see fit.\nIf you want an additional challenge, each team member can create their own Git branch gert::git_branch_checkout(\"branch_name\") and work there. When done, merge your branch with the main branch. That is a more clean and ‚Äòproper‚Äô workflow, but completely optional.\nIf you are in doubt what the underlying functions do, run ?function_name to get a hint about their purpose. Also, remember the functions are replicating the central dogma. Let that inspire you, when naming the functions and variables. If you get stuck, ask your teammates about their functions.\nFunction five is a bit more involved. Do it together or help your teammate out if it causes problems.\nFor each function, complete the following steps:\n\nLook carefully at your function. Choose a fitting name for it\nRun usethis::use_r(\"function_name\") to create an .R file for the function\nPaste in the function and rename all places it says name_me with fitting names\nClick somewhere in the function. In the toolbar at the very top of the page, choose Code and click Insert Roxygen Skeleton\nFill out the function documentation\n\nGive the function a title\nDo not write anything after @export\nThe parameters should have the format: @param param_name description of parameter\nThe return has the format: @return description of function output\nExamples can span multiple lines and what you write will be run.\nImportant! Either fill out everything or delete what you don‚Äôt describe. Otherwise, the package check will fail (Do not delete @export for these functions).\n\nRun the three lines of codes from The Package Workflow section\n\n\nrstudioapi::documentSaveAll()  # Saves all you files\ndevtools::document()  # Writes all your manuals for you\ndevtools::load_all()  # Simulates library(\"your package\"), allowing you to use your functions\n\n\nView your function documentation with ?function_name\nDefining a test or a series of tests around your newly created function ensures future corrections will not yield undesired results. Create such a test for your function. Run usethis::use_test(\"function_name\") and write a test. Draw some inspiration from here or from running ?testthat::expect_equal.\n\nSkip this step for function five. Instead, write inline code comments for each chunk of code. Press Cmd+Shift+c / Ctrl+Shift+C to comment your currently selected line.\n\nRerun the three lines from The Package Workflow section and check that the package works with devtools::check()\n\n\n\nBriefly about check\n\ndevtools::check() will check every aspect of your package and run the tests you created. You will likely get some warnings or notes like ‚ÄòNo visible binding for global variables‚Äô. They are often harmless, but, if you like, you can get rid of them as described here. The check will tell you what might cause problems with your package and often also how to fix it. If there are any errors, fix those. Warnings and notes are also good to address. Feel free to do that if any pops up.\n\n\nIf it succeeds without errors, push your changes to GitHub\n\nUse the RStudio GUI if you prefer\nRemember to pull first\nIf you chose to create your own branch, merge it with master/main.\n\n\n\n\nFunction one\n\n\nname_me1 <- function(name_me2){\n  name_me3 <- sample(c(\"A\", \"T\", \"G\", \"C\"), size = name_me2, replace = TRUE)\n  name_me4 <- paste0(name_me3, collapse = \"\")\n  return(name_me4)\n}\n\n\n\n\nFunction two\n\n\nname_me1 <- function(name_me2){\n  name_me3 <- gsub(\"T\", \"U\", name_me2)\n  return(name_me3)\n}\n\n\n\n\nFunction three\n\n\nname_me1 <- function(name_me2, start = 1){\n  name_me3 <- nchar(name_me2)\n  codons <- substring(name_me2,\n                      first = seq(from = start, to = name_me3-3+1, by = 3),\n                      last = seq(from = 3+start-1, to = name_me3, by = 3))\n  return(codons)\n}\n\n\n\n\nFunction four\n\nname_of_your_object refers to the codon table you stored in Task 2.\n\nname_me <- function(codons){\n  name_me2 <- paste0(name_of_your_object[codons], collapse = \"\")\n  return(name_me2)\n}\n\n\n\n\nFunction five\n\nThis function will be the first to use dependencies. As a reminder, a dependency is a package that your package depends on. In this case, it will be magrittr, stringr, and ggplot2. They are already installed, so you don‚Äôt need to do that. The best way to add these packages to your own is to first add them to your package dependencies with usethis::use_package(\"package_name\"). Do that with all three.\nFor magrittr it is a good idea to add the Tidyverse pipe %>% to your package namespace. When writing your function documentation, add a line with @importFrom magrittr %>%. For the ggplot2 functions, we will use ggplot2::function_name everywhere a ggplot2 function is used. Also add @import ggplot2 to the function description. The same applies for stringr, but since we only use a few functions, add @importFrom stringr str_split boundary str_count to the function description.\n\nname_me1 <- function(name_me2){\n  name_me3 <- name_me2 |>  \n    stringr::str_split(pattern = stringr::boundary(\"character\"), simplify = TRUE) %>%\n    as.character() |> \n    unique()\n  \n  counts <- sapply(name_me3, function(amino_acid) stringr::str_count(string = name_me2, pattern =  amino_acid)) |>  \n    as.data.frame()\n  \n  colnames(counts) <- c(\"Counts\")\n  counts[[\"Name_me2\"]] <- rownames(counts)\n  \n  name_me4 <- counts |>  \n    ggplot2::ggplot(ggplot2::aes(x = Name_me2, y = Counts, fill = Name_me2)) +\n    ggplot2::geom_col() +\n    ggplot2::theme_bw() +\n    ggplot2::theme(legend.position = \"none\")\n  \n  return(name_me4)\n}\n\n\n\n\n\n\nTask 3 - Group discussion\n\nDescribe each function to each other in order - both what it does and which names you gave them and their variables.\nThe person(s) responsible for function five, describe how you added the three packages as dependencies and included the pipe in the package namespace.\nDiscuss why it is a good idea to limit the number of dependencies your package has. When can‚Äôt it be avoided?\nDiscuss the difference between adding an @importFrom package function tag to a function description compared to using package::function(). Read this section if you are not sure or just want to learn more.\n\n\n\n\nGROUP ASSIGNMENT\n\nFor this week‚Äôs group assignment, write a vignette (user guide) for your package (max 2 pages). The vignette should include a brief description of what the package is about and demonstrate how each function in the package is used (individually and in conjunction with each other). As a final section, discuss use cases for the package and what other functions could be included. Also include the main points from your discussion in Task 3.\nInclude a link to your group‚Äôs GitHub repository at the top of the vignette. Hand it in as a pdf in DTU Learn, as you did the previous weeks.\n\nCreate a vignette with usethis::use_vignette(\"package name\").\nWhen you are done writing it, run devtools::build_vignettes().\nAt the top line of your vignette, change rmarkdown::html_vignette to rmarkdown::pdf_document\nRerun devtools::build_vignettes() - the created pdf-file in the doc-folder is the document to hand it.\nLastly, duplicate the vignette as the GitHub README\n\nCreate a README with usethis::use_readme_rmd( open = TRUE ).\nCopy the content of the vignette Rmarkdown into the readme Keep the top part of the README as is. If you overwrote it anyway, change rmarkdown::pdf_document to rmarkdown::github_document.\nRun devtools::build_readme()\nPush the changes to GitHub\n\n\n\n\nLast tip on packages\n\nNext week, I will introduce the golem package for building production-grade Shiny apps. However, I personally also use it to quickly get going with packages.\nWhen starting out with an R package, it may seem complicated with a lot of things to remember. golem remembers these things for you. When setting up your package / Shiny app with golem::create_golem(\"Your awesome package/app name\"), it creates a dev folder with a few files listing all you need to get started with an R package.\nIt does also give you a lot of other things that you will rarely use, and it also sets up some basic structures for Shiny apps. These can simply be deleted if you are not also building a Shiny app."
  },
  {
    "objectID": "lab09.html#packages",
    "href": "lab09.html#packages",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Package(s)",
    "text": "Package(s)\n\nshiny\ngolem"
  },
  {
    "objectID": "lab09.html#schedule",
    "href": "lab09.html#schedule",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap of exercises from last lab\n08.15 - 08.30: Assignment walk-through\n08.30 - 09.00: Slides\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab09.html#learning-materials",
    "href": "lab09.html#learning-materials",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: Mastering Shiny by Hadley Wickham ‚Äì Read Chapter 1 (Chapter 2 and 3 are good to read as well, if you want).\nBook: Engineering Production-Grade Shiny Apps ‚Äì Read Chapter 2 - 5 (they are fairly short, but if you don‚Äôt find Shiny Apps super cool, feel free to skip Chapter 3 and 5 and Sections 2.2.2 and 4.2.3 - 4.2.4), the rest are quite important for the exercises.\nCheatsheet: Shiny ‚Äì This cheatsheet is a bit cluttered, but useful\nCheatsheet: Golem ‚Äì Look through this after reading the chapters in ‚ÄúEngineering Production-Grade Shiny Apps‚Äù - the exercises will remind you to look at the cheatsheet as well.\n\nNote: The following are suggested learning materials, i.e., do not go over everything, but poke around. You will use these materials as a point of reference for the group exercises\n\nShiny Input Gallery\nWeb: Shiny from RStudio\nWeb: RStudio tutorials on Shiny\nVideo: Playlist: Web Apps in R: Building your First Web Application in R | Shiny Tutorial\nExample: nnvizRt\nMore inspiration: Shiny Gallery"
  },
  {
    "objectID": "lab09.html#learning-objectives",
    "href": "lab09.html#learning-objectives",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nPrepare a simple shiny application\nUsing relevant online ressources to autonomously identify and obtain new and expand on existing knowledge of R"
  },
  {
    "objectID": "lab10.html#schedule",
    "href": "lab10.html#schedule",
    "title": "Lab 10 Project Startup & Industry Talks",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap and Assignment\n08.15 - 08.45: Introduction to Project Period and Exam\n08.45 - 09.00: Break\n09.00 - 10.00: Mini Symposium (See below)"
  },
  {
    "objectID": "lab10.html#mini-symposium-applications-of-r-for-bio-data-science-in-industry",
    "href": "lab10.html#mini-symposium-applications-of-r-for-bio-data-science-in-industry",
    "title": "Lab 10 Project Startup & Industry Talks",
    "section": "Mini Symposium: Applications of R for Bio Data Science in Industry",
    "text": "Mini Symposium: Applications of R for Bio Data Science in Industry\nThis mini symposium on Applications of R for Bio Data Science in Industry aims to give students a glimpse into how modern data science is transforming value creation in the pharmaceutical industry and healthcare sector.\n\n2023\n\nCompanies Represented\n\nTBA\n\n\n\nSpeaker List\n\nTBA\n\n\n\n\n2022\n\nCompanies Represented\n\nAbzu\nBristol Myers Squibb\nClinical Microbiomics\nNovo Nordisk Bioinformatics\nNovo Nordisk Biostatistics\nSteno Diabetes Center Copenhagen\n\n\n\nSpeaker List\n\nAndrea Marquard, Head of Data Science and Automation, Clinical Microbiomics: ‚ÄúHow we use internal R packages, and why you should learn to make one‚Äù\nAnders Gorst-Rasmussen, Statistical Director, Novo Nordisk Biostatistics, Novo Nordisk: ‚ÄúHow we use R in Novo Nordisk Biostatistics‚Äù\nAnne-Mette Bjerregaard, Senior Scientist in Computational Biology, Novo Nordisk: ‚ÄúThe Evolution of a Computational Biologist‚Äù\nSam Demharter, Bioinformatics Specialist, Abzu: ‚ÄúExplainable AI in Precision Medicine - Closing the Loop from Patient to Drug‚Äù\nLykke Pedersen, Head of RNA Therapeutics, Abzu: ‚ÄúExplainable AI in Precision Medicine - Closing the Loop from Patient to Drug‚Äù\nSimon Papillon-Cavanagh, Principal Scientist, Bristol Myers Squibb (USA): ‚ÄúA Career Built on Mistakes‚Äù\nSofie Olund Villumsen, Bioinformatician, Steno Diabetes Center Copenhagen: ‚ÄúUsing R for Data Analysis in Clinical Studies‚Äù\nTarun Veer Singh Ahluwalia, Assoc. Prof., Steno Diabetes Center Copenhagen: ‚ÄúUsing R for Data Analysis in Clinical Studies‚Äù\n\n\n\n\n2021\n\nCompanies Represented\n\nAbzu\nALK\nChr. Hansen\nLundbeck\nNovozymes\nTeraData\n\n\n\nSpeaker List\n\nSamuel Demharter, Senior Bioinformatician, Abzu\nLykke Pedersen, Senior Bioinformatician, Abzu\nMarie-Catherine Le Bihan, Senior Data Scientist, Chr. Hansen\nThomas Strantzl, Head of Bioinformatics, ALK\nMaria Dalby, Postdoctoral Researcher, Lundbeck\nThomas Poulsen, Science Manager, Novozymes\nMikkel Freltoft Krogsholm, Team lead, Senior Data Scientist, TeraData"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Various resources pertaining to the course"
  },
  {
    "objectID": "paths_and_projects.html",
    "href": "paths_and_projects.html",
    "title": "Paths & Projects",
    "section": "",
    "text": "You step onto the road, and if you don‚Äôt keep your feet, there‚Äôs no knowing where you might be swept off to\nIn context of R, a path is a way to tell R, where to look for a file. First, let us familiarise us a bit with paths in RStudio."
  },
  {
    "objectID": "paths_and_projects.html#getting-familiar-with-paths-in-rstudio",
    "href": "paths_and_projects.html#getting-familiar-with-paths-in-rstudio",
    "title": "Paths & Projects",
    "section": "Getting Familiar with Paths in RStudio",
    "text": "Getting Familiar with Paths in RStudio\nLog on to the RStudio Cloud Server and in case you do so for the first time, your files-pane should look something like:\n Here,  Home defines your home, that is where you ‚Äúlive‚Äù on the server. Now, click the button  New Folder and create a new folder called projects. Hereafter, your files-pane should look something like:\n\nNow, click the projects folder you created and then you should see:\n\nNote how the Home now is extended to  Home > projects. This signifies, that you are looking at the projects folder in your home.\nTry to click on the 2 dots in .. (the green arrow won‚Äôt do it, so hit those dots!), this will take you one level up, so you again see:\n\nNote, that you are now back in  Home"
  },
  {
    "objectID": "paths_and_projects.html#intermezzo-creating-a-project",
    "href": "paths_and_projects.html#intermezzo-creating-a-project",
    "title": "Paths & Projects",
    "section": "Intermezzo: Creating a Project",
    "text": "Intermezzo: Creating a Project\nOk, so far so good. Now again click into projects and click  Project: (None) and from here select  New Project.... Now you will se a dialogue window open, i.e.¬†RStudio requires input from you:\n\nClick  New Directory and you should see:\n\nClick  New Project:\n\nIn the Directory name:, enter e.g.¬†r_for_bio_data_science and note how we are creating the folder (In this case Directory name, which is equivalent) as a sub-folder of projects. The funny wavy symbol followed by a slash ~/ is simply a short hand for ‚Äúin this users home folder‚Äù. So let us proceed:\n\nand then click Create Project. Now, depending on your choice of directory name, you should see:\n\nBriefly, the created r_for_bio_data_science.Rproj file contains the settings for your project. You can verify this, by clicking on the file and you should see:\n\nWe will leave this as is for now, so go ahead and click OK.\nNow, back to folders and paths - note how you now see  Home > projects > r_for_bio_data_science, this means that you are now in the r_for_bio_data_science folder, which is inside the projects folder which in turn is inside the Home folder. If you take a look at the  Home > projects > r_for_bio_data_science, you can in fact also click directly on e.g.¬†projects - Try it!\nBut why? Don‚Äôt worry, we will return to why RStudio Projects are indispensable when working with reproducible Bio Data Science"
  },
  {
    "objectID": "paths_and_projects.html#locating-data",
    "href": "paths_and_projects.html#locating-data",
    "title": "Paths & Projects",
    "section": "Locating Data",
    "text": "Locating Data\nLet‚Äôs move on. Now, again click the  New Folder-button and create a data-folder, make sure it ends up in the r_for_bio_data_science-folder. This should result in:\n\nNote here how we now have  data and  r_for_bio_data_science.Rproj. These are different, one is a folder, data, and the other is a file, r_for_bio_data_science.Rproj, containing settings for your RStudio Project.\nLet us try to put som data into the data-folder. In the console window, run the following command\n\nwrite.table(x = datasets::Puromycin, file = \"data/puromycin.tsv\", sep = \"\\t\")\n\nThis should look like so:\n\nThis command write a table containing the Puromycin from an R-package named datasets, this is done using the x-parameter. The next paramter is file and we set that to indicate, that the file should go into the data-folder we created and that we would like the file to be named puromycin.tsv, where tsv is an abbreviation for tab-separated-values and then the last parameter sep is set to \"\\t\" indicating, that we want the values to be separated by a tab, as indicated, when we named the file.\nOnce you have run the command, click the data-folder and you should now see:\n\nAgain, click .., this will take you one level up, so you again see:\n\nNow, we have a data file called puromycin.tsv. Let us read that file into R. We can do that like so:\n\nmy_data <- read.table(file = \"puromycin.tsv\")\n\nEnter the command into the console and run it like we did before. You will now see the following:\n\nSo, what happend? The blue writing is your command and the red is an error message from R. Always read error messages carefully, they will inform you what went wrong. In this case, we can see that cannot open file 'puromycin.tsv': No such file or directory.\nThis happened because we forgot to specify where the puromycin.tsv-file is located. R is very picky here, you have to specify exactly where R should find things. Recall, that we decided to create a data-folder and that we placed the puromycin.tsv-file into that folder. This we have to specify, when we use the file parameter in the read.table()-function. So, let us fix that:\n\nmy_data <- read.table(file = \"data/puromycin.tsv\")\n\nThis data/puromycin.tsv is the path to the file and now, that we have got that correct, you will see no error message and furthermore, you will see in the environment-pane, that we now have an object called my_data, containing 23 obs. of 3 varibles, i.e.¬†a data set with 23 rows and 3 columns."
  },
  {
    "objectID": "paths_and_projects.html#absolute-versus-relative-paths",
    "href": "paths_and_projects.html#absolute-versus-relative-paths",
    "title": "Paths & Projects",
    "section": "Absolute versus Relative Paths",
    "text": "Absolute versus Relative Paths\nLet us get back to why we have to work using RStudio Projects, recall we created the r_for_bio_data_science.Rproj-file, defining out project. You can verify, that we are indeed working in that project, by looking in the upper right corner of the RStudio IDE and you should see  r_for_bio_data_science.\nGood, now in the console, enter the command:\n\ngetwd()\n\nYou should see something along the lines of:\n\n\"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science\"\n\nSo, when we read the puromycin.tsv-file using the path data/puromycin.tsv, we specify, that R should look for the file puromycin.tsv in the data folder. So why did we not have to specify /net/pupilx/home/people/student_id/projects/r_for_bio_data_science? Well indeed, we could have specified the full location of the puromycin.tsv-file, which would be:\n\n\"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science/data/puromycin.tsv\"\n\nThis is called the absolute path and here you should note, that it begins with a /. But let us say, that we had indeed in our code stated:\n\nmy_data <- read.table(file = \"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science/data/puromycin.tsv\")\n\nThen that would work‚Ä¶ On OUR computer. If we were to share our code to a colleague or a collaborator, then that would not work, because that person would have a different path, e.g.¬†a different student_id. The code would break! Imagine that you have thousands of line of code with hundreds of absolute paths - You would spend hours-and-hours on fixing all the absolute paths, so they matched that particular computer. Then every time we would want to share the analysis project, we would have to redo this tedious proces!\nThis is why we work in RStudio Projects! RStudio Projects allows us to specifiy where everything is located relative to where the .Rproj-file is. So in our case, the r_for_bio_data_science.Rproj-file is located in the same place as the data-folder, namely in the folder containing our entire project, the r_for_bio_data_science-folder, which in turn is located in the projects-folder.\nThis means, that all paths in the analysis project, can be stated relative to the location of the .Rproj-file and hence we have relative paths, meaning that anyone can receive the project and run it straight-out-of-the-box!"
  },
  {
    "objectID": "paths_and_projects.html#working-in-multiple-projects",
    "href": "paths_and_projects.html#working-in-multiple-projects",
    "title": "Paths & Projects",
    "section": "Working in Multiple Projects",
    "text": "Working in Multiple Projects\nNow, we did add that plural s, when we created the projects-folder. When you have completed this course, perhaps you want to attend the ‚ÄúIntroduction to Systems Biology‚Äù-course. In that case, we would setup a new project, so use the Files-pane to navigate to the projects-folder:\n\nThen, we simply repeat the proces: Click  r_for_bio_data_science in the upper right corner and from here select  New Project.... Now you will se a dialogue window open, i.e.¬†RStudio requires input from you:\n\nClick  New Directory and you should see:\n\nClick  New Project:\n\nIn the Directory name:, enter e.g.¬†introduction_to_systems_biology:\n\nand then click Create Project. Now, you should see:\n\nNow, note how you now see  Home > projects > introduction_to_systems_biology, meaning that you are now in your Home and then in your folder containing projects, one of which is your introduction_to_systems_biology project.\nClick .. and you will see:\n\nThis is now your two project folders and you can add others, such as yet another course or e.g.¬†special_course, bachelor_thesis or master_thesis.\nNow, we can easily switch between different projects. In the upper right corner you will see, that you are currently in the introduction_to_systems_biology project, meaning that R will look for all files relative to the introduction_to_systems_biology.Rproj-file. Naturally, we would want to switch back to the project, we created for the ‚ÄúR for Bio Data Science‚Äù-course. To do this, we simply click  introduction_to_systems_biology and if you look at the drop-down menu, you should see r_for_bio_data_science - Click it! Notice how R automatically restarts and you are moved to the correct folder for this project."
  },
  {
    "objectID": "paths_and_projects.html#learning-objectives",
    "href": "paths_and_projects.html#learning-objectives",
    "title": "Paths & Projects",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIf you made it this far, you should now be able to:\n\nNavigate the RStudio IDE in context of folders and projects\nCreate a new project\nCreate a new folder\nRead and write data files from relative paths\nWork in and switch between different projects\nExplain the difference between a folder, a data file and a Rproj-file\nExplain the difference between absolute and relative paths\nExplain why RStudio Projects are an essential part of doing reproducible bio data science"
  },
  {
    "objectID": "paths_and_projects.html#epilogue",
    "href": "paths_and_projects.html#epilogue",
    "title": "Paths & Projects",
    "section": "Epilogue",
    "text": "Epilogue\nThat‚Äôs all folks! I hope this cleared up some things - Please feel free to revisit this chapter, as needed!\nRemember - Have fun! No one ever got really good at something they didn‚Äôt think was fun!"
  },
  {
    "objectID": "variable_assignment_in_r.html",
    "href": "variable_assignment_in_r.html",
    "title": "Variable Assignment in R",
    "section": "",
    "text": "In R we operate with variables. A variable can be seen as a container for a value. To get a better conceptual understanding of this, you can go through the following and code-along in your own R-session."
  },
  {
    "objectID": "variable_assignment_in_r.html#assigning-a-value-to-a-variable",
    "href": "variable_assignment_in_r.html#assigning-a-value-to-a-variable",
    "title": "Variable Assignment in R",
    "section": "Assigning a Value to a Variable",
    "text": "Assigning a Value to a Variable\n\nIn R, we state values directly in the chunk or the console, e.g.:\n\n\n3\n\n[1] 3\n\n\n\nHere, we just state 3, so R simply ‚Äúthrows‚Äù that right back at you!\nNow, if want to ‚Äúcatch‚Äù that 3 we have to assign it to a variable, e.g.:\n\n\nx <- 3\n\n\nNotice how now we ‚Äúcatch‚Äù the 3 and nothing is ‚Äúthrown‚Äù back to you, because we now have the 3 stored in x:\n\n\nx\n\n[1] 3"
  },
  {
    "objectID": "variable_assignment_in_r.html#updating-the-value-of-a-variable",
    "href": "variable_assignment_in_r.html#updating-the-value-of-a-variable",
    "title": "Variable Assignment in R",
    "section": "Updating the Value of a Variable",
    "text": "Updating the Value of a Variable\n\nNow, we can of course use x moving forward, e.g.¬†by adding 2:\n\n\nx + 2\n\n[1] 5\n\n\n\nNotice how this does not change x and the result is simply ‚Äúthrown‚Äù right-back-at-ya\n\n\nx\n\n[1] 3\n\n\n\nIf we wanted to update x by adding 2, we would have to ‚Äúcatch‚Äù the result as before:\n\n\nx <- x + 2\n\n\nNow, we have updated x:\n\n\nx\n\n[1] 5"
  },
  {
    "objectID": "variable_assignment_in_r.html#use-one-variable-in-the-creation-of-another",
    "href": "variable_assignment_in_r.html#use-one-variable-in-the-creation-of-another",
    "title": "Variable Assignment in R",
    "section": "Use one Variable in the Creation of Another",
    "text": "Use one Variable in the Creation of Another\n\nAnalogue, we can create a new variable using x:\n\n\ny <- x + 3\n\n\nAgain, this does not change x\n\n\nx\n\n[1] 5\n\n\n\nBut rather the result is now stored in y\n\n\ny\n\n[1] 8"
  },
  {
    "objectID": "variable_assignment_in_r.html#summary",
    "href": "variable_assignment_in_r.html#summary",
    "title": "Variable Assignment in R",
    "section": "Summary",
    "text": "Summary\n\nIn R, we use the assignment operator <- to perform assignment\nVariables are not change in place, but needs to be stored\nNote, this also applies to running e.g.¬†a dplyr-pipeline, where we do not change the dataset by running the pipeline, but we must store the result of the pipeline\n\nBefore continuing, make sure that you are on track with the above concepts!\n\nCreate a new variable my_age containing‚Ä¶ You guessed it!\nAdd 0.5 to the variable (I.e. your age, when you‚Äôre done with this course)\nCheck the value of my_age, did you remember to assign, thereby updating?"
  },
  {
    "objectID": "variable_assignment_in_r.html#pipeline-example",
    "href": "variable_assignment_in_r.html#pipeline-example",
    "title": "Variable Assignment in R",
    "section": "Pipeline Example",
    "text": "Pipeline Example\n\n\n\n\nLet us create some example sequence data:\n\n\ntibble(sequence = c(\"aggtgtgag\", \"tggaatgaaccgcctacc\",\n                    \"aagaatgga\", \"tct\", \"tgtatt\", \"tgg\",\n                    \"accttcaacgagtcccactgt\", \"cgt\",\n                    \"gaggctgagctggttgta\", \"ggggaacag\"))\n\n# A tibble: 10 √ó 1\n   sequence             \n   <chr>                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nNotice, that our data creation is just ‚Äúthrown‚Äù back at us, we forgot something!\n\n\nmy_dna_data <- tibble(sequence = c(\"aggtgtgag\", \"tggaatgaaccgcctacc\",\n                                   \"aagaatgga\", \"tct\", \"tgtatt\", \"tgg\",\n                                   \"accttcaacgagtcccactgt\", \"cgt\",\n                                   \"gaggctgagctggttgta\", \"ggggaacag\"))\n\n\nNow, we have stored the data in the variable my_dna_data\n\n\nmy_dna_data\n\n# A tibble: 10 √ó 1\n   sequence             \n   <chr>                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nNote here, that a variable can as we saw before with x and y store a single value, e.g.¬†2, but here, we are storing a tibble-object in the variable my_dna_data and in that tibble-object, we have a variable sequence, which contains some randomly generated dna.\nBut what if we wanted to add a new variable to the tibble-object, which is the lenght of each of the dna-sequences?\n\n\nmy_dna_data |>\n  mutate(dna_length = str_length(sequence))\n\n# A tibble: 10 √ó 2\n   sequence              dna_length\n   <chr>                      <int>\n 1 aggtgtgag                      9\n 2 tggaatgaaccgcctacc            18\n 3 aagaatgga                      9\n 4 tct                            3\n 5 tgtatt                         6\n 6 tgg                            3\n 7 accttcaacgagtcccactgt         21\n 8 cgt                            3\n 9 gaggctgagctggttgta            18\n10 ggggaacag                      9\n\n\nNice! Let‚Äôs see that data again then:\n\nmy_dna_data\n\n# A tibble: 10 √ó 1\n   sequence             \n   <chr>                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nWait! What? Where is the variable we literally just created?\nWe forgot something‚Ä¶ We did not update the my_dna_data, let‚Äôs fix that:\n\n\nmy_dna_data <- my_dna_data |>\n  mutate(dna_length = str_length(sequence))\n\n\nNote, nothing is ‚Äútrown‚Äù back at us! Let‚Äôs verify, that we did indeed update the my_dna_data:\n\n\nmy_dna_data\n\n# A tibble: 10 √ó 2\n   sequence              dna_length\n   <chr>                      <int>\n 1 aggtgtgag                      9\n 2 tggaatgaaccgcctacc            18\n 3 aagaatgga                      9\n 4 tct                            3\n 5 tgtatt                         6\n 6 tgg                            3\n 7 accttcaacgagtcccactgt         21\n 8 cgt                            3\n 9 gaggctgagctggttgta            18\n10 ggggaacag                      9\n\n\nDid it make sense? Check yourself, add a new variable to my_dna_data called sequence_capital by using the function str_to_upper()\nThat‚Äôs it - Hope it helped and remember‚Ä¶ Bio data science in R is really fun!"
  },
  {
    "objectID": "code_styling.html",
    "href": "code_styling.html",
    "title": "Code Styling",
    "section": "",
    "text": "This is a condensed primer on code styling, based on The tidyverse style guide"
  },
  {
    "objectID": "code_styling.html#why",
    "href": "code_styling.html#why",
    "title": "Code Styling",
    "section": "Why?",
    "text": "Why?\n‚ÄúGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread‚Äù source\nIt is not uncommon in a bio data science industry setting, where there is an aspect of production code, i.e.¬†the code you are writing is being used professionally by other users either in an organisation or as an actual software product, that you will have to adhere to a certain style, when coding. This facilitates lower maintanence on code. Imagine the case, where you are perhaps 10 bio data scientists working on e.g.¬†an R-package, which will used downstream by another department. You really need to make sure, that everything is top notch, so you decide to do code reviewing. You write your code up and then your colleague will go over the code to check it. But you and your colleague have completely different opinions on how the code should be styled. This will result in an unnecessary time overhead. Another case, could be a colleague leaving for another position, where you then have to take over that colleague‚Äôs code base and you end up re-styling the code, so it matches your preferences, again this will increase maintenance unnecessarily. The solution is to agree on a set of rules and principles on how to style your code - This is code styling!\nBelow follows the code styling you will have to adhere to in this course"
  },
  {
    "objectID": "code_styling.html#so-how-should-we-style-the-code",
    "href": "code_styling.html#so-how-should-we-style-the-code",
    "title": "Code Styling",
    "section": "So, how should we style the code?",
    "text": "So, how should we style the code?\nIn this course, we will use principles from the The tidyverse style guide. At first, it may seem constraining, but you will quickly get used to it and then it will be easier moving forward. Note, for your project, being able to review consistant code, will save you valuable time!\nRecall the cancer_data (gravier) dataset, we worked with:\n\nlibrary(\"tidyverse\")\nlibrary(\"curl\")\nbase_url <- \"https://github.com/\"\ntarget_file <- \"ramhiser/datamicroarray/raw/master/data/gravier.RData\"\noutput_file <- \"data/gravier.RData\"\ncurl_download(url = str_c(base_url,\n                          target_file),\n              destfile = output_file)\n\nPrinciples:\n\nQuote the packages you load using the library-function\nUse the proper variable assignment in R, namely <-\nStay within 80 characters width. Note, you can set a ‚Äúhelp line‚Äù: Tools \\(\\rightarrow\\) Global Options... \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Show margin \\(\\rightarrow\\) Margin column: 80\nUse double quotes. This is for consistency with other languages\nTab out parameters of functions\nDo line breaks after commas\nDo one space on each side of =, when assigning arguments to parameters, e.g.¬†my_function(parameter_1 = argument_1), etc.\nMatch indentations, this RStudio will do for you in most cases\n\nOk, a bit of data wrangling:\n\nload(file = \"data/gravier.RData\")\ncancer_data <- gravier |> \n  bind_cols() |> \n  rename(early_metastasis = y) |> \n  mutate(pt_id = str_c(\"pt_\", row_number()),\n         pt_has_early_metastasis = case_when(\n    early_metastasis == \"good\" ~ \"No\",\n    early_metastasis == \"poor\" ~ \"Yes\"))\n\nPrinciples:\n\nLine break after each pipe |>, recall we prononuce the pipe as ‚Äúthen‚Äù\nLine break after commas inside functions, such as here with mutate()\nIf the line in mutate() becomes wide, then consider using a single linebreak after opening the function call\nuse proper descriptive variable names in snake case like pt_has_early_metastasis, there is no overhead in understanding what this variable means\n\nLet‚Äôs do a simple plot:\n\ncancer_data |>\n  ggplot(aes(x = early_metastasis,\n             y = g8A08)) +\n  geom_hline(yintercept = 0,\n             linetype = \"dashed\") +\n  geom_boxplot() +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw() +\n  labs(x = \"Early Metastasis\")\n\n\n\n\nPrinciples:\n\nLinebreak after +, just like with the pipe\nSpace after comma inside a vector, like when we define the limits\n\nDo it - It‚Äôll be fun and it does not take a long time to adapt!"
  },
  {
    "objectID": "external_resources.html",
    "href": "external_resources.html",
    "title": "External Resources",
    "section": "",
    "text": "Here, you will find various valuable resources, to aid your bio data science workflow"
  },
  {
    "objectID": "external_resources.html#a-few-quick-ones",
    "href": "external_resources.html#a-few-quick-ones",
    "title": "External Resources",
    "section": "A few quick ones‚Ä¶",
    "text": "A few quick ones‚Ä¶\n\nA very handy ggplot cheat-sheet can be found here\nSo which plot to choose? Check this handy guide\nExplore ways of plotting here\nThere is a nice tool to aid in choosing colours for visualisations here\nThe Posit community pages is a very nice place to get help if you‚Äôre stuck"
  },
  {
    "objectID": "external_resources.html#open-source-data-science-books",
    "href": "external_resources.html#open-source-data-science-books",
    "title": "External Resources",
    "section": "Open source data science books",
    "text": "Open source data science books\n\nHands-On Programming with R by Garrett Grolemund\nStatistical Inference via Data Science - A moderndive into R and the tidyverse by Chester Ismay and Albert Y. Kim\nIntroduction to Data Science, Data Analysis and Prediction Algorithms with R by Rafael A. Irizarry\nMastering Shiny by Hadley Wickham\nAn Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\nSTAT 545 - Data wrangling, exploration, and analysis with R by Jenny Bryan\nHappy Git and GitHub for the useR by Jenny Bryan, the STAT 545 TAs, Jim Hester"
  },
  {
    "objectID": "external_resources.html#software-links",
    "href": "external_resources.html#software-links",
    "title": "External Resources",
    "section": "Software Links",
    "text": "Software Links\n\nThe R Project for Statistical Computing\nRStudio - Open Source and Enterprise-ready professional software for R\nTidyverse website"
  },
  {
    "objectID": "external_resources.html#some-useful-links",
    "href": "external_resources.html#some-useful-links",
    "title": "External Resources",
    "section": "Some Useful Links",
    "text": "Some Useful Links\n\nFrom data to Viz: Find the graphic you need\nR for Data Science: Exercise Solutions\nR colour guide by Tian Zheng\nThe tidyverse style guide - By Hadley Wickham\nRStudio Primers - Learn data science basics with the interactive tutorials\nThe tidyverse style guide\nswirl - Learn R, in R\nRStudio Cheat Sheets\nRStudio Community - Stuck? Ask a question and get help moving on\nHarvardX Biomedical Data Science Open Online Training\nData Analysis Playlist"
  },
  {
    "objectID": "external_resources.html#on-data-science",
    "href": "external_resources.html#on-data-science",
    "title": "External Resources",
    "section": "On Data Science",
    "text": "On Data Science\n\nThe Role of Academia in Data Science Education"
  },
  {
    "objectID": "external_resources.html#guides-on-good-data-practices",
    "href": "external_resources.html#guides-on-good-data-practices",
    "title": "External Resources",
    "section": "Guides on Good Data Practices",
    "text": "Guides on Good Data Practices\n\nA Guide to Reproducible Code by the British Ecology Society\nA Quick Guide to Organizing Computational Biology Projects\nHow to pick more beautiful colors for your data visualizations\nTalk: Steps toward reproducible research by Karl Broman"
  },
  {
    "objectID": "course_elements.html",
    "href": "course_elements.html",
    "title": "Course Elements",
    "section": "",
    "text": "Various elements central to the course"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "As part of this course, starting from Lab 2, there will be a weekly hand in. Here, follows the instructions for creating and handing in course assignments."
  },
  {
    "objectID": "assignments.html#assignment-instructions",
    "href": "assignments.html#assignment-instructions",
    "title": "Assignments",
    "section": "Assignment Instructions",
    "text": "Assignment Instructions\nThe assignment will be marked ‚ÄúGroup Assignment‚Äù with red font in the exercises. In your group, you are to prepare an answer to just this one question. Think about reproducibility from the get-go, so include in your assignment, what is needed to re-create your micro-report, e.g.¬†if you‚Äôre using external data, from where and how did you get it? You will then receive group feedback on your assignment, which you should make sure to go over in your group.\nPlease note, that in order to include all elements in a self-contained html file, you will have to use the following YAML-header in your Quarto document:\n\n---\ntitle: \"Lab 2 Assignment: Group 02\"\nformat:\n  html:\n    embed-resources: true\neditor: visual\n---\n\nOr at least one similar, format title and other elements as you see fit, key here is the html-format with embedded resources."
  },
  {
    "objectID": "assignments.html#how-to-hand-in",
    "href": "assignments.html#how-to-hand-in",
    "title": "Assignments",
    "section": "How to hand in",
    "text": "How to hand in\n\nCheck that your assignment conforms to the assignment checklist, as defined in the next section\nGo to DTU Learn\nFind and click your R for Bio Data Science course\nMake sure you are enrolled in the correct group, as defined by the Group Formation Sheet (see the Getting Started Section)\nIn case groups consists of a mix of BSc. and MSc. students, then enroll under your respective courses\nOnly hand in one assignment, meaning either under the BSc. or the MSc. course, not both\nYou should hand in the rendered html-file, make sure to compress it to a zip-file prior to upload"
  },
  {
    "objectID": "assignments.html#assignment-checklist",
    "href": "assignments.html#assignment-checklist",
    "title": "Assignments",
    "section": "Assignment Checklist",
    "text": "Assignment Checklist\nDid you?\n\nMatch your group number with DTU Learn groups\nJust answer the one task marked as ‚ÄúGroup Assignment‚Äù\nCreate the assignment using Quarto\nModify the YAML-header to ensure encapsulation\nInclude Group number, names and student ids\nUse proper markdown headers as defined by #, ##, ###\nWrite a few sentences under each section\nSeparate text clearly from code chunks\nFollow course code styling\nThink about reproducibility with respect to data and libraries?\nRender to html\nCompress the html-file to a zip-file, before upload to DTU Learn\nMake sure to download the upload‚Äôed zip-file and check that everything looks right?"
  },
  {
    "objectID": "project_faq.html",
    "href": "project_faq.html",
    "title": "Project FAQ",
    "section": "",
    "text": "Where can I find data? Rethink the question: Discuss in your group what are your interests and then find data related to your problem, there are literally terabytes of publicly available data, i.e.¬†don‚Äôt just google ‚Äúdata‚Äù, be specific\nIn order for you to demonstrate that you master the entire bio data science cycle, you must choose a bio data set, which requires cleaning and tidying\nIt would be good, if the data set requires joining, if not, consider artificially splitting it, to demonstrate that you master joining\nOne approach could be to work on reproducing results from a paper. Here, you can even consider improving the data visualisations or adding something extra\nDo not use a data set from one of the course exercises. Doing so would not allow you to demonstrate that you are independently capable of performing a bio data science analysis\nYou are 100% free to choose bio data from any resource as long as it meets the above requirements\n\n\n\n\n\nDepends, does your data set come with a readme, which allows you to make a decision regarding NAs?\nBe careful not to simply drop all NAs as you might drop observations, where columns of interest have complete data\n\n\n\n\n\nIn case your variable contains categories, i.e.¬†values, where the question ‚ÄúIs one larger than the other‚Äù is nonsensical, then encode as factors\n\n\n\n\n\nIf you believe the data points are nonsensical, e.g.¬†manual typing errors, then exclude them, but be very clear about which data (groups of) points were excluded and why\n\n\n\n\n\nYes, if you are particularly interested in a subset of the data, then that is perfectly fine. Just be aware, that any choice you take with the data, you must be able to account for why you took that choice\nYou should NOT just sample 100 observations as was done in the exercises to reduce runtime. However, if your data set is large, consider down-sampling either randomly or by some metric of association. For the latter e.g.¬†perform a test and identify the top X observations, save those to file and continue from there.\nFor large data, try appending ‚Äú.gz‚Äù to your files, when you write them using write_tsv(), this will invoke gzip-compression"
  },
  {
    "objectID": "project_faq.html#inputoutput-files",
    "href": "project_faq.html#inputoutput-files",
    "title": "Project FAQ",
    "section": "Input/output files",
    "text": "Input/output files\n\nHow can we get a better overview of the data file flow?\n\nConsider creating a flow chart of ‚Äúyour data journey‚Äù. It will force you to think about how files are connected and how input/output flows\n\n\n\nHow can we write a file with e.g.¬†factor encoding or a nested tibble?\n\nThis can only be done by writing an R-object. However, this is for future reference - in this course stick to flat text files, e.g.¬†.tsv or .csv\n\n\n\nHow do we handle different naming conventions?\n\nDO NOT do a manual search and replace in Excel. That defeats the whole purpose of this course\nFix names using a programmatic approach. The stringr-package is extremely useful in this context\n\n\n\nWhere should we place external files?\n\nConsider creating an ‚Äúimages‚Äù folder in the ‚Äúdoc‚Äù folder, from where you can input images to your presentation"
  },
  {
    "objectID": "project_faq.html#modelling",
    "href": "project_faq.html#modelling",
    "title": "Project FAQ",
    "section": "Modelling",
    "text": "Modelling\n\nWhat should we include in the modelling part?\n\nWe have worked with fitting a logistic regression, we have done a PCA and we have performed clustering using k-means. You could do something along those lines or something of similar complexity\nThis is not a modelling course. We briefly visited modelling to bridge the process of going from the raw data to analysis ready and then communication via data visualisation. Therefore, do not start fitting a full fledged machine/deep learning model, it is a time-void, which you cannot ‚Äúafford‚Äù to get sucked into\nAlso, mind that the focus of this course is to make you realise that even though you have been trained in delivering results, then the process of arriving at the results in a reproducible manner is equally important\n\n\n\nDo we HAVE to do a PCA?\n\nDoes it make sense to do a PCA-analysis in your project? If you have group labels and you want to visualise to see if there is a separation, then a PCA is a good first step\n\n\n\nOur model is not performing very well. What should we do?\n\nLeave it as is. The focus of this course is not on the results, but on the reproducible process of arriving and communicating said results"
  },
  {
    "objectID": "project_faq.html#coding",
    "href": "project_faq.html#coding",
    "title": "Project FAQ",
    "section": "Coding",
    "text": "Coding\n\nWhat goes in the ‚Äúaugment‚Äù script?\n\nIf you google ‚ÄúAugment‚Äú, you will get ‚Äúmake (something) greater by adding to it; increase‚Äù, in other words, when you add e.g.¬†variables to your data, which was not there initially, e.g.¬†like we did, when we calculated the BMI from existing information on weight and height\nYou should think of the augment script as the place, where you create your database for everything that happens afterwards, i.e.¬†all your analyses and ideas\nCreate that ‚Äúdatabase‚Äù and then load it into your downstream scripts\nAugment == Adding something\n\n\n\nCan we mix base R and tidyverse?\n\nNo, you might as well get used to it - Tidyverse all the way! (This is a tidyverse course)\n\n\n\nWhat if we can‚Äôt make it work in tidyverse but only in base R?\n\nIf you can make it work in base R, you can make it work in tidyverse\n\n\n\nIs it okay to use e.g.¬†the base function sum()?\n\nYes, think of it this way: R has as core functionality to do statistics. Tidyverse is for performing the data manipulation surrounding these calculations. Therefore, using functions such as sum(), mean(), sd(), etc. is naturally fine\n\n\n\nHow do we know if there is a tidyverse function we should use rather than base?\n\nIdentify the general area of what you‚Äôre working with. E.g. if strings, then go to your RStudio session and find the console and type ‚Äústringr::‚Äù and hit tab, then you can look through the functions\nLook in the R-for-Data-Science book https://r4ds.had.co.nz\nAsk on the community pages https://community.rstudio.com\n\n\n\nCan we use package X for analysis/visualisation?\n\nIf the package performs a lot of the work, which you are to demonstrate that you can do, then no. Do not use packages, where you call a plot-my-data-function and the ggplot-magick happens that will not allow you to demonstrate that you have met the course learning objectives\n\n\n\nHow do we run the entire project incl.¬†the presentation?\n\nMake sure to include a programmatic call to knitr at the end of your doit-script\n\n\n\nShould we create a Shiny app?\n\nThe project deliverables are the GitHub repo and your presentation. Remember, you do this project not for me as a teacher, but for you to internalize the knowledge you have been exposed to during the initial 10 weeks of teaching. If you find Shiny interesting/fun, then by all means, please do create an app. Shiny apps are optional\n\n\n\nShould we create a package?\n\nOptional, if you do, be careful with your time usage. You should focus on the data science cycle\n\n\n\nWhen should we create functions?\n\nRemember DRY (Dont-Repeat-Yourself), so generally, if you do something more than once, it is a function. However, we do not focus much on functions in this course, so don‚Äôt put too much effort into creating functions\nPlace functions in 99_proj_functions.R as illustrated in the overview\nDO NOT hide your code away in functions, so that your main script just becomes 10 function calls. For this process-oriented course, show the code in your main scripts, i.e.¬†01_, 02_, ‚Ä¶\n\n\n\nCan we use loops?\n\nNo, you should instead embrace functional programming and use functions from the ‚Äúpurrr‚Äù-package. Revisit lab 6, if needed\n\n\n\nCan we directly copy/paste a code chunk from an online source?\n\nNo, that would be plagiarism. Understand the steps in the chunk and make the code your own\nI acknowledge that for coding this is not completely black and white, but please refrain from a direct copy/paste Coding style\n\n\n\nHow should we comment on our code?\n\nRemember, the point of writing verbose tidyverse code is that the code-becomes-the-comments. Think of it this way: The pipeline is the text on a page in a book, so before your pipeline, put a header/title on what is happening below, just as a title/header in a book\nThe title/header will be what is generally going on, e.g.¬†‚ÄúNormalise all gene expression values using standard score approach‚Äù and the pipeline will be how that is actually done\nThe pipe ‚Äú%>%‚Äù is pronounced as ‚Äúthen‚Äù, when you ‚Äúread‚Äù your code\n\n\n\nHow should we style our code?\n\nUse the tidyverse style guide as introduced in the course: https://style.tidyverse.org/index.html\nMake sure that all scripts are styled the same way and be consistent in your coding style\nYour code should not be >80 chars wide, follow the vertical line in your editor\n\n\n\nHow should we style our plots?\n\nBe concise, ‚Äúless is more‚Äù\nMake some nice and relevant plots. Show us that you‚Äôve learned data visualisation. Remember legends, titles etc.\nDo not put 3 messages in 1 plot. Instead put each message into a different plot\nBe careful with matching the text size in your plots to that of your presentation (trial-and-error)"
  },
  {
    "objectID": "project_faq.html#github",
    "href": "project_faq.html#github",
    "title": "Project FAQ",
    "section": "GitHub",
    "text": "GitHub\n\nShould our GitHub be public or private?\n\nThat is completely up to you. As students, you ‚Äúown‚Äù anything you create while studying. The teaching staff must however have complete access.\n\n\n\nShould we keep all the data on GitHub?\n\nYes, for this course. However, be aware that GitHub is meant for code, not data.\n\n\n\nWhat goes in the ‚Äúdoc‚Äù folder?\n\nThe project organisation chart is a generic figure. You should not hand in a report. Your deliverable/product outcome for the project period is: 1) Your GitHub repository and 2) A presentation. Therefore, the ‚Äúdoc‚Äù folder would in this case contain your presentation\n\n\n\nWhy are there multiple ‚Äúreports‚Äù in the ‚Äúdoc‚Äù folder?\n\nIn case you have a computationally intensive part of the project it can be an advantage to split into sub-reports and then collect in a master report (think large LaTeX reports)\nOverview, even if your report is not computationally intensive, then it may be long and splitting into sub-reports facilitate better overview\n\n\n\nShould we include a README on GitHub?\n\nThat would be a good way to briefly introduce what the contents of this repo is and something one would usually do\n\n\n\nShould everything be on Github, also plots we don‚Äôt present?\n\nYes, everything!"
  },
  {
    "objectID": "project_faq.html#examhand-in-deliverables",
    "href": "project_faq.html#examhand-in-deliverables",
    "title": "Project FAQ",
    "section": "Exam/hand-in deliverables",
    "text": "Exam/hand-in deliverables\n\nWhat is the final product?\n\nA GitHub repository organised according to principles of reproducible data analysis\nA ioslides/rmarkdown HTML-presentation following the IMRAD structure as elaborated in the project introduction slides (see lab 10)\nThis final presentation in HTML-format should be uploaded to DTU Learn\n\n\n\nDo we need to hand in a report?\n\nNo, no report is required!"
  },
  {
    "objectID": "project_faq.html#presentation",
    "href": "project_faq.html#presentation",
    "title": "Project FAQ",
    "section": "Presentation",
    "text": "Presentation\n\nShould we include code in our presentation?\n\nYour presentation should follow the standard scientific IMRAD-structure, i.e.¬†introduction, materials-and-methods, results, and discussion.\nInclude certain decisions you took with the data and your reasoning herfore.\nIf you found a particular challenging problem in coding, to which you found an elegant solution, include that in your presentation as an example\nThe presentation is your chance to practice communicating insights to stakeholders\nYour audience will be same-level bioinformaticians\nPerhaps consider a graphical representation of your process going from raw to analysis-ready data\n\n\n\nShould we include tables or plots?\n\nWhat makes sense? If you only have two numerical values, then perhaps a table is fine. If you have 100 observations, then a plot is likely better\nRemember, we are as humans evolutionary encoded to interpret visual information, not numbers\n\n\n\nHow do we add plots to the rmarkdown presentation?\n\nOutput the plot as a png file using the function ggsave and include that png in your presentation. Think dynamically, we don‚Äôt want to type anything manually in our presentation\n\n\n\nHow do we get the presentation in wide-/full screen?\n\nHit w and f while the HTML presentation is loaded\n\n\n\nShould we split our presentation into sub-presentations?\n\nNo, this is not necessary given the extend/size of this project. Just create one Rmd file, which outputs an HTML-presentation\n\n\n\nWe are working on re-creating paper results. Should we re-create the exact same plots?\n\nIdeally, you re-create the plots and then you create your own improved version of the visualisation.\n\n\n\nWhere should we ‚Äúplace‚Äù our Shiny app in our presentation?\n\nDemo it briefly at the end\n\n\n\nHow can we illustrate our data handling?\n\nConsider creating a flow chart, what are input files and how are they connected to final output?\n\n\n\nHow much code should we include in the presentation?\n\nThe exam deliverables are two-fold. Code is the GitHub repo and your ability to communicate biological insights is in the presentation. Therefore, code could be included in the presentation if you faced and solved a particularly challenging problem\n\n\n\nHow do we include data numbers in the presentation?\n\nYou could from your script output a results table with relevant numbers as a .tsv file and then read that into the presentation and extract the numbers\nRemember, think dynamical reporting, what if your input data changes and you manually entered the numbers in your presentation? Then you wouldn‚Äôt be much farther than a powerpoint\n\n\n\nWhat goes into the materials and methods section?\n\nMaterials: What data did you use and where did you get it from?\nMethods: Which modelling did you use? Think of the methods section as a recipe for how to go from raw to results => Flow chart?\n\n\n\nShould we interpret our results?\n\nYes, you have to think about it as a ‚Äúnormal‚Äù project presentation\n\n\n\nWhat about all the stuff we tried, which did not work?\n\nIn the presentation, you should focus on what worked and what results you arrived at\nExclude dead-ends from the presentation, but‚Ä¶ Leave them in the repo, perhaps with an initial comment signifying that the following turned out to be a dead-end\nThink about this - Scenario: You‚Äôre working in a company and you and your team of 3 spend 6 months on a project, which turns out to be a dead-end. For future reference: Would the company be interested in knowing that this project was a dead end or should you delete everything and never speak of it again?"
  },
  {
    "objectID": "project_faq.html#project-check-up-and-summary",
    "href": "project_faq.html#project-check-up-and-summary",
    "title": "Project FAQ",
    "section": "Project Check Up and Summary",
    "text": "Project Check Up and Summary\n\nIMPORTANT: Ask yourselves:\n\n\n‚ÄúDoes our presentation follow the IMRAD structure?‚Äù\n\nIs the presentation created as one ioslides Rmd file and output to a HTML?\nIs the presentation clear and concise?\nAre we doing good data communication via good visualisations?\nDo we present a clear overview of the data process incl.¬†any decisions made, e.g.¬†using a flow chart?\nAre we clearly communicating a biological insight?\nAre we following std guidelines? Sources, references, etc.\n\n\n\n\n\n\n\n\n‚ÄúDoes our project include all components of the data science cycle?‚Äù\n\n\n\n\n\n\n\n‚ÄúAre we aware of and have included learning objectives as appropriate in the project?‚Äù\n\n\n\n\n\n\n\n‚ÄúIs our project-GitHub organised as instructed and can it run end-to-end via doit?‚Äù\n(Note, this is a generic representation, your doc folder will contain your presentation)\n\n\n\n\n\n\n\n‚ÄúDoes our code in all our scripts follow the Tidyverse style guide?‚Äù\n\nRecall what we discussed in the course on styling\nAlso, see: https://style.tidyverse.org/index.html\nE.g.:\n\n\n\n\n\n\n\n\n‚ÄúAre we using base-R, where we should use tidyverse-R?‚Äù\n\nE.g. think about the following:\n\n\n\n\n\n\n\n\n‚ÄúCan we explain and justify the data decisions in the project?‚Äù\n\nIMPORTANT: In essence it does not matter which decision you took, what matters is your ability to explain and justify why you decided on that particular path in your analysis"
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Exam",
    "section": "",
    "text": "The exam consists of 3 components:\n\nA group project, where all members are responsible for all parts of the project. This is handed in as a code base on GitHub\nAn oral group presentation of the project, also to be handed in on GitHub\n2 hour MCQ exam, where general course learning objectives are examined, see course description\n\nDeadline for completing the project code base and presentation is 23:59 on the day before lab 13\nPlease note that active participation in the group work and timely submission of project and code base are both indispensable prerequisites for exam participation. The final grade is based on an overall assessment of all three components.\n\n\n\nBe sure that everyone understands ALL code in the project as all group members are responsible for ALL code\nThe point of the project is to cover more or less the entire course cycle, so if you have decided to leave out some parts, then you are also expected to be able to answer questions relating to these\nExpect a few overall questions regarding the decisions you have made throughout your project\nBe sure to read Project FAQ\n\n\n\n\n\nThe format is a 10-slides-in-10-mins and everyone in the group must present a part\nThe group presentations will be on the last day of the course, i.e.¬†lab 13\n\n\n\n\n\nA 2-hour individual multiple choice exam\nIn 2022 the exam contained 60 questions, expect a similar number of questions\nAll aids are allowed, but no open internet\nEach question will have 4 possible answers and only 1 is the right one and you can only choose one answer per question"
  }
]