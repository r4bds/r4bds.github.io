[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R for Bio Data Science",
    "section": "",
    "text": "A tidy approach to wrangling, exploring, visualising and communicating bio data with an emphasis on doing collaborative and reproducible bioinformatics projects \nLeon Eyrich Jessen & the TA team\n\nWelcome to R for Bio Data Science\nSo, you signed up for the 22100/22160 bioinformatics study line course - Congratulations! That was your first step towards getting a set of bio data science skills, which will serve you through your future career regardless of your path!\nInspirational quotes can be cliche, however this one hits the nail on the head:\n\n‚ÄúThink about the readability of your code. Every project your work on is fundamentally collaborative. Even if you are not working with any other person, you are always working with future you and you really do not want to be in a situation where future you has no idea what past you was thinking, because past you will not respond to any emails!‚Äù Hadley Wickham\n\nBio Data Science in intrinsically collaborative (even if it‚Äôs just you working) and intrinsically interdisciplinary, so collaborative-, reproducibility- and communication- skills are key. In this course, you will learn how to do modern project oriented collaborative bio data science in tidyverse R - Welcome!"
  },
  {
    "objectID": "prologue.html",
    "href": "prologue.html",
    "title": "Prologue",
    "section": "",
    "text": "The course is designed as a semi-flipped classroom, with an emphasis on active learning. This means that during the 4h classes, the first hour will be dedicated to reviewing key points from last week and then a brief introduction to the topic of the day followed by a break. Hereafter, the students will work hands-on in groups on computational exercises using cloud computing infrastructure. The exercises will rely on relevant bioinformatics data from publicly available databases and gradually build the students toolbox with an emphasis on collaborative project work. This part will take up the first 9 labs. Each lab is defined by a set of specified learning objectives, it is essential that students continuously make sure, that they are on track with these LOs.\nThe fist hour of the 10th lab is dedicated to introducing the project part of the course and the subsequent 3h are dedicated to a mini symposium on ‚ÄúApplication of R for Bio Data Science in Industry‚Äù. Here students will get a change to get insights into how the course topics are implemented in industry and get a glimpse into what options are available upon completing their education. This hybrid event typically attracts ~250 participants and have featured talks from major national and international Pharma/biotech companies, such as: Novo Nordisk, Lundbeck, Chr. Hansen, Bristol Meyer Squibb, a.o.\nIn the project part of the course, students will form groups of 4-5 students based on common interests. Hereafter, the students will seek out and select a data set, which will form the foundation for the project work. In the project work, the students will go through the entire data science cycle and produce the code base for a complete bioinformatics project. This entails a complete synthesis of all components of the exercise labs and supports the collaborative aspect. The groups must then condense the project into a presentation, thereby addressing communicative competencies as an essential part of being a modern bio data scientist."
  },
  {
    "objectID": "getting_started.html",
    "href": "getting_started.html",
    "title": "Getting Started",
    "section": "",
    "text": "This section contain essential information for getting up and running for the classes"
  },
  {
    "objectID": "getting_started.html#when-and-where",
    "href": "getting_started.html#when-and-where",
    "title": "Getting Started",
    "section": "When and Where",
    "text": "When and Where\nTeaching sessions will be E3A, Tuesday mornings 8 - 12 in building 358, room 060a (exercises: Also room 045) and the general schedule will be:\n\n08.00 - 08.30 Recap of key points from last weeks exercises\n08.30 - 09.00 Introduction to theme of the day\n09.00 - 12.00 Exercises\n\nPlease note, at DTU we do not use ‚Äúthe academic quarter‚Äù, this means that 8 am, means that class will commence at 8 am"
  },
  {
    "objectID": "getting_started.html#logging-onto-cloud-server",
    "href": "getting_started.html#logging-onto-cloud-server",
    "title": "Getting Started",
    "section": "Logging onto Cloud Server",
    "text": "Logging onto Cloud Server\nFirst, make sure you have a working DTU account, either as a student id or employee initials (e.g.¬†PhD-students or postdocs). In this course we will be using a cloud server infrastructure to perform our work.\nPlease click here to test your access the cloud server: R for Bio Data Science Cloud Server\nNote, that there is a 24h time out on the sessions, meaning that if you logon and forget to sign out, your session will be terminated after 24h."
  },
  {
    "objectID": "getting_started.html#class-communication",
    "href": "getting_started.html#class-communication",
    "title": "Getting Started",
    "section": "Class Communication",
    "text": "Class Communication\nTo streamline class communication, we will be using Slack. The aim is to facilitate getting you help fast and efficiently from classmates, the TAs, and myself. Rather than emailing questions to the teaching staff, I encourage you to post your questions on Slack. Each teaching lab will have its own dedicated Slack channel. Note: You can also make group specific Slack channels!\nPlease join the Slack workspace by clicking here"
  },
  {
    "objectID": "getting_started.html#setting-up-a-github-account",
    "href": "getting_started.html#setting-up-a-github-account",
    "title": "Getting Started",
    "section": "Setting up a GitHub account",
    "text": "Setting up a GitHub account\nPrior to class, please go to GitHub and setup and account, shouldn‚Äôt take long. During the registration process you can set up a student account, which will give you additional benefits, including GitHub Pro. In order to get it, you need to use your DTU email when setting account and select Apply for your GitHub student benefits when asked during the registration process. You‚Äôll be then asked to apply for GitHub Student Developer Pack, which will require uploading your student id photo. The process of confirming a student account may take up to a few days (however, it can be almost instantaneous). In the meantime you can already use your free account. Free account should be sufficient for this class, so if you don‚Äôt want to set up a school account, you don‚Äôt need to.\nPlease state your GitHub username in this google sheet."
  },
  {
    "objectID": "getting_started.html#group-formation",
    "href": "getting_started.html#group-formation",
    "title": "Getting Started",
    "section": "Group Formation",
    "text": "Group Formation\nThe backbone of this course is modern collaborative data science and as such group work and active participation herein is mandatory. Furthermore, It is important that course participants prioritise to be present during classes as the course design is based on student-student interaction in an active learning environment. Therefore, students will work in groups of 4-5 students.\nPlease click here to go to the group formation sheet"
  },
  {
    "objectID": "getting_started.html#pre-course-questionnaire",
    "href": "getting_started.html#pre-course-questionnaire",
    "title": "Getting Started",
    "section": "Pre Course Questionnaire",
    "text": "Pre Course Questionnaire\nIn order for the teaching team to understand the class composition this year, please take 3 minutes to fill in this brief and 100% anonymous questionnaire"
  },
  {
    "objectID": "lab00.html",
    "href": "lab00.html",
    "title": "Course Labs",
    "section": "",
    "text": "This chapter provides a complete overview of the course curriculum"
  },
  {
    "objectID": "lab01.html",
    "href": "lab01.html",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "",
    "text": "Base"
  },
  {
    "objectID": "lab01.html#schedule",
    "href": "lab01.html#schedule",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Arrival, pre-course anonymous questionaire and interest based group formation\n08.15 - 08.45: Lecture: Course Introduction\n08.45 - 09.00: Break\n09.00 - 11.15: Exercises\n11.15 - 11.30: Break\n11.30 - 12.00: Lecture: Reproducibility in Modern Bio Data Science"
  },
  {
    "objectID": "lab01.html#learning-materials",
    "href": "lab01.html#learning-materials",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nRead the full course description here: 22100 / 22160\nAnswer the brief anonymous R for Bio Data Science Pre-course Questionnaire (See schedule above)\nRead course site sections: Welcome to R for Bio Data Science, Prologue and lastly Getting Started, where it is important that you perform any small tasks mentioned\nBook: R4DS2e: Welcome\nBook: R4DS2e: Preface to the second edition\nBook: R4DS2e: Introduction\nBook: R4DS2e: Chapter 2 Workflow: basics\nBook: R4DS2e: Chapter 28 Quarto (Don‚Äôt do the exercises)\nVideo: RStudio for the Total Beginner\nPaper: A Quick Guide to Organizing Computational Biology Projects"
  },
  {
    "objectID": "lab01.html#learning-objectives",
    "href": "lab01.html#learning-objectives",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nMaster the very basics of R\nNavigate the RStudio IDE\nCreate, edit and run a basic Quarto document\nExplain why reproducible data analysis is important, as well as identify relevant challenges and explain replicability versus reproducibility\nDescribe the components of a reproducible data analysis"
  },
  {
    "objectID": "lab01.html#sec-exercises",
    "href": "lab01.html#sec-exercises",
    "title": "Lab 1: Course Intro & the Very Basics",
    "section": "Exercises",
    "text": "Exercises\nToday, we will focused on getting you started and up and running with the first elements of the course, namely the RStudio IDE (Integrated Developer Environment) and Quarto. If the relationship between R and RStudio is unclear, think of it this way: Consider a car, in that case, R would be the engine and RStudio would be the rest of the car. Neither is particularly useful, but together they form a functioning unit. Before you continue, make sure you in fact did watch the ‚ÄúRStudio for the Total Beginner‚Äù video (See the Learning Materials for todays session).\n\nCloud server and the RStudio IDE\nGo to the R for Bio Data Science Cloud Server and follow the login procedure. Upon login, you will see this:\n\n\n\n\n\nThis is the RStudio IDE. It allows you to consolidate all features needed to develop R code for analysis. Now, click Tools \\(\\rightarrow\\) Global Options... \\(\\rightarrow\\) Pane Layout and you will see this:\n\n\n\n\n\nThis outlines the four panes you have in your RStudio IDE and allow you rearrange them as you please. Now, re-arrange them, so that they look like this:\n\n\n\n\n\nClick Apply \\(\\rightarrow\\) OK and you should see this:\n\n\n\n\n\n\n\nFirst steps\n\nThe Console\nNow, in the console, as you saw in the video, you can type commands like:\n\n2+2\n1:100\n3*3\nsample(1:9)\nX <- matrix(sample(1:9), nrow = 3, ncol = 3)\nX\nsum(X)\nmean(X)\n?sum\nsum\nnucleotides <- c(\"a\", \"c\", \"g\", \"t\")\nnucleotides\nsample(nucleotides, size = 100, replace = TRUE)\ntable(sample(nucleotides, size = 100, replace = TRUE))\npaste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\")\nreplicate(n = 10, expr = paste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\"))\ndf <- data.frame(id = paste0(\"seq\", 1:10), seq = replicate(n = 10, expr = paste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\")))\ndf\nstr(df)\nls()\n\nTake some time and play around with these commands and other things you can come up with. Use the ?function to get help on what that function does. Be sure to discuss what you observe in the console. Do not worry too much on the details for now, we are just getting started. But as you hopefully can see, R is very flexible and basically the message is: ‚ÄúIf you can think it, you can build it in R‚Äù.\n\nGo to Chapter 3 in R4DS2e and do the exercises\n\n\n\nThe Terminal\nNotice how in the console pane, you also get a Terminal, click and enter:\n\nls\nmkdir tmp\ntouch tmp/test.txt\nls tmp\nrm tmp/test.txt\nrmdir tmp\nls\necho $SHELL\n\nBasically, here you have access to a full terminal, which can prove immensely useful! Note, you may or may not be familiar with the concept of a terminal. Simply think of it as a way to interact with the computer using text command, rather than clicking on icons etc. Click back to the console.\n\n\nThe Source\nThe source is where you will write scripts. A script is a series of commands to be executed sequentially, i.e.¬†first line 1, then line 2 and so on. Right now, you should have a open script called Untitled1. If not, you can create a new script by clicking white paper with a round green plus sign in the upper left corner.\nTaking inspiration from the examples above, try to write a series of commands and include a print()-statement at the very end. Click File \\(\\rightarrow\\) Save and save the file as e.g.¬†my_first_script.R. Now, go to the console and type in the command source(\"my_first_script.R\"). Congratulations! You have now written your very first reproducible R-program!\n\n\n\nThe Whole Shebang\nEnough playing around, let us embark on our modern Bio Data Science in R journey.\n\nIn the Files pane, click New Folder and create a folder called projects\nIn the upper right corner, click where it says Project: (None) and then click New Project...\nClick New Directory and then New Project\nIn the Directory name:, enter e.g.¬†r_for_bio_data_science\nClick the Browse... button and select your newly created projects directory and then click Choose\nClick Create Project and wait a bit for it to get created\n\n\nOn Working in Projects\nProjects allow you to create fully transferable bio data science projects, meaning that the root of the project will be where the .Rproj file is located. You can confirm this by entering getwd() in the console. This means that under no circumstance should ever not work in a project nor should ever use absolute paths. Every single path you state in your project must be relative to the project root.\nBut why? Imagine you have create a project, where you have indeed used absolute paths. Now you want to share that project with a colleague. Said colleague gets your project and tests the reproducibility by running the project end-to-end. But it completely fails because you have hardcoded your paths to be absolute, meaning that all files and project ressources locations points to locations on your laptop.\nProjects are a must and allows you to create reproducible encapsulated bio data science projects. Note, the concept of reproducibility is absolute central to this course and must be considered in all aspect of the life cycle of a project!\n\n\nQuarto\nWhile .R-scripts are a perfectly valid way to write scripts, there is another Skywalker:\n\nIn the upper left corner, again, click the white paper with the round green plus, but this time select Quarto Document\nEnter a Title:, e.g.¬†‚ÄúLab 1 Exercises‚Äù and enter your name below in the box Author:\nClick Create\nImportant: Save your Quarto document! Click File \\(\\rightarrow\\) Save and name it e.g.¬†lab_01_exercises.qmd\nMinimise the Environment-pane\n\nYou should now see something like this:\n\n\n\n\n\nTry clicking the Render button just above the quarto-document. This will create the HTML5 output file. Note! You may get a Connection Refused message. If so, don‚Äôt worry, just close the page to return to the cloud server and find the generated .html file, left-click and select View in Web Browser.\nIf you have previously worked with Rmarkdown, then many features of Quarto will be familiar. Think of Quarto as the complete rethink of Rmarkdown, i.e.¬†based on all the experience gained, what would the optimal way of constructing an open-source scientific and technical publishing system?\nPerhaps you have previously encountered Jupyter notebooks, Quarto is similar. The basic idea is to have one document covering the entire project cycle, i.e.¬†code, plots and text will be in the same document.\nProceed R for Data Science (2e), chapter 29 and do the exercises.\n\n\n\nBio Data Science with a Virtual AI Assistant\nI am pretty sure you all know of chatGPT by now, so let us address the elephant in the room!\n\nGetting started\n\nGo to the ChatGPT site\nCreate a user and login\n\n\nLet us get acquainted\nNow, at the bottom it says ‚ÄúSend a message‚Äù, let us ask 3 simple question and see if we can find out what is what. Type the following questions in the prompt and read the answers:\n\nExplain in simple terms what you are\nExplain in simple terms how you work\nExplain in simple terms how you can be used to generate value as a virtual AI assistant, when doing Bio Data Science for R\nWhat is chatGPT and abbreviation for?\n\n\n\nMoving onto R\nIn the upper left corner, click ChatGPT it to start a new session.\nAgain, type the following questions in the prompt and read the answers:\n\nR\nWhat is R?\nGive a few simple examples\nExplain the difference between base R and Tidyverse R\n\nIf you do get some code examples, try to copy/paste into the console in RStudio and see if they run\n\n\nPrompt Engineering\nStart a new chat and enter:\n\nGive a few simple examples\n\nCompare the response with the one from before, is it the same or different and why so?\nDiscuss in your group what is ‚ÄúPrompt Engineering‚Äù and how does it relate to the above few tests you did? (Bonus info: Prompt engineer is already a job and people are making money off of selling ‚Äúprompts‚Äù)\n\n\n‚Ä¶a bit more\nStart a new chat and enter:\n\nTell me about DNA\n\nCheck if it gives you correct information?\nNow, write:\n\nGive a few fun examples on how to get started with the R programming language using DNA\nCopy/paste the code into the Console, do the examples all run?\nDiscuss in your group if you understand what is going on\nEarlier you were told to play around with some code snippets. Perhaps you didn‚Äôt fully catch what was going on? If so, try to type in e.g.:\n\nI'm new to R, please explain in simple terms, what the following code does:\n\n\"\nnucleotides <- c(\"a\", \"c\", \"g\", \"t\")\nreplicate(n = 10, expr = paste0(sample(nucleotides, size = 100, replace = TRUE), collapse = \"\"))\n\"\n\nNow, you could also ask a question like this:\n\nI am a student in the course \"R for Bio Data Science\" and we have been asked to solve the following problem:\n\n\"\nNow it is time to take it a step further. A) Create a function, which returns random RNA of length 'l' and B) another function, which performs reverse transcription.\n\"\n\nPlease give me the solution to the problem\n\nTry to run the code, you were presumable given. Likely, the tasks have been solved, but did you actually learn something? Do you understand the details of the code?\n\n\n\nStrawberry Fields Forever\nStart yet a new chat and run:\n\nHow many Rs are there in the word strawberry?\nHow many Rs are there in the word strawberries?\nWhat is the difference between strawberry and strawberries?\nTry as above, but using ‚Äòraspberry‚Äô and ‚Äòcranberry‚Äô\nInspect the answers, did ChatGPT get these simple questions right?\n\n\n\nSummary\nThe following is IMPORTANT, so please read carefully and please discuss in your group\nWhile chatGPT can be a powerful tool for code productivity, it comes with a major caveat: When it fails, it fails with confidence (See Strawberry Fields Forever above). This means that it will be equally confident whether it is right or wrong! The optimal yield is when you are working on a problem with a tool, where you already have a good knowledge of the problem and the tool. Here, you can use your experience to evaluate if you are heading in the right direction or if you‚Äôre being send on a wild goose chase. In this situation, ChatGPT can be a powerful sparring partner to augment your bio data science workflow to enhance productivity.\nKnow the difference between ‚Äúasking for a solution‚Äù and ‚Äúasking for an explanation‚Äù - Do not cheat yourself here, it is very important that the yield of this course is that you learn Tidyverse R and not how to ‚Äúuse ChatGPT to produced code‚Äù.\nTherefore, do not use chatGPT to solve the exercises, rather use ChatGPT as a learning assistant. Here is an example of the difference:\n\nWrong: Give me the tidyverse code for adding a new variable z to a dataset, which is the sum of x and y\nRight: In simple terms, explain which tidyverse function is used to add variables to datasets and give a very basic example of how it is used\n\nThe wrong way will give you the solution directly, robbing you of the learning process of internalising the functionality and the translation of the general explanation to the specific application to your problem. The right way, will allow you to work with your understanding of the problem and how to translate the general explation into a specific solution to your problem."
  },
  {
    "objectID": "lab02.html",
    "href": "lab02.html",
    "title": "Lab 2: Data Visualisation I",
    "section": "",
    "text": "ggplot2"
  },
  {
    "objectID": "lab02.html#schedule",
    "href": "lab02.html#schedule",
    "title": "Lab 2: Data Visualisation I",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Pre-course Survey Walk-through\n08.15 - 08.30: Recap: RStudio Cloud, RStudio and R - The Very Basics (Live session)\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab02.html#learning-materials",
    "href": "lab02.html#learning-materials",
    "title": "Lab 2: Data Visualisation I",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nBook: R4DS2e: Chapter 1 Data Visualisation\nPaper: ‚ÄúA Layered Grammar of Graphics‚Äù by Hadley Wickham\nVideo: The best stats you‚Äôve ever seen\nVideo: The SDGs aren‚Äôt the same old same old\nVideo: EMBL Keynote Lecture - ‚ÄúData visualization and data science‚Äù by Hadley Wickham"
  },
  {
    "objectID": "lab02.html#learning-objectives",
    "href": "lab02.html#learning-objectives",
    "title": "Lab 2: Data Visualisation I",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nExplains the basic theory of data visualisation\nDecipher the components of a simple ggplot\nUse ggplot to do basic data visualisation"
  },
  {
    "objectID": "lab02.html#sec-exercises",
    "href": "lab02.html#sec-exercises",
    "title": "Lab 2: Data Visualisation I",
    "section": "Exercises",
    "text": "Exercises"
  },
  {
    "objectID": "lab02.html#prelude",
    "href": "lab02.html#prelude",
    "title": "Lab 2: Data Visualisation I",
    "section": "Prelude",
    "text": "Prelude\nDiscuss these 4 visualisations with your group members\n\nWhat is problematic?\nWhat could be done to rectify?\n\n\n\n\nClick here for visualisation 1\n\n\nNote, AMR = Antimicrobial Resistance\n\n\n\n\n\nClick here for visualisation 2\n\n\n\n\n\n\n\nClick here for visualisation 3\n\n\n\n\n\n\n\nClick here for visualisation 4"
  },
  {
    "objectID": "lab02.html#getting-started",
    "href": "lab02.html#getting-started",
    "title": "Lab 2: Data Visualisation I",
    "section": "Getting Started",
    "text": "Getting Started\nFirst of all, make sure to read every line in these exercises carefully!\nIf you get stuck with Quarto, revisit R4DS2e, chapter 29 or take a look at the Comprehensive guide to using Quarto\n\nGo to the R for Bio Data Science RStudio Cloud Server session from last time and login and choose the project you created.\nCreate a new Quarto Document for todays exercises, e.g.¬†lab02_exercises.qmd\n\nRecall the layout of the IDE (Integrated Development Environment)\n\n\n\n\n\nThen, before we start, we need to fetch some data to work on.\n\nSee if you can figure out how to create a new folder called ‚Äúdata‚Äù, make sure to place it the same place as your my_project_name.Rproj file.\n\nThen, without further ado, run each of the following lines separately in your console:\n\ntarget_url <- \"https://github.com/ramhiser/datamicroarray/raw/master/data/gravier.RData\"\noutput_file <- \"data/gravier.RData\"\ncurl::curl_download(url = target_url,\n                    destfile = output_file)\n\n\nUsing the files pane, check the folder you created to see if you managed to retrieve the file.\n\nRecall the syntax for a new code chunk:\n  ```{r}\n  #| echo: true\n  #| eval: true\n  # Here goes the code... Note how this part does not get executed because of the initial hashtag, this is called a code-comment\n  1 + 1\n  my_vector <- c(1, 2, 3)\n  my_mean <- mean(my_vector)\n  print(my_mean)\n  ```\nIMPORTANT! You are mixing code and text in a Quarto Document! Anything within a ‚Äúchunk‚Äù as defined above will be evaluated as code, whereas anything outside the chunks is markdown. You can use shortcuts to insert new code chunks:\n\nMac: CMD + OPTION + i\nWindows: CTRL + ALT + i\n\nNote, this might not work, depending on your browser. In that case you can insert a new code chunk using  or You can change the shortcuts via ‚ÄúTools‚Äù > ‚ÄúModify Keyboard shortcuts‚Ä¶‚Äù > Filter for ‚ÄúInsert Chunk‚Äù and then choose the desired shortcut. E.g. change the shortcut for code chunks to Shift+Cmd+i or similar.\n\nAdd a new code chunk and use the load()-function to load the data you retrieved.\n\n\n\n\nClick here for a hint\n\n\nRemember, you can use ?load to get help on how the function works and remember your project root path is defined by the location of your .Rproj file, i.e.¬†the path. A path is simply where R can find your file, e.g.¬†/home/projects/r_for_bio_data_science/ or similar depending on your particular setup.\n\nNow, in the console, run the ls()-command and confirm, that you did indeed load the gravier data.\n\nRead the information about the gravier-data here\n\nNow, in your Quarto Document, add a new code chunk like so\n\nlibrary(\"tidyverse\")\n\nThis will load our data science toolbox, including ggplot."
  },
  {
    "objectID": "lab02.html#create-data",
    "href": "lab02.html#create-data",
    "title": "Lab 2: Data Visualisation I",
    "section": "Create data",
    "text": "Create data\nBefore we can visualise the data, we need to wrangle it a bit. Nevermind the details here, we will get to that later. Just create a new chunk, copy/paste the below code and run it:\n\nset.seed(676571)\ncancer_data=mutate(as_tibble(pluck(gravier,\"x\")),y=pluck(gravier,\"y\"),pt_id=1:length(pluck(gravier, \"y\")),age=round(rnorm(length(pluck(gravier,\"y\")),mean=55,sd=10),1))\ncancer_data=rename(cancer_data,event_label=y)\ncancer_data$age_group=cut(cancer_data$age,breaks=seq(10,100,by=10))\ncancer_data=relocate(cancer_data,c(pt_id,age,age_group,pt_id,event_label))\n\nNow we have the data set as an tibble, which is an augmented data frame (we will also get to that later):\n\ncancer_data\n\n# A tibble: 168 √ó 2,909\n   pt_id   age age_group event_label    g2E09    g7F07    g1A01   g3C09    g3H08\n   <int> <dbl> <fct>     <fct>          <dbl>    <dbl>    <dbl>   <dbl>    <dbl>\n 1     1  34.2 (30,40]   good        -0.00144 -0.00144 -0.0831  -0.0475  1.58e-2\n 2     2  47   (40,50]   good        -0.0604   0.0129  -0.00144  0.0104  3.16e-2\n 3     3  60.3 (60,70]   good         0.0398   0.0524  -0.0786   0.0635 -3.95e-2\n 4     4  57.8 (50,60]   good         0.0101   0.0314  -0.0218   0.0215  8.68e-2\n 5     5  54.9 (50,60]   good         0.0496   0.0201   0.0370   0.0311  2.07e-2\n 6     6  58.8 (50,60]   good        -0.0664   0.0468   0.00720 -0.370   2.88e-3\n 7     7  52.9 (50,60]   good        -0.00289 -0.0816  -0.0291  -0.0249 -1.74e-2\n 8     8  74.5 (70,80]   good        -0.198   -0.0499  -0.0634  -0.0298  3.00e-2\n 9     9  47.6 (40,50]   good         0.00288  0.0201   0.0272   0.0174 -7.89e-5\n10    10  55.8 (50,60]   good        -0.0574  -0.0574  -0.0831  -0.0897 -1.01e-1\n# ‚Ñπ 158 more rows\n# ‚Ñπ 2,900 more variables: g1A08 <dbl>, g1B01 <dbl>, g1int1 <dbl>, g1E11 <dbl>,\n#   g8G02 <dbl>, g1H04 <dbl>, g1C01 <dbl>, g1F11 <dbl>, g3F05 <dbl>,\n#   g3B09 <dbl>, g1int2 <dbl>, g2C01 <dbl>, g1A05 <dbl>, g1E01 <dbl>,\n#   g1B05 <dbl>, g3C05 <dbl>, g3A07 <dbl>, g1F01 <dbl>, g2D01 <dbl>,\n#   g1int3 <dbl>, g1int4 <dbl>, g1D05 <dbl>, g1E05 <dbl>, g1G05 <dbl>,\n#   g1C05 <dbl>, g1G11 <dbl>, g2D08 <dbl>, g2E06 <dbl>, g3H09 <dbl>, ‚Ä¶\n\n\n\nQ1: What is this data?\n\n\n\n\nClick here for a hint\n\n\nWhere did the data come from?\n\n\nQ2: How many rows and columns are there in the data set in total?\n\n\n\n\nClick here for a hint\n\n\nDo you think you are the first person in the world to try to find out how many rows and columns are in a data set in R?\n\n\nQ3: Which are the variables and which are the observations in relation to rows and columns?"
  },
  {
    "objectID": "lab02.html#ggplot---the-very-basics",
    "href": "lab02.html#ggplot---the-very-basics",
    "title": "Lab 2: Data Visualisation I",
    "section": "ggplot - The Very Basics",
    "text": "ggplot - The Very Basics\n\nGeneral Syntax\nThe general syntax for a basic ggplot is:\n\nggplot(data = my_data,\n       mapping = aes(x = variable_1_name,\n                     y = variable_2_name)) +\n  geom_something() +\n  labs()\n\nNote the + for adding layers to the plot\n\nggplot the plotting function\nmy_data the data you want to plot\naes() the mappings of your data to the plot\nx data for the x-axis\ny data for the y-axis\ngeom_something() the representation of your data\nlabs() the x-/y-labels, title, etc.\n\nNow:\n\nReivisit this illustration and discuss in your group what is what:\n\n\nA very handy ggplot cheat-sheet can be found here\n\n\nBasic Plots\nRemember to write notes in your rmarkdown document. You will likely revisit these basic plots in future exercises.\nPrimer: Plotting 2 x 20 random normally distributed numbers, can be done like so:\n\nggplot(data = tibble(x = rnorm(20),\n                     y = rnorm(20)),\n       mapping = aes(x = x,\n                     y = y)) +\n  geom_point()\n\n\n\n\nUsing this small primer, the materials you read for today and the cancer_data you created, in separate code-chunks, create a:\n\nT1: scatterplot of one variable against another\nT2: linegraph of one variable against another\nT3: boxplot of one variable (Hint: Set x = \"my_gene\" in aes())\nT4: histogram of one variable\nT5: densitogram of one variable\n\nRemember to write notes to yourself, so you know what you did and if there is something in particular you want to remember.\n\nQ4: Do all geoms require both x and y?\n\n\n\nExtending Basic Plots\n\nT6: Pick your favourite gene and create a boxplot of expression levels stratified on the variable event_label\nT7: Like T6, but with densitograms GROUP ASSIGNMENT\nT8: Pick your favourite gene and create a boxplot of expression levels stratified on the variable age_group\n\nThen, add stratification on event_label\nThen, add transparency to the boxes\nThen, add some labels\n\nT9: Pick your favourite gene and create a scatter-plot of expression levels versus age\n\nThen, add stratification on event_label\nThen, add a smoothing line\nThen, add some labels\n\nT10: Pick your favourite two genes and create a scatter-plot of their expression levels\n\nThen, add stratification on event_label\nThen, add a smoothing line\nThen, show split into seperate panes based on the variable age_group\nThen, add some labels\nChange the event_label title of the legend\n\nT11: Recreate the following plot\n\n\n\n\n\n\n\nQ5: Using your biological knowledge, what is your interpretation of the plot?\nT12: Recreate the following plot\n\n\n\n\n\n\n\nQ6: Using your biological knowledge, what is your interpretation of the plot?\nT13: If you arrive here and there is still time left for the exercises, you are probably already familiar with ggplot - Use what time is left to challenge yourself to further explore the cancer_data and create some nice data visualisations - Show me what you come up with!"
  },
  {
    "objectID": "lab02.html#further-ressources-for-data-visualisation",
    "href": "lab02.html#further-ressources-for-data-visualisation",
    "title": "Lab 2: Data Visualisation I",
    "section": "Further ressources for data visualisation",
    "text": "Further ressources for data visualisation\n\nA very handy ggplot cheat-sheet can be found here\nSo which plot to choose? Check this handy guide\nExplore ways of plotting here"
  },
  {
    "objectID": "lab03.html",
    "href": "lab03.html",
    "title": "Lab 3: Data Visualisation II",
    "section": "",
    "text": "ggplot2\npatchwork\nscales\nggridges"
  },
  {
    "objectID": "lab03.html#schedule",
    "href": "lab03.html#schedule",
    "title": "Lab 3: Data Visualisation II",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.45: Recap of Lab 2 and Lecture\n08.45 - 09.00: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab03.html#learning-materials",
    "href": "lab03.html#learning-materials",
    "title": "Lab 3: Data Visualisation II",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: R4DS2e: ‚ÄúVisualize‚Äù, chapters 9, 10 and 11\nVideo: William Chase | The Glamour of Graphics | RStudio (2020)\nWeb: Patchwork - Getting started\nWeb: Scales - Getting started"
  },
  {
    "objectID": "lab03.html#learning-objectives",
    "href": "lab03.html#learning-objectives",
    "title": "Lab 3: Data Visualisation II",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUse more advanced ggplot features\nCustomise the data visualisation\nCombine multiple plots into one pane\nLook at a more advanced ggplot and decipher the components used"
  },
  {
    "objectID": "lab03.html#sec-exercises",
    "href": "lab03.html#sec-exercises",
    "title": "Lab 3: Data Visualisation II",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nRead the steps of this exercises carefully, while completing them\n\nIntroduction\nSome authors are kind enough to supply the data they used for their paper, e.g.:\n\n‚ÄúAssessment of the influence of intrinsic environmental and geographical factors on the bacterial ecology of pit latrines‚Äù\n\nWhere the supporting data can be found here:\n\nhttp://userweb.eng.gla.ac.uk/umer.ijaz/bioinformatics/ecological.html\n\n\n\nGetting Started\nAgain, go to the R for Bio Data Science RStudio Cloud Server session from last time and login and choose the project you created\n\nCreate a new Quarto Document for todays exercises, e.g.¬†lab03_exercises.qmd\nNB! The Quarto document MUST be placed together with your .Rproj file (defining, the project root - look in your Files-tab) and also there, the data-folder should be placed!\nREMEMBER paths are important! Also, R is case-sensitive, i.e.¬†‚Äúdata‚Äù is not the same as ‚ÄúData‚Äù\n\nSee Paths and Projects\n\n\nGetting the data\nAdd a new code chunk and add the following code (Never mind the details, we will get back to this), remember you can use headers to nicely section your quarto Document.\n\nbase_url <- \"http://userweb.eng.gla.ac.uk/umer.ijaz/bioinformatics/ecological/\"\n\nSPE <- read_csv(file = str_c(base_url, \"SPE_pitlatrine.csv\"))\nwrite_csv(x = SPE,\n          file = \"data/SPE_pitlatrine.csv\")\n\nENV <- read_csv(file = str_c(base_url, \"ENV_pitlatrine.csv\"))\nwrite_csv(x = ENV,\n          file = \"data/ENV_pitlatrine.csv\")\n\nAdd the chunk settings #| echo: true and #| eval: true, then run the block and change the latter to #| eval: false.\n\nDiscuss in your group, what this means and why we do it\n\n\n\n\nClick here for hint\n\n\nFrom where do we retrieve the data and to where do we write it and what happens if we run the chunk more than one time?\n\n\n\nWrangling the data\n\nWhat is data wrangling?\n\nBefore we continue with plotting, we want to unify the data, so here again you will run some code, where the details are not important right now.\nBut‚Ä¶ Make sure, that you have run library(\"tidyverse\") somewhere in your Quarto document - Perhaps under an initial header saying ‚ÄúLoad Libraries‚Äù or similar?\n\nSPE |> \n  pivot_longer(cols = -Taxa,\n               names_to = \"Samples\",\n               values_to = \"OTU_Count\")  |> \n  full_join(ENV, by = \"Samples\") |> \n  mutate(site = case_when(str_detect(Samples, \"^T\") ~ \"Tanzania\",\n                          str_detect(Samples, \"^V\") ~ \"Vietnam\")) |>  \n  write_tsv(file = \"data/SPE_ENV.tsv\")\n\nChange the chunk settings as before"
  },
  {
    "objectID": "lab03.html#data-visualisation-ii",
    "href": "lab03.html#data-visualisation-ii",
    "title": "Lab 3: Data Visualisation II",
    "section": "Data Visualisation II",
    "text": "Data Visualisation II\n\nRead the data\n\nSPE_ENV <- read_tsv(file = \"data/SPE_ENV.tsv\")\nSPE_ENV\n\n# A tibble: 4,212 √ó 15\n   Taxa   Samples OTU_Count    pH  Temp    TS    VS   VFA  CODt  CODs perCODsbyt\n   <chr>  <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>      <dbl>\n 1 Acido‚Ä¶ T_2_1           0  7.82  25.1  14.5 71.3   71     874   311         36\n 2 Acido‚Ä¶ T_2_10          0  9.08  24.2  37.8 31.5    2     102     9          9\n 3 Acido‚Ä¶ T_2_12          0  8.84  25.1  71.1  5.94   1      35     4         10\n 4 Acido‚Ä¶ T_2_2           0  6.49  29.6  13.9 64.9    3.7   389   180         46\n 5 Acido‚Ä¶ T_2_3           0  6.46  27.9  29.4 26.8   27.5   161    35         22\n 6 Acido‚Ä¶ T_2_6           0  7.69  28.7  65.5  7.03   1.5    57     3          6\n 7 Acido‚Ä¶ T_2_7           0  7.48  29.8  36.0 34.1    1.1   107     9          8\n 8 Acido‚Ä¶ T_2_9           0  7.6   25    46.9 19.6    1.1    62     8         13\n 9 Acido‚Ä¶ T_3_2           0  7.55  28.8  12.6 51.8   30.9   384    57         15\n10 Acido‚Ä¶ T_3_3           0  7.68  28.9  14.6 48.1   24.2   372    57         15\n# ‚Ñπ 4,202 more rows\n# ‚Ñπ 4 more variables: NH4 <dbl>, Prot <dbl>, Carbo <dbl>, site <chr>\n\n\n\n\nIMPORTANT INSTRUCTIONS - READ!\nFor these exercises, you will have to identify what you see in the plot!\nFor each plot, complete the following steps\n\nLook at this overview of the components of a ggplot (see below)\nLook at the plot you are to recreate and discuss in the group:\n\nWhat is the data? Take a look at it and understand what is in the data\nWhat are the mappings? I.e. what variables are on the x-/y-axis?\nAre there any colour-/fill-mappings?\nWhat are the geoms used?\nAre there any modifications to theme?\n\n\n\n\n\nClick here for hint\n\n\n\nConsult the Data visualization with ggplot2 cheatsheet\nCheck which options you have available\nConsult the chapters in the book you read, see preparation materials for labs 2 and 3\n\n\n\n\n\nTASKS\n\nTask 1 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 2 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 3 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 4 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\nTask 5 - Recreate the following plots\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nSame data, but a transformation happened, changing the representation of the data. Look carefully at the axes.\n\n\n\nTask 6 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nSee if you can find something online on geom_smooth()\n\n\n\nTask 7 - Recreate the following plot\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nThink about fill and then see if you can find something online on geom_tile(), scale_fill_gradient2 and how to ggplot rotate axis labels\n\n\n\nTask 8 - Recreate the following plot\nStart by running this code in a new chunk (ignore details for now)\n\ntargets <- c(\"Methanobacteria\", \"Clostridia\", \"Actinobacteria\",\n            \"Sphingobacteria\", \"Anaerolineae\")\nSPE_ENV_targets <- SPE_ENV |>\n  filter(Taxa %in% targets)\n\nand then use the created dataset SPE_ENV_targets to recreate this plot:\nDiscuss in your group, which ggplot elements can you identify?\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nHere we need to use geom_density_ridges(), but which package contains this? Also we are using a colour scale called viridis, but how do we add this? Also, perhaps there are more themes we can use than just theme_classic()?\n\n\n\nTask 9 - GROUP ASSIGNMENT\nFor this assignment you and your group are to apply what you have learned in the two data visualisation labs. The task is to create a really nice plot using one of two datasets, the cancer_data or the SPE_ENV\nTry to play around with some custom colouring. There is a nice tool to aid in choosing colours for visualisations here\nBe sure to read the assignment instructions before submitting your solution."
  },
  {
    "objectID": "lab04.html",
    "href": "lab04.html",
    "title": "Lab 4: Data Wrangling I",
    "section": "",
    "text": "dplyr\nreadr\ntibble"
  },
  {
    "objectID": "lab04.html#schedule",
    "href": "lab04.html#schedule",
    "title": "Lab 4: Data Wrangling I",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap of Lab 3\n08.15 - 08.30: Assignment 2 walk-through\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab04.html#learning-materials",
    "href": "lab04.html#learning-materials",
    "title": "Lab 4: Data Wrangling I",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nBook: R4DS2e: Chapter 3 Data transformation\nBook: R4DS2e: Chapter 4 Workflow: Code Style\nBook: R4DS2e: Chapter 7 Data import\nBook: R4DS2e: Chapter 8 Workflow: getting help\nWeb: What is data wrangling? Intro, Motivation, Outline, Setup ‚Äì Pt. 1 Data Wrangling Introduction\nWeb: (NB! STOP at 7:45, i.e.¬†skip tidyr) Tidy Data and tidyr ‚Äì Pt 2 Intro to Data Wrangling with R and the Tidyverse\nWeb: Data Manipulation Tools: dplyr ‚Äì Pt 3 Intro to the Grammar of Data Manipulation with R"
  },
  {
    "objectID": "lab04.html#learning-objectives",
    "href": "lab04.html#learning-objectives",
    "title": "Lab 4: Data Wrangling I",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUnderstand and apply the 6 basic dplyr verbs: filter(), arrange(), select(), mutate(), summarise() and group_by()\nConstruct and apply logical statements in context with dplyr pipelines\nUnderstand and apply the additional verbs count(), drop_na(), View()\nCombine dplyr verbs to form a data manipulation pipeline using the pipe |> operator\nDecipher the components and functions hereof in a dplyr pipeline"
  },
  {
    "objectID": "lab04.html#sec-exercises",
    "href": "lab04.html#sec-exercises",
    "title": "Lab 4: Data Wrangling I",
    "section": "Exercises",
    "text": "Exercises\n\n\n\nImportant: As we progress, avoid using black-box do-everything-with-one-command R-packages like e.g.¬†ggpubr - I want you to learn the underlying technical bio data science! ‚Ä¶and why is that? Because if you use these type of packages, you will be limited to their functionality, whereas if you truly understand ggplot - The sky is the limit! Basically, it the clich√© that ‚ÄúIf you give a man a fish, you feed him for a day. If you teach a man to fish, you feed him for a lifetime‚Äù\n\nGetting Started\nUse this link to go to the R for Bio Data Science RStudio Cloud Server\nFirst things first:\n\nCreate a new Quarto Document for todays exercises, e.g.¬†lab04_exercises.qmd\n\nNote that: - The Quarto document MUST be placed together with your .Rproj file (defining, the project root - look in your Files-pane) and also there, the data-folder should be placed!\nUnderstanding how paths and projects work is important. In case this is no entirely clear, a new chapter has been added on Paths and Projects.\n\n\nBrief refresh of Quarto so far\nRecall the syntax for a new code chunk, where all your R code goes any text and notes must be outside the chunk tags:\n\n```{r}\n# Here the code goes\n1 + 1\nx <- c(1, 2, 3)\nmean(x)\n```\n\n[1] 2\n[1] 2\n\n\nOutside the code-chunks, go our markdown, e.g.:\n  # Header level 1\n  ## Header level 2\n  ### Header level 3\n  *A text in italics*\n  **A text in bold**\n  Normal text describing and explaining\nNow, in your new Quarto document, create a new Header level 2, e.g.:\n  ## Load Libraries\nand under your new header, add a new code chunk, like so\n\n```{r}\n#| message: false\nlibrary(\"tidyverse\")\n```\n\nAnd run the chunk. This will load our data science toolbox, including dplyr (and ggplot). But wait, what does the #| message: false-part do? ü§∑Ô∏è ü§î\nTry to structure your Rmarkdown document as your course notes, i.e.¬†add notes while solving the exercises aiming at creating a revisitable reference for your final project work\nBonus info: We are engineers, so of course, we love equations, we can include standard \\(\\LaTeX\\) syntax, e.g.:\n  $E(x) = \\frac{1}{n} \\cdot \\sum_{i=1}^{n} x_{i}$\nTry it!\n\n\nA few handy short cuts\nInsert new code chunk:\n\nMac: CTRL + OPTION + i\nWindows: CTRL + OPTION + i\n\nRender my Quarto document\n\nMac: CMD + SHIFT + k\nWin: CTRL + SHIFT + k\n\nRun line in chunk\n\nMac: CMD + ENTER\nWin: CTRL + ENTER\n\nRun entire chunk\n\nMac: CMD + SHIFT + ENTER\nWin: CTRL + SHIFT + ENTER\n\nInsert the pipe symbol |>\n\nMac: CMD + SHIFT + m\nWin: CTRL + SHIFT + m\n\nNote, if you‚Äôre trying this out and you see %>% instead of |>, then go to Tools, Global Options..., Code and check the box Use native pipeoperator"
  },
  {
    "objectID": "lab04.html#a-few-initial-questions",
    "href": "lab04.html#a-few-initial-questions",
    "title": "Lab 4: Data Wrangling I",
    "section": "A few initial questions",
    "text": "A few initial questions\nFirst, if you don‚Äôt feel completely comfortable with the group_by |> summarise-workflow, then no worries - Feel free to visit this short R Tutorial: Grouping and summarizing\nThen, in your groups, discuss the following primer questions. Note, when asked for ‚Äúwhat is the output‚Äù, do not run the code in the console, instead try to talk and think about it and write your answers and notes in your Quarto document for the day:\nFirst, in a new chunk, run tibble(x = c(4, 3, 5, 1, 2)), so you understand what it does, then - Discuss in your group, what is the output of, remember first talk, then check understanding by running code:\n\nQ1: tibble(x = c(4, 3, 5, 1, 2)) |> filter(x > 2)?\nQ2: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(x)?\nQ3: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(desc(x))?\nQ4: tibble(x = c(4, 3, 5, 1, 2)) |> arrange(desc(desc(x)))?\nQ5: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(x)?\nQ6: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(y)?\nQ7: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(-x)?\nQ8: tibble(x = c(4, 3, 5, 1, 2), y = c(2, 4, 3, 5, 1)) |> select(-x, -y)?\nQ9: tibble(x = c(4, 3, 5, 1, 2)) |> mutate(x_dbl = 2*x)?\nQ10: tibble(x = c(4, 3, 5, 1, 2)) |> mutate(x_dbl = 2 * x, x_qdr = 2*x_dbl)?\nQ11: tibble(x = c(4, 3, 5, 1, 2)) |> summarise(x_mu = mean(x))?\nQ12: tibble(x = c(4, 3, 5, 1, 2)) |> summarise(x_max = max(x))?\nQ13: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, NA, 5, 1, 2)) |> group_by(lbl) |> summarise(x_mu = mean(x), x_max = max(x))?\nQ14: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, 3, 5, 1, 2)) |> group_by(lbl) |> summarise(n = n())?\nQ15: tibble(lbl = c(\"A\", \"A\", \"B\", \"B\", \"C\"), x = c(4, 3, 5, 1, 2)) |> count(lbl)?\n\nIn the following, return to these questions and your answers for reference on the dplyr verbs!"
  },
  {
    "objectID": "lab04.html#load-data",
    "href": "lab04.html#load-data",
    "title": "Lab 4: Data Wrangling I",
    "section": "Load data",
    "text": "Load data\nAgain, add a new header to your Quarto document, e.g.¬†## Load Data, then:\n\nGo to the Vanderbilt Biostatistics Datasets site\nFind Diabetes data and download the diabetes.csv file\nYou should have a data-folder, if not, then in the Files pane, click the New Folder button, enter folder name data and click ok\nNow, click on the folder you created\nClick the  Upload-button and navigate to the diabetes.csv file you downloaded\nClicking the two dots .. above the file you uploaded, look for  .., will take you one level up in your project path\nInsert a new code chunk in your Quarto document\nAdd and then run the following code\n\n\ndiabetes_data <- read_csv(file = \"data/diabetes.csv\")\ndiabetes_data\n\nThen realise that we could simply have run the following code to do the exact same thing (Yes, readr is pretty nifty):\n\n# Create the data directory programmatically\ndir_create(x = \"data\")\n\n# Retrieve the data directly\ndiabetes_data <- read_csv(file = \"https://hbiostat.org/data/repo/diabetes.csv\")\n\n# Write the data to disk\nwrite_csv(x = diabetes_data,\n          file = \"data/diabetes.csv\")\n\nJust remember the echo/eval trick from last session to avoid retrieving online data each time you render your Quarto document"
  },
  {
    "objectID": "lab04.html#work-with-the-diabetes-data-set",
    "href": "lab04.html#work-with-the-diabetes-data-set",
    "title": "Lab 4: Data Wrangling I",
    "section": "Work with the diabetes data set",
    "text": "Work with the diabetes data set\nUse the pipe |> to use the View()-function to inspect the data set. Note, if you click the -button, you will get a spreadsheet-like view of the data, allowing you to get an overview.\n\nQ1: How many observations and how many variables?\nQ2: Is this a tidy data set? Which three rules must be satisfied?\nQ3: When you run the chunk, then underneath each column name is stated <chr> and <dbl> what is that?\n\nBefore we continue\n\nT1: Change the height, weight, waist and hip from inches/pounds to the metric system (cm/kg), rounding to 1 decimal\n\n\n\n\nLet us try to take a closer look at the data by various subsetting (How many‚Ä¶ is equal to the number of rows in the subset of the data you created):\n\nQ4: How many weigh less than 100kg?\nQ5: How many weigh more than 100kg?\nQ6: How many weigh more than 100kg and are less than 1.6m tall?\nQ7: How many women are taller than 1.8m?\nQ8: How many men are taller than 1.8m?\nQ9: How many women in Louisa are older than 30?\nQ10: How many men in Buckingham are younger than 30 and taller than 1.9m?\nT2: Make a scatter plot of weight versus height and colour by sex for inhabitants of Louisa above the age of 40\nT3: Make a boxplot of height versus location stratified on sex for people above the age of 50\n\nSorting columns can aid in getting an overview of variable ranges (don‚Äôt use the summary function yet for this one)\n\nQ11: How old is the youngest person?\nQ12: How old is the oldest person?\nQ13: Of all the 20-year olds, what is the height of the tallest?\nQ14: Of all the 20-year olds, what is the height of the shortest?\n\nChoosing specific columns can be used to work with a subset of the data for a specific purpose\n\nQ15: How many columns (variables) starts_with a ‚Äúb‚Äù?\nQ16: How many columns (variables) contains the word ‚Äúeight‚Äù?\n\nCreating new variables is an integral part of data manipulation\n\nT4: Create a new variable, where you calculate the BMI\n\n\n\n\n\nT5: Create a BMI_class variable\n\nTake a look at the following code snippet to get you started:\n\ntibble(x = rnorm(10)) |> \n  mutate(trichotomised = case_when(\n    x < -1 ~ \"Less than -1\",\n    -1 <= x & x < 1 ~ \"larger than or equal to -1 and smaller than 1\",\n    1 <= x ~ \"Larger than or equal to 1\"))\n\nand then go read about BMI classification here and discuss in your group how to extract classifications from the Definition/Introduction section\nNote, the cut()-function could be used here, but you should try to use case_when() as illustrated in the example chunk above.\n\n\n\nOnce you have created the variable, you will need to convert it to a categorical variable, in R, these are called a factor and you can set the levels like so:\n\ndiabetes_data <- diabetes_data |>\n  mutate(BMI_class = factor(BMI_class,\n                            levels =  c(\"my 1st category\", \"my 2nd category\",\n                                        \"my 3rd category\", \"my nth category\")))\n\nThis is very important for plotting, as this will determine the order in which the categories appear on the plot!\n\nT6: Create a boxplot of hdl versus BMI_class\nQ17: What do you see?\nT7: Create a BFP (Body fat percentage) variable\n\n\n\n\nClick here for hint\n\n\nBFP can be calculated usin the below equation source:\n\\[BFP = 1.39 \\cdot BMI + 0.16 \\cdot age - 10.34 \\cdot sex - 9\\]\nWhere \\(sex\\) is defined as being \\(0\\) for female and \\(1\\) for male.\n\n\n\n\n\nT8: Create a WHR (waist-to-hip ratio) variable\nQ18: Which correlate better with BMI, WHR or BFP? GROUP ASSIGNMENT\n\n\n\n\nClick here for hint\n\n\nIs there a certain plot-type, which can visualise if the relationship between two variables and give insights to if they are correlated? Can you perhaps use an R-function to compute the ‚Äúcorrelation coefficient‚Äù?. Do not use e.g.¬†ggpubr, use only tidyverse and base)\n\nNow, with this augmented data set, let us create some summary statistics\n\nQ19: How many women and men are there in the data set?\nQ20: How many women and men are there from Buckingham and Louisa respectively in the data set?\nQ21: How many are in each of the BMI_class groups?\nQ22: Given the code below, explain the difference between A and B?\n\n\n# A\ndiabetes_data |>\n  ggplot(aes(x = BMI_class)) +\n  geom_bar()\n\n# B\ndiabetes_data |>\n  count(BMI_class) |>\n  ggplot(aes(x = BMI_class, y = n)) +\n  geom_col()\n\n\nT9: For each BMI_class group, calculate the average weight and associated standard deviation\nQ23: What was the average age of the women living in Buckingham in the study?\n\nFinally, if you reach this point and there is still time left. Take some time to do some exploratory plots of the data set and see if you can find something interesting."
  },
  {
    "objectID": "lab05.html",
    "href": "lab05.html",
    "title": "Lab 5: Data Wrangling II",
    "section": "",
    "text": "dplyr\nstringr\ntidyr\nforcats\npatchwork\nggseqlogo\ntable1"
  },
  {
    "objectID": "lab05.html#schedule",
    "href": "lab05.html#schedule",
    "title": "Lab 5: Data Wrangling II",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.30: Recap of Lab 4\n08.30 - 08.35: Lecture\n08.35 - 08.45: Break\n08.45 - 12.00: Exercises"
  },
  {
    "objectID": "lab05.html#learning-materials",
    "href": "lab05.html#learning-materials",
    "title": "Lab 5: Data Wrangling II",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: R4DS2e: Chapter 5 Data tidying\nBook: R4DS2e: Chapter 14 Strings\nBook: R4DS2e: Chapter 16 Factors\nBook: Chapter 19 Joins\nVideo: Tidy Data and tidyr - NB! Start at 7:45 and please note: gather() is now pivot_longer() and spread() is now pivot_wider()\nVideo: Working with Two Datasets: Binds, Set Operations, and Joins\nVideo: stringr (Playlist with 7 short videos)"
  },
  {
    "objectID": "lab05.html#learning-objectives",
    "href": "lab05.html#learning-objectives",
    "title": "Lab 5: Data Wrangling II",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nUnderstand and apply the various str_*()-functions for string manipulation\nUnderstand and apply the family of *_join()-functions for combining data sets\nUnderstand and apply pivot_wider() and pivot_longer()\nUse factors in context with plotting categorical data using ggplot"
  },
  {
    "objectID": "lab05.html#sec-exercises",
    "href": "lab05.html#sec-exercises",
    "title": "Lab 5: Data Wrangling II",
    "section": "Exercises",
    "text": "Exercises\n\nPrologue\nToday will not be easy! But please try to remember Hadley‚Äôs word-of-advise:\n\n‚ÄúThe bad news is, whenever you‚Äôre learning a new tool, for a long time, you‚Äôre going to suck! It‚Äôs gonna be very frustrating! But the good news is that that is typical and something that happens to everyone and it‚Äôs only temporary! Unfortunately, there is no way to going from knowing nothing about the subject to knowing something about a subject and being an expert in it without going through a period of great frustration and much suckiness! Keep pushing through!‚Äù - H. Wickham (dplyr tutorial at useR 2014, 4:10 - 4:48)"
  },
  {
    "objectID": "lab05.html#intro",
    "href": "lab05.html#intro",
    "title": "Lab 5: Data Wrangling II",
    "section": "Intro",
    "text": "Intro\nWe are upping the game here, so expect to get stuck at some of the questions. Remember - Discuss with your group how to solve the task, revisit the materials you prepared for today and naturally, the TAs and I are happy to nudge you in the right direction. Finally, remember‚Ä¶ Have fun!\nRemember what you have worked on so far:\n\nRStudio\nQuarto\nggplot\nfilter\narrange\nselect\nmutate\ngroup_by\nsummarise\nThe pipe and creating pipelines\nstringr\njoining data\npivotting data\n\nThat‚Äôs quite a lot! Well done - You‚Äôve come quite far already! Remember to think about the above tools in the following as we will synthesise your learnings so far into an analysis!"
  },
  {
    "objectID": "lab05.html#sec-background",
    "href": "lab05.html#sec-background",
    "title": "Lab 5: Data Wrangling II",
    "section": "Background",
    "text": "Background\nIn the early 20s, the world was hit by the coronavirus disease 2019 (COVID-19) pandemic. The pandemic was caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). In Denmark the virus first confirmed case was on 27 February 2020.\nWhile initially very little was known about the SARS-CoV-2 virus, we did know the general pathology of vira. Briefly, the virus invades the cells and hijacks the intra-cellular machinery. Using the hijacked machinery, components for new virus particles are produced, eventually being packed into the viral envelope and released from the infected cell. Some of these components, viral proteins, is broken down into smaller fragments called peptides by the proteasome. These peptides are transported into the endoplasmatic reticulum by the Transporter Associated with antigen Processing (TAP) protein complex. Here, they are aided by chaperones bound to the Major Histocompatilibty Complex class I (MHCI) and then across the Golgi Aparatus they finally get displayed on the surface of the cells. Note, in humans, MHC is also called Human Leukocyte Antigen (HLA) and represents the most diverse genes. Each of us have a total of 6 HLA-alleles, 3 from the maternal and 3 from the paternal side. These are further divided into 3 classes HLA-A, HLA-B and HLA-C and the combination of these constitute the HLA-haplotype for an individual. Once the peptide is bound to the MHC Class I at the cell surface and exposed, the MHCI-peptide complex can be recognised by CD8+ Cytotoxic T-Lymphocytes (CTLs) via the T-cell Receptor (TCR). If a cell displays peptides of viral origin, the CTL gets activated and via a cascade induces apoptosis (programmed cell death) of the infected cell. The proces is summarised in the figure below.\n\n\n\n\n\nImage source: 10.3389/fmicb.2015.00021\nThe data we will be working with today contains data on sequenced T-cell receptors, viral antigens, HLA-haplotypes and clinical meta data for a cohort:\n\nA large-scale database of T-cell receptor beta (TCR\\(\\beta\\)) sequences and binding associations from natural and synthetic exposure to SARS-CoV-2"
  },
  {
    "objectID": "lab05.html#your-task-today",
    "href": "lab05.html#your-task-today",
    "title": "Lab 5: Data Wrangling II",
    "section": "Your Task Today",
    "text": "Your Task Today\nToday, we will emulate the situation, where you are working as a Bioinformatician / Bio Data Scientist and you have been given the data and the task of answering these two burning questions:\n\nWhat characterises the peptides binding to the HLAs?\nWhat characterises T-cell Receptors binding to the pMHC-complexes?\n\nGROUP ASSIGNMENT: Today, your assignment will be to create a micro-report on these 2 questions!\nMAKE SURE TO READ THE LAST SECTION ON THE ASSIGNMENT"
  },
  {
    "objectID": "lab05.html#getting-started",
    "href": "lab05.html#getting-started",
    "title": "Lab 5: Data Wrangling II",
    "section": "Getting Started",
    "text": "Getting Started\n\nClick here to go to the course RStudio cloud server and login\nMake sure you are in your r_for_bio_data_science-project, you can verify this in the upper right corner\nIn the same place as your r_for_bio_data_science.Rproj-file and existing data-folder, create a new folder and name it doc\nGo to the aforementioned manuscript. Download the PDF and upload it to your new doc-folder\nOpen the PDF and find the link to the data\nGo to the data site (Note, you may have to create and account to download, shouldn‚Äôt take too long) . Find and download the file ImmuneCODE-MIRA-Release002.1.zip (CAREFUL, do not download the superseded files)\nUnpack the downloaded file\nFind the files peptide-detail-ci.csv and subject-metadata.csv and compress to .zip-files\nUpload the compressed peptide-detail-ci.csv.zip- and subject-metadata.csv.zip-files to your data-folder in your RStudio Cloud session\nFinally, once again, create a new Quarto document for today‚Äôs exercises, containing the sections:\n\nBackground\nAim\nLoad Libraries\nLoad Data\nData Description\nAnalysis"
  },
  {
    "objectID": "lab05.html#creating-the-micro-report",
    "href": "lab05.html#creating-the-micro-report",
    "title": "Lab 5: Data Wrangling II",
    "section": "Creating the Micro-Report",
    "text": "Creating the Micro-Report\n\nBackground\nFeel free to copy paste the one stated in the background-section above\n\n\nAim\nState the aim of the micro-report, i.e.¬†what are the questions you are addressing?\n\n\nLoad Libraries\n\n\n\nLoad the libraries needed\n\n\nLoad Data\nRead the two data sets into variables peptide_data and meta_data.\n\n\n\nClick here for hint\n\n\nThink about which Tidyverse package deals with reading data and what are the file types we want to read here?\n\n\n\n\n\n\nData Description\nIt is customary to include a description of the data, helping the reader if the report, i.e.¬†your stakeholder, to get an easy overview\n\nThe Subject Meta Data\nLet‚Äôs take a look at the meta data:\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 30\n   Experiment Subject `Cell Type` `Target Type` Cohort          Age Gender Race \n   <chr>        <dbl> <chr>       <chr>         <chr>         <dbl> <chr>  <chr>\n 1 eHO134         178 PBMC        C19_cI        COVID-19-Con‚Ä¶    36 M      White\n 2 eNL187        2686 B-CD8-_PBMC C19_cII       COVID-19-Con‚Ä¶    NA <NA>   <NA> \n 3 eHO140        1809 PBMC        C19_cI        COVID-19-Con‚Ä¶    NA <NA>   <NA> \n 4 eJL149        3911 PBMC        C19_cI        COVID-19-Con‚Ä¶    60 F      <NA> \n 5 eQD128        1499 PBMC        C19_cI        COVID-19-Con‚Ä¶    53 F      Asian\n 6 eQD138         242 PBMC        C19_cII       COVID-19-Con‚Ä¶    NA <NA>   <NA> \n 7 eJL152        7842 PBMC        C19_cI        COVID-19-Con‚Ä¶    41 F      <NA> \n 8 eNL189        1566 B-CD8-_PBMC C19_cII       COVID-19-Exp‚Ä¶    NA <NA>   <NA> \n 9 eLH45         3739 PBMC        C19_cI        COVID-19-Con‚Ä¶    53 M      <NA> \n10 eQD108      500001 PBMC        C19_cI        COVID-19-Con‚Ä¶    NA <NA>   <NA> \n# ‚Ñπ 22 more variables: `HLA-A...9` <chr>, `HLA-A...10` <chr>,\n#   `HLA-B...11` <chr>, `HLA-B...12` <chr>, `HLA-C...13` <chr>,\n#   `HLA-C...14` <chr>, DPA1...15 <chr>, DPA1...16 <chr>, DPB1...17 <chr>,\n#   DPB1...18 <chr>, DQA1...19 <chr>, DQA1...20 <chr>, DQB1...21 <chr>,\n#   DQB1...22 <chr>, DRB1...23 <chr>, DRB1...24 <chr>, DRB3...25 <chr>,\n#   DRB3...26 <chr>, DRB4...27 <chr>, DRB4...28 <chr>, DRB5...29 <chr>,\n#   DRB5...30 <chr>\n\n\n\nQ1: How many observations of how many variables are in the data?\nQ2: Are there groupings in the variables, i.e.¬†do certain variables ‚Äúgo together‚Äù somehow?\nT1: Re-create this plot\n\nRead this first:\n\nThink about: What is on the x-axis? What is on the y-axis? And also, it looks like we need to do some counting. Recall, that we can stick together a dplyr-pipeline with a call to ggplot, so here we will have to count of Cohort and Gender before plotting\n\n\n\n\n\n\nDoes your plot look different somehow? Consider peeking at the hint‚Ä¶\n\n\n\nClick here for hint\n\n\nPerhaps not everyone agrees on how to denote NAs in data. I have seen -99, -11, _ and so on‚Ä¶ Perhaps this can be dealt with in the instance we read the data from the file? I.e. in the actual function call to your read-function. Recall, how can we get information on the parameters of a ?function\n\n\nT2: Re-create this plot\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nPerhaps there is a function, which can cut continuous observations into a set of bins?\n\n\nSTOP! Make sure you handled how NAs are denoted in the data before proceeding, see hint below T1\n\nT3: Look at the data and create yet another plot as you see fit. Also skip the redundant variables Subject, Cell Type and Target Type\n\n\n\n\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 27\n   Experiment Cohort      Age Gender Race  `HLA-A...9` `HLA-A...10` `HLA-B...11`\n   <chr>      <chr>     <dbl> <chr>  <chr> <chr>       <chr>        <chr>       \n 1 eHO125     COVID-19‚Ä¶    52 M      <NA>  A*02:01:01  A*02:01:01   B*39:01:01  \n 2 eJL147     Healthy ‚Ä¶    40 M      Mixe‚Ä¶ A*02:01     A*11:01      B*07:06     \n 3 eHO131     COVID-19‚Ä¶    58 F      <NA>  A*02:01:01  A*02:01:01   B*15:01:01  \n 4 eQD121     COVID-19‚Ä¶    38 M      <NA>  A*01:01:01  A*24:02:01   B*18:01:01  \n 5 eAM23      COVID-19‚Ä¶    48 M      <NA>  A*11:01:01  A*24:02:01   B*15:01:01  \n 6 eXL31      Healthy ‚Ä¶    28 M      White A*02:01     A*29:02      B*07:02     \n 7 eOX54      Healthy ‚Ä¶    39 F      Afri‚Ä¶ A*02:01     A*23:17      B*15:03     \n 8 eHH169     Healthy ‚Ä¶    24 F      Blac‚Ä¶ A*02:01     A*74:01      B*35:01     \n 9 eLH58      COVID-19‚Ä¶    NA <NA>   <NA>  A*01:01:01  A*02:01:01   B*40:01:02  \n10 eQD129     COVID-19‚Ä¶    60 F      White A*02:01:01  A*02:01:01   B*44:02:01  \n# ‚Ñπ 19 more variables: `HLA-B...12` <chr>, `HLA-C...13` <chr>,\n#   `HLA-C...14` <chr>, DPA1...15 <chr>, DPA1...16 <chr>, DPB1...17 <chr>,\n#   DPB1...18 <chr>, DQA1...19 <chr>, DQA1...20 <chr>, DQB1...21 <chr>,\n#   DQB1...22 <chr>, DRB1...23 <chr>, DRB1...24 <chr>, DRB3...25 <chr>,\n#   DRB3...26 <chr>, DRB4...27 <chr>, DRB4...28 <chr>, DRB5...29 <chr>,\n#   DRB5...30 <chr>\n\n\nNow, a classic way of describing a cohort, i.e.¬†the group of subjects used for the study, is the so-called table1 and while we could build this ourselves, this one time, in the interest of exercise focus and time, we are going to ‚Äúcheat‚Äù and use an R-package, like so:\nNB!: This may look a bit odd initially, but if you render your document, you should be all good!\n\nlibrary(\"table1\") # <= Yes, this should normally go at the beginning!\nmeta_data |>\n  mutate(Gender = factor(Gender),\n         Cohort = factor(Cohort)) |>\n  table1(x = formula(~ Gender + Age + Race | Cohort),\n         data = _)\n\n\n\n\n\n\nCOVID-19-Acute(N=4)\nCOVID-19-B-Non-Acute(N=8)\nCOVID-19-Convalescent(N=90)\nCOVID-19-Exposed(N=3)\nHealthy (No known exposure)(N=39)\nOverall(N=144)\n\n\n\n\nGender\n\n\n\n\n\n\n\n\nF\n1 (25.0%)\n4 (50.0%)\n33 (36.7%)\n1 (33.3%)\n17 (43.6%)\n56 (38.9%)\n\n\nM\n2 (50.0%)\n3 (37.5%)\n36 (40.0%)\n0 (0%)\n21 (53.8%)\n62 (43.1%)\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n21 (23.3%)\n2 (66.7%)\n1 (2.6%)\n26 (18.1%)\n\n\nAge\n\n\n\n\n\n\n\n\nMean (SD)\n50.7 (17.0)\n43.7 (7.74)\n51.5 (15.3)\n35.0 (NA)\n33.3 (9.93)\n44.9 (15.7)\n\n\nMedian [Min, Max]\n52.0 [33.0, 67.0]\n42.0 [33.0, 53.0]\n53.0 [21.0, 79.0]\n35.0 [35.0, 35.0]\n31.0 [21.0, 62.0]\n42.0 [21.0, 79.0]\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n21 (23.3%)\n2 (66.7%)\n0 (0%)\n25 (17.4%)\n\n\nRace\n\n\n\n\n\n\n\n\nAfrican American\n1 (25.0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (2.6%)\n2 (1.4%)\n\n\nWhite\n2 (50.0%)\n7 (87.5%)\n13 (14.4%)\n0 (0%)\n28 (71.8%)\n50 (34.7%)\n\n\nAsian\n0 (0%)\n0 (0%)\n3 (3.3%)\n0 (0%)\n2 (5.1%)\n5 (3.5%)\n\n\nHispanic or Latino/a\n0 (0%)\n0 (0%)\n1 (1.1%)\n0 (0%)\n0 (0%)\n1 (0.7%)\n\n\nNative Hawaiian or Other Pacific Islander\n0 (0%)\n0 (0%)\n0 (0%)\n1 (33.3%)\n0 (0%)\n1 (0.7%)\n\n\nBlack or African American\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n3 (7.7%)\n3 (2.1%)\n\n\nMixed Race\n0 (0%)\n0 (0%)\n0 (0%)\n0 (0%)\n1 (2.6%)\n1 (0.7%)\n\n\nMissing\n1 (25.0%)\n1 (12.5%)\n73 (81.1%)\n2 (66.7%)\n4 (10.3%)\n81 (56.3%)\n\n\n\n\n\n\nNote how good this looks! If you have ever done a ‚ÄúTable 1‚Äù before, you know how painful they can be and especially if something changes in your cohort - Dynamic reporting to the rescue!\nLastly, before we proceed, the meta_data contains HLA data for both class I and class II (see background), but here we are only interested in class I, recall these are denoted HLA-A, HLA-B and HLA-C, so make sure to remove any non-class I, i.e.¬†the one after, denoted D-something.\n\nT4: Create a new version of the meta_data, which with respect to allele-data only contains information on class I and also fix the odd naming, e.g.¬†HLA-A...9 becomes A1 oand HLA-A...10 becomes A2 and so on for B1, B2, C1 and C2 (Think: How can we rename variables? And here, just do it ‚Äúmanually‚Äù per variable). Remember to assign this new data to the same meta_data-variable\n\n\n\n\nClick here for hint\n\n\nWhich tidyverse function subsets variables? Perhaps there is a function, which somehow matches a set of variables? And perhaps for the initiated this is compatible with regular expressions (If you don‚Äôt know what this means - No worries! If you do, see if you utilise this to simplify your variable selection)\n\n\n\n\nBefore we proceed, this is the data we will carry on with:\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 11\n   Experiment Cohort        Age Gender Race  A1    A2    B1    B2    C1    C2   \n   <chr>      <chr>       <dbl> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 ePD82      COVID-19-C‚Ä¶    60 F      <NA>  \"A*2‚Ä¶ \"A*3‚Ä¶ \"B*4‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*1‚Ä¶\n 2 eLH50      COVID-19-C‚Ä¶    28 M      <NA>  \"A*0‚Ä¶ \"A*2‚Ä¶ \"B*1‚Ä¶ \"B*2‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 3 eQD136     COVID-19-C‚Ä¶    NA <NA>   <NA>  \"A*0‚Ä¶ \"A*6‚Ä¶ \"B*0‚Ä¶ \"B*1‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 4 eJL151     COVID-19-C‚Ä¶    79 F      <NA>  \"A*2‚Ä¶ \"A*6‚Ä¶ \"B*1‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 5 eJL146     Healthy (N‚Ä¶    30 M      White \"A*0‚Ä¶ \"A*3‚Ä¶ \"B*1‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 6 eJL161     COVID-19-C‚Ä¶    31 F      White \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*0‚Ä¶ \"B*1‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 7 eQD127     COVID-19-C‚Ä¶    61 F      <NA>  \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*2‚Ä¶ \"B*3‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 8 eQD131     COVID-19-E‚Ä¶    NA <NA>   <NA>  \"A*0‚Ä¶ \"A*3‚Ä¶ \"B*1‚Ä¶ \"B*5‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 9 eMR25      COVID-19-C‚Ä¶    21 F      <NA>  \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n10 eQD117     COVID-19-C‚Ä¶    70 F      <NA>  \"A*0‚Ä¶ \"A*2‚Ä¶ \"B*3‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n\n\nNow, we have a beautiful tidy-dataset, recall that this entails, that each row is an observation, each column is a variable and each cell holds one value.\n\n\n\nThe Peptide Details Data\nLet‚Äôs start with simply having a look see:\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   `TCR BioIdentity`            TCR Nucleotide Seque‚Ä¶¬π Experiment `ORF Coverage`\n   <chr>                        <chr>                  <chr>      <chr>         \n 1 CASRETGLGNQPQHF+TCRBV02-01+‚Ä¶ CGGTCCACAAAGCTGGAGGAC‚Ä¶ eOX52      ORF1ab        \n 2 CASSLGLGNTGELFF+TCRBV19-01+‚Ä¶ ACATCGGCCCAAAAGAACCCG‚Ä¶ ePD85      ORF3a         \n 3 CASSGSGTGYEQYF+TCRBV09-01+T‚Ä¶ CTGAGCTCTCTGGAGCTGGGG‚Ä¶ eQD137     ORF1ab        \n 4 CASSIGTGRSYEQYF+TCRBV19-01+‚Ä¶ ACATCGGCCCAAAAGAACCCG‚Ä¶ ePD83      ORF3a         \n 5 CASSYGSSYEQYF+TCRBV06-05+TC‚Ä¶ NNNNNNNTGTCGGCTGCTCCC‚Ä¶ eOX52      nucleocapsid ‚Ä¶\n 6 CASSGGTGELFF+TCRBV09-01+TCR‚Ä¶ CTAAACCTGAGCTCTCTGGAG‚Ä¶ eEE226     ORF1ab        \n 7 CASSLGVSSNQPQHF+TCRBV07-09+‚Ä¶ CAGCGCACAGAGCAGGGGGAC‚Ä¶ eOX46      surface glyco‚Ä¶\n 8 CASSLWAAYNEQFF+TCRBV27-01+T‚Ä¶ CTGGAGTCGCCCAGCCCCAAC‚Ä¶ eOX46      ORF7b         \n 9 CASSLAGGLGNEKLFF+TCRBV05-04‚Ä¶ GCCTTGGAGCTGGACGACTCG‚Ä¶ eOX46      ORF7b         \n10 CASSLVGATKNIQYF+TCRBV07-02+‚Ä¶ CAGCGCACAGAGCAGGAGGAC‚Ä¶ eOX54      ORF1ab        \n# ‚Ñπ abbreviated name: ¬π‚Äã`TCR Nucleotide Sequence`\n# ‚Ñπ 3 more variables: `Amino Acids` <chr>, `Start Index in Genome` <dbl>,\n#   `End Index in Genome` <dbl>\n\n\n\nQ3: How many observations of how many variables are in the data?\n\nThis is a rather big data set, so let us start with two ‚Äútricks‚Äù to handle this, first:\n\nWrite the data back into your data-folder, using the filename peptide-detail-ci.csv.gz, note the appending of .gz, which is automatically recognised and results in gz-compression\nNow, check in your data folder, that you have two files peptide-detail-ci.csv and peptide-detail-ci.csv.gz, delete the former\nAdjust your reading-the-data-code in the ‚ÄúLoad Data‚Äù-section, to now read in the peptide-detail-ci.csv.gz-file\n\n\n\n\nClick here for hint\n\n\nJust as you can read a file, you can of course also write a file. Note the filetype we want to write here is csv. If you in the console type e.g.¬†readr::wr and then hit the tab-button, you will see the different functions for writing different filetypes\n\nThen:\n\nT5: As before, let‚Äôs immediately subset the peptide_data to the variables of interest: TCR BioIdentity, Experiment and Amino Acids. Remember to assign this new data to the same peptide_data-variable to avoid cluttering your environment with redundant variables. Bonus: Did you know you can click the Environment pane and see which variables you have?\n\n\n\n\nOnce again, before we proceed, this is the data we will carry on with:\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 3\n   Experiment `TCR BioIdentity`                         `Amino Acids`           \n   <chr>      <chr>                                     <chr>                   \n 1 eAV88      CSARGRNQPQHF+TCRBV20-X+TCRBJ01-05         DFLEYHDVR,EDFLEYHDVR,LE‚Ä¶\n 2 eQD132     CASSPSTLGGTDTQYF+TCRBV07-06+TCRBJ02-03    FGEVFNATRF,FNATRFASVY,G‚Ä¶\n 3 eHO141     unproductive+TCRBV07-09+TCRBJ02-02        YLQPRTFL,YLQPRTFLL,YYVG‚Ä¶\n 4 eHO134     CASSQDPGGENEQFF+TCRBV04-03+TCRBJ02-01     FLQSINFVR,FLQSINFVRI,FL‚Ä¶\n 5 ePD87      CASSRTHRQGRNTDTQYF+TCRBV18-01+TCRBJ02-03  LSPRWYFYY,SPRWYFYYL     \n 6 eHH173     CASSQSSGLAGDTQYF+TCRBV04-03+TCRBJ02-03    APAHISTI,LIVNSVLLFL,LLF‚Ä¶\n 7 eOX49      CASSLGWGFNEQFF+TCRBV05-04+TCRBJ02-01      VLWAHGFEL               \n 8 eXL30      CASSPNGYEQYF+TCRBV07-03+TCRBJ02-07        FPNITNLCPF,QPTESIVRF,RF‚Ä¶\n 9 eOX54      CASSKPRASGRRGPYEQYF+TCRBV21-01+TCRBJ02-07 KLNVGDYFV               \n10 eMR23      CSARRTGANTEAFF+TCRBV20-X+TCRBJ01-01       ALNTPKDHI,ATEGALNTPK    \n\n\n\nQ4: Is this tidy data? Why/why not?\nT6: See if you can find a way to create the below data, from the above\n\n\n\n\n\npeptide_data |> \n  sample_n(size = 10)\n\n# A tibble: 10 √ó 5\n   Experiment CDR3b            V_gene           J_gene     `Amino Acids`        \n   <chr>      <chr>            <chr>            <chr>      <chr>                \n 1 eHO134     CASSLNGDMPYGYTF  TCRBV27-01       TCRBJ01-02 HTTDPSFLGRY          \n 2 eOX52      CSADQSGGAVDEQFF  TCRBV20-X        TCRBJ02-01 KLSYGIATV            \n 3 eEE240     CASSLVRQNTEAFF   TCRBV12-X        TCRBJ01-01 TVLSFCAFA,VLSFCAFAV  \n 4 eEE226     CASSSEGQGRAYEQYF TCRBV07-09       TCRBJ02-07 FKVSIWNLDY,ILLIIMRTF‚Ä¶\n 5 eOX52      CATSDFSGQETQYF   TCRBV24-01       TCRBJ02-05 FVDGVPFVV            \n 6 eEE226     CASSEASGGYNEQFF  TCRBV03-01/03-02 TCRBJ02-01 TVLSFCAFA,VLSFCAFAV  \n 7 eEE224     CASSVAGTPSETQYF  TCRBV09-01       TCRBJ02-05 ELYSPIFLI,LYSPIFLIV,‚Ä¶\n 8 eOX54      CASSFPGGYGYTF    TCRBV12-03/12-04 TCRBJ01-02 KLNVGDYFV            \n 9 eQD117     CASSLGPRDTEAFF   TCRBV27-01       TCRBJ01-01 APHGVVFL,APHGVVFLHV,‚Ä¶\n10 eHO136     CAWSALRDRADEQYF  TCRBV30-01       TCRBJ02-07 FLYIIKLIFL,FLYIIKLVF‚Ä¶\n\n\n\n\n\nClick here for hint\n\n\nFirst: Compare the two datasets and identify what happened? Did any variables ‚Äúdissappear‚Äù and did any ‚Äúappear‚Äù? Ok, so this is a bit tricky, but perhaps there is a function to separate a composit (untidy) column into a set of new variables based on a separator? But what is a separator? Just like when you read a file with Comma Separated Values, a separator denotes how a composite string is divided into fields. So look for such a repeated values, which seem to indeed separate such fields. Also, be aware, that character, which can mean more than one thing, may need to be ‚Äúescaped‚Äù using an initial two backslashed, i.e.¬†‚Äú\\x‚Äù, where x denotes the character needing to be ‚Äúescaped‚Äù\n\n\nT7: Add a variable, which counts how many peptides are in each observation of Amino Acids\n\n\n\n\n\n\n\nClick here for hint\n\n\nWe have been working with the stringr-package, perhaps the contains a function to somehow count the number of occurrences of a given character in a string? Again, remember you can type e.g.¬†stringr::str_ and then hit the tab-button to see relevant functions\n\n\npeptide_data |> \n  sample_n(size = 10)\n\n# A tibble: 10 √ó 6\n   Experiment CDR3b              V_gene     J_gene     `Amino Acids`  n_peptides\n   <chr>      <chr>              <chr>      <chr>      <chr>               <dbl>\n 1 eQD114     CASSLHEVGTDSYEQYF  TCRBV09-01 TCRBJ02-07 HTTDPSFLGRY             1\n 2 eOX54      CSGRWGTEAFF        TCRBV20-01 TCRBJ01-01 KLNVGDYFV               1\n 3 eEE240     CASSYAGQYETQYF     TCRBV06-05 TCRBJ02-05 YLNTLTLAV               1\n 4 eQD114     CASSWLGDYEQYF      TCRBV05-06 TCRBJ02-07 HTTDPSFLGRY             1\n 5 eXL27      CASRPAGAYEQYF      TCRBV02-01 TCRBJ02-07 FVDGVPFVV               1\n 6 eOX43      CSVLTGTYEQYF       TCRBV29-01 TCRBJ02-07 AFPFTIYSL,GYI‚Ä¶          7\n 7 eEE228     CATSDPKTESYSNQPQHF TCRBV24-01 TCRBJ01-05 VLWAHGFEL               1\n 8 eEE228     CASSETVNTEAFF      TCRBV19-01 TCRBJ01-01 DGVYFASTEK,GV‚Ä¶          6\n 9 eXL30      CASSWTGTFSGANVLTF  TCRBV12-X  TCRBJ02-06 AFPFTIYSL,GYI‚Ä¶          7\n10 eOX49      CASSVVVVAEQFF      TCRBV10-02 TCRBJ02-01 TVLSFCAFA,VLS‚Ä¶          2\n\n\n\nT8: Re-create the following plot\n\n\n\n\n\n\n\nQ4: What is the maximum number of peptides assigned to one observation?\nT9: Using the str_c- and the seq-functions, re-create the below\n\n\n\n[1] \"peptide_1\" \"peptide_2\" \"peptide_3\" \"peptide_4\" \"peptide_5\"\n\n\n\n\n\nClick here for hint\n\n\nIf you‚Äôre uncertain on how a function works, try going into the console and in this case e.g.¬†type str_c(\"a\", \"b\") and seq(from = 1, to = 3) and see if you combine these?\n\n\nT10: Use, what you learned about separating in T6 and the vector-of-strings you created in T9 adjusted to the number from Q4 to create the below data\n\n\n\n\n\n\n\nClick here for hint\n\n\nIn the console, write ?separate and think about how you used it earlier. Perhaps you can not only specify a vector to separate into, but also specify a function, which returns a vector?\n\n\npeptide_data |> \n  sample_n(size = 10)\n\n# A tibble: 10 √ó 18\n   Experiment CDR3b        V_gene J_gene peptide_1 peptide_2 peptide_3 peptide_4\n   <chr>      <chr>        <chr>  <chr>  <chr>     <chr>     <chr>     <chr>    \n 1 eOX43      CASESQTSGSG‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ FVCNLLLL‚Ä¶ LLFVTVYS‚Ä¶ TVYSHLLLV <NA>     \n 2 eEE224     CASSDPGTYEQ‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ FLQSINFVR FLQSINFV‚Ä¶ FLYLYALV‚Ä¶ GLEAPFLY‚Ä¶\n 3 eXL27      CASSLEQGPDT‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ AFLLFLVLI FLAFLLFLV FYLCFLAFL FYLCFLAF‚Ä¶\n 4 eHH175     CASSPRQSSEQ‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ LSPRWYFYY SPRWYFYYL <NA>      <NA>     \n 5 eXL27      CASSPEDMNTE‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ AFLLFLVLI FLAFLLFLV FYLCFLAFL FYLCFLAF‚Ä¶\n 6 eQD131     CASSPQTTRGR‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ FVCNLLLL‚Ä¶ LLFVTVYS‚Ä¶ TVYSHLLLV <NA>     \n 7 eQD114     CASRLAGPTLT‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ HTTDPSFL‚Ä¶ <NA>      <NA>      <NA>     \n 8 eOX49      CASSSMPQGPH‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ AFLLFLVLI FLAFLLFLV FYLCFLAFL FYLCFLAF‚Ä¶\n 9 eEE240     CASSLGLAGGN‚Ä¶ TCRBV‚Ä¶ TCRBJ‚Ä¶ GMEVTPSG‚Ä¶ MEVTPSGT‚Ä¶ TPSGTWLTY VTPSGTWL‚Ä¶\n10 eOX52      CASSWDRVEQYF TCRBV‚Ä¶ TCRBJ‚Ä¶ QYIKWPWYI YEQYIKWPW YEQYIKWP‚Ä¶ <NA>     \n# ‚Ñπ 10 more variables: peptide_5 <chr>, peptide_6 <chr>, peptide_7 <chr>,\n#   peptide_8 <chr>, peptide_9 <chr>, peptide_10 <chr>, peptide_11 <chr>,\n#   peptide_12 <chr>, peptide_13 <chr>, n_peptides <dbl>\n\n\n\nQ5: Now, presumable you got a warning, discuss in your group why that is?\nQ6: With respect to peptide_n, discuss in your group, if this is wide- or long-data?\n\nNow, finally we will use the what we prepared for today, data-pivotting. There are two functions, namely pivot_wider() and pivot_longer(). Also, now, we will use a trick when developing ones data pipeline, while working with new functions, that on might not be completely comfortable with. You have seen the sample_n()-function several times above and we can use that to randomly sample n-observations from data. This we can utilise to work with a smaller data set in the development face and once we are ready, we can increase this n gradually to see if everything continues to work as anticipated.\n\nT11: Using the peptide_data, run a few sample_n()-calls with varying degree of n to make sure, that you get a feeling for what is going on\nT12: From the peptide_data data above, with peptide_1, peptide_2, etc. create this data set using one of the data-pivotting functions. Remember to start initially with sampling a smaller data set and then work on that first! Also, once you‚Äôre sure you‚Äôre good to go, reuse the peptide_data-variable as we don‚Äôt want huge redundant data sets floating around in our environment\n\n\n\n\n\n\n\nClick here for hint\n\n\nIf the pivotting is not clear at all, then do what I do, create some example data:\n\nmy_data <- tibble(\n  id = str_c(\"id_\", 1:10),\n  var_1 = round(rnorm(10),1),\n  var_2 = round(rnorm(10),1),\n  var_3 = round(rnorm(10),1))\n\n‚Ä¶and then play around with that. A small set like the one above is easy to handle, so perhaps start with that and then pivot back and forth a few times using the pivot_wider()-/pivot_longer()-functions. Use the View()-function to inspect and get a better overview of the results of pivotting.\n\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment CDR3b                V_gene    J_gene n_peptides peptide_n peptide\n   <chr>      <chr>                <chr>     <chr>       <dbl> <chr>     <chr>  \n 1 eEE226     CASSAPGQPYEQYF       TCRBV11-‚Ä¶ TCRBJ‚Ä¶          1 peptide_‚Ä¶ <NA>   \n 2 eEE228     CASSSGPGYTF          TCRBV12-‚Ä¶ TCRBJ‚Ä¶          7 peptide_7 YINVFA‚Ä¶\n 3 eEE243     CASSSWDSVNEQFF       TCRBV09-‚Ä¶ TCRBJ‚Ä¶          1 peptide_‚Ä¶ <NA>   \n 4 eQD126     CASSLETPDEQFF        TCRBV27-‚Ä¶ TCRBJ‚Ä¶          1 peptide_‚Ä¶ <NA>   \n 5 eOX54      CASSQDWLAGQDEQFF     TCRBV04-‚Ä¶ TCRBJ‚Ä¶          1 peptide_‚Ä¶ <NA>   \n 6 eOX43      CASTLLLPGTSNQPQHF    TCRBV28-‚Ä¶ TCRBJ‚Ä¶          2 peptide_4 <NA>   \n 7 eEE240     CASRGTGGGSRLFYGYTF   TCRBV25-‚Ä¶ TCRBJ‚Ä¶         11 peptide_9 MIELSL‚Ä¶\n 8 eEE226     CASSLALADEQYF        TCRBV11-‚Ä¶ TCRBJ‚Ä¶          6 peptide_2 GVYFAS‚Ä¶\n 9 eXL30      CASSFSRGGGYGYTF      TCRBV19-‚Ä¶ TCRBJ‚Ä¶          1 peptide_7 <NA>   \n10 eQD111     RASSPDPMPGLPLGRDEQYF TCRBV07-‚Ä¶ TCRBJ‚Ä¶          1 peptide_2 <NA>   \n\n\n\nQ7: You will see some NAs in the peptide-variable, discuss in your group from where these arise?\nQ8: How many rows and columns now and how does this compare with Q3? Discuss why/why not it is different?\nT13: Now, loose the redundant variables n_peptides and peptide_n and also get rid of the NAs in the peptide-column and make sure, that we only have unique observations, i.e.¬†there are no repeated rows/observations\n\n\n\n\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 5\n   Experiment CDR3b                V_gene     J_gene     peptide   \n   <chr>      <chr>                <chr>      <chr>      <chr>     \n 1 eEE224     CSARGGAVGTEAFF       TCRBV20-X  TCRBJ01-01 LIDFYLCFL \n 2 eEE228     CASSLSGVSTEAFF       TCRBV27-01 TCRBJ01-01 FLQSINFVRI\n 3 eEE224     CASSSGQVEAFF         TCRBV13-01 TCRBJ01-01 MIELSLIDFY\n 4 eEE240     CASSLGDGSSTDTQYF     TCRBV07-09 TCRBJ02-03 QLMCQPILL \n 5 eXL31      CASSKTCLGQGLYYSYEQYF TCRBV21-01 TCRBJ02-07 TLKSFTVEK \n 6 eQD132     CASSQDGPGLSYEQYF     TCRBV04-01 TCRBJ02-07 SINFVRIIMR\n 7 eHO124     CSARDPGGQGVYEQYF     TCRBV20-X  TCRBJ02-07 LIDFYLCFL \n 8 eOX46      CASSLDWGGTEAFF       TCRBV11-02 TCRBJ01-01 VLWAHGFEL \n 9 eHH173     CASSLGPGMMFGYTF      TCRBV28-01 TCRBJ01-02 FLQSINFVRI\n10 eEE228     CASSYLSAYNEQFF       TCRBV11-02 TCRBJ02-01 LLTDEMIAQY\n\n\n\nQ8: Now how many rows and columns and is this data tidy? Discuss in your group why/why not?\n\nAgain, we turn to the stringr-package, as we need to make sure that the sequence data does indeed only contain valid characters. There are a total of 20 proteogenic amino acids, which we symbolise using ARNDCQEGHILKMFPSTWYV.\n\nT14: Use the str_detect()-function to filter the CDR3b and peptide variables using a pattern of [^ARNDCQEGHILKMFPSTWYV] and then play with the negate-parameter so see what happens\n\n\n\n\n\n\n\nClick here for hint\n\n\nAgain, try to play a bit around with the function in the console, type e.g.¬†str_detect(string = \"ARND\", pattern = \"A\") and str_detect(string = \"ARND\", pattern = \"C\") and then recall, that the filter-function requires a logical vector, i.e.¬†a vector of TRUE and FALSE to filter the rows\n\n\nT15: Add two new variables to the data, k_CDR3b and k_peptide each signifying the length of the respective sequences\n\n\n\n\n\n\n\nClick here for hint\n\n\nAgain, we‚Äôre working with strings, so perhaps there is a package of interest and perhaps in that package, there is a function, which can get the length of a string?\n\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment CDR3b              V_gene         J_gene peptide k_CDR3b k_peptide\n   <chr>      <chr>              <chr>          <chr>  <chr>     <int>     <int>\n 1 eQD125     CASSLSSNSYEQYF     TCRBV28-01     TCRBJ‚Ä¶ KVPTDN‚Ä¶      14        11\n 2 eEE226     CASSQRSGGYYNEQFF   TCRBV03-01/03‚Ä¶ TCRBJ‚Ä¶ LPFNDG‚Ä¶      16         9\n 3 eQD137     CSATSGDTYEQYF      TCRBV20-X      TCRBJ‚Ä¶ FLYLYA‚Ä¶      13        10\n 4 eEE240     CATSREATGLLSGNTIYF TCRBV15-01     TCRBJ‚Ä¶ AFLLFL‚Ä¶      18         9\n 5 eQD114     CASSIVEGTGELFF     TCRBV19-01     TCRBJ‚Ä¶ ALRKVP‚Ä¶      14        14\n 6 eAV88      CASSLWGGGSNQPQHF   TCRBV28-01     TCRBJ‚Ä¶ FYLCFL‚Ä¶      16         9\n 7 eEE224     CASSYGQLHLNEQFF    TCRBV13-01     TCRBJ‚Ä¶ YLYALV‚Ä¶      15         9\n 8 eQD108     CASSQRWGYQPQHF     TCRBV04-01     TCRBJ‚Ä¶ TSRTLS‚Ä¶      14         9\n 9 eHO132     CASTESPSGSYEQYF    TCRBV06-05     TCRBJ‚Ä¶ FIASFR‚Ä¶      15         9\n10 eEE243     CASSPISGLGYEQYF    TCRBV27-01     TCRBJ‚Ä¶ KLSYGI‚Ä¶      15         9\n\n\n\nT16: Re-create this plot\n\n\n\n\n\n\n\nQ9: What is the most predominant length of the CDR3b-sequences?\nT17: Re-create this plot\n\n\n\n\n\n\n\nQ10: What is the most predominant length of the peptide-sequences?\nQ11: Discuss in your group, if this data set is tidy or not?\n\n\npeptide_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment CDR3b               V_gene        J_gene peptide k_CDR3b k_peptide\n   <chr>      <chr>               <chr>         <chr>  <chr>     <int>     <int>\n 1 eOX46      CASTNRVDNTEAFF      TCRBV27-01    TCRBJ‚Ä¶ VYFLQS‚Ä¶      14         9\n 2 eXL37      CASSFVDWYNFGQGDGYTF TCRBV07-09    TCRBJ‚Ä¶ DFLEYH‚Ä¶      19         9\n 3 eHO135     CASSQGDTNQPQHF      TCRBV12-X     TCRBJ‚Ä¶ FLQSIN‚Ä¶      14        10\n 4 eXL30      CASSPLGGTGAHEQYF    TCRBV18-01    TCRBJ‚Ä¶ LWLLWP‚Ä¶      16         9\n 5 eJL161     CASRFSGGAADTQYF     TCRBV19-01    TCRBJ‚Ä¶ FLQSIN‚Ä¶      15         9\n 6 eXL27      CASRPSGGLDTQYF      TCRBV03-01/0‚Ä¶ TCRBJ‚Ä¶ TLIGDC‚Ä¶      14         9\n 7 eOX49      CASSRLAGGPNEQFF     TCRBV14-01    TCRBJ‚Ä¶ YLCFLA‚Ä¶      15         9\n 8 eEE226     CASSPGTGGFRSYEQYF   TCRBV05-01    TCRBJ‚Ä¶ LLLDDF‚Ä¶      17         9\n 9 eOX49      CASSFYGQGGRQTQYF    TCRBV12-X     TCRBJ‚Ä¶ FYLCFL‚Ä¶      16         9\n10 eXL31      CSARDLREIISYEQYF    TCRBV20-X     TCRBJ‚Ä¶ INFVRI‚Ä¶      16         9\n\n\n\n\nCreating one data set from two data sets\nBefore we move onto using the family of *_join-functions you prepared for today, we will just take a quick peek at the meta data again:\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 11\n   Experiment Cohort        Age Gender Race  A1    A2    B1    B2    C1    C2   \n   <chr>      <chr>       <dbl> <chr>  <chr> <chr> <chr> <chr> <chr> <chr> <chr>\n 1 ePD73      Healthy (N‚Ä¶    37 F      White \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*1‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 2 eHO126     COVID-19-C‚Ä¶    37 F      <NA>  \"A*0‚Ä¶ \"A*2‚Ä¶ \"B*0‚Ä¶ \"B*5‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 3 eQD116     COVID-19-C‚Ä¶    66 F      <NA>  \"A*0‚Ä¶ \"A*1‚Ä¶ \"B*3‚Ä¶ \"B*3‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 4 eHO138     COVID-19-B‚Ä¶    NA <NA>   <NA>  \"\"    \"\"    \"\"    \"\"    \"\"    \"\"   \n 5 eHO132     COVID-19-C‚Ä¶    65 F      White \"A*0‚Ä¶ \"A*2‚Ä¶ \"B*1‚Ä¶ \"B*3‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 6 ePD80      COVID-19-C‚Ä¶    67 M      <NA>  \"A*0‚Ä¶ \"A*6‚Ä¶ \"B*1‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*1‚Ä¶\n 7 eXL27      Healthy (N‚Ä¶    24 M      White \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*2‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n 8 eHO127     COVID-19-C‚Ä¶    28 M      <NA>  \"A*2‚Ä¶ \"A*2‚Ä¶ \"B*4‚Ä¶ \"B*5‚Ä¶ \"C*0‚Ä¶ \"C*1‚Ä¶\n 9 eQD126     COVID-19-C‚Ä¶    54 F      <NA>  \"A*0‚Ä¶ \"A*0‚Ä¶ \"B*0‚Ä¶ \"B*0‚Ä¶ \"C*0‚Ä¶ \"C*0‚Ä¶\n10 ePD81      COVID-19-C‚Ä¶    64 M      <NA>  \"A*2‚Ä¶ \"A*2‚Ä¶ \"B*4‚Ä¶ \"B*4‚Ä¶ \"C*0‚Ä¶ \"C*1‚Ä¶\n\n\nRemember you can scroll in the data.\n\nQ12: Discuss in your group, if this data with respect to the A1-, A2-, B1-, B2-, C1- and C2-variables is a wide- or a long-data format?\n\nAs with the peptide_data, we will now have to use data-pivotting again. I.e.:\n\nT18: use either the pivot_wider- or pivot_longer-function to create the following data:\n\n\n\n\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment Cohort                        Age Gender Race  Gene  Allele    \n   <chr>      <chr>                       <dbl> <chr>  <chr> <chr> <chr>     \n 1 eQD113     COVID-19-Convalescent          36 M      <NA>  C2    C*15:02:01\n 2 ePD73      Healthy (No known exposure)    37 F      White B1    B*15:01   \n 3 eQD131     COVID-19-Exposed               NA <NA>   <NA>  B2    B*51:01:01\n 4 eLH57      COVID-19-Convalescent          NA <NA>   <NA>  C1    C*06:02:01\n 5 eQD132     COVID-19-Convalescent          NA <NA>   <NA>  B1    B*39:01:01\n 6 eAV100     COVID-19-Convalescent          29 F      <NA>  B2    B*40:01:02\n 7 eQD120     COVID-19-Convalescent          62 F      <NA>  B1    B*08:01:01\n 8 eXL32      Healthy (No known exposure)    37 F      White A2    A*02:01   \n 9 eHO134     COVID-19-Convalescent          36 M      White B1    B*49:01:01\n10 eLH43      COVID-19-Convalescent          57 M      <NA>  B2    B*44:03:01\n\n\nRemember, what we are aiming for here, is to create one data set from two. So:\n\nQ13: Discuss in your group, which variable(s?) define the same observations between the peptide_data and the meta_data?\n\nOnce you have agreed upon Experiment, then use that knowledge to subset the meta_data to the variables-of-interest:\n\n\n\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 2\n   Experiment Allele    \n   <chr>      <chr>     \n 1 eLH46      B*51:01:01\n 2 eJL143     A*32:01   \n 3 ePD80      B*15:01:01\n 4 eQD123     B*07:02:01\n 5 eQD112     A*24:02:01\n 6 eOX52      A*02:01   \n 7 eLH41      C*06:02:01\n 8 eDH105     A*24:02:01\n 9 eQD132     A*02:03:01\n10 eQD120     B*08:01:01\n\n\nUse the View()-function again, to look at the meta_data - Notice something? Some alleles are e.g.¬†A*11:01, whereas others are B*51:01:02. You can find information on why, by visiting Nomenclature for Factors of the HLA System.\nLong story short, we only want to include Field 1 (allele group) and Field 2 (Specific HLA protein). You have prepared the stringr-package for today. See if you can find a way to reduce e.g.¬†B*51:01:02 to B*51:01 and then create a new variable Allele_F_1_2 accordingly, while also removing the ...x (where x is a number) subscripts from the Gene-variable (It is an artifact from having the data in a wide format, where you cannot have two variables with the same name) and also, remove any NAs and \"\"s, denoting empty entries.\n\n\n\nClick here for hint\n\n\nThere are several ways this can be achieved, the easiest being to consider if perhaps a part of the string based on indices could be of interest. This term ‚Äúa part of a string‚Äù is called a substring, perhaps the stringr-package contains a function work with substring? In the console, type stringr:: and hit tab. This will display the functions available in the stringr-package. Scroll down and find the functionst starting with str_ and look for on, which might be relevant and remember you can use ?function_name to get more information on how a given function works.\n\n\n\n\n\nT19: Create the following data, according to specifications above:\n\n\nmeta_data |> \n  sample_n(10)\n\n# A tibble: 10 √ó 3\n   Experiment Allele     Allele_F_1_2\n   <chr>      <chr>      <chr>       \n 1 eAV105     C*07:02:01 C*07:02     \n 2 eLH41      C*06:02:01 C*06:02     \n 3 eLH43      A*23:01:01 A*23:01     \n 4 eQD110     B*44:02:01 B*44:02     \n 5 eMR17      B*57:01:01 B*57:01     \n 6 eHO135     A*03:01:01 A*03:01     \n 7 eQD109     B*07:02:01 B*07:02     \n 8 eQD121     C*07:01:01 C*07:01     \n 9 eQD113     C*15:02:01 C*15:02     \n10 eLH54      C*07:02:01 C*07:02     \n\n\nThe asterix, i.e.¬†* is a rather annoying character because of ambiguity, so:\n\nT20: Clean the data a bit more, by removing the asterix and redundant variables:\n\n\n\n\n\nmeta_data |> \n  sample_n(size = 10)\n\n# A tibble: 10 √ó 2\n   Experiment Allele\n   <chr>      <chr> \n 1 eGK111     B08:01\n 2 eOX54      A02:01\n 3 eLH45      A03:01\n 4 eOX54      B15:03\n 5 eJL147     B38:02\n 6 eLH53      A11:01\n 7 eHO125     A02:01\n 8 eMR12      A01:01\n 9 eQD134     A34:01\n10 eQD127     C02:02\n\n\n\n\n\nClick here for hint 1\n\n\nAgain, the stringr-package may come in handy. Perhaps there is a function remove, one or more such pesky characters?\n\n\n\n\nClick here for hint 2\n\n\nGetting a weird error? Recall, that character ambiguity needs to be ‚Äúescaped‚Äù, you did this somehow earlier on‚Ä¶\n\nRecall the peptide_data?\n\npeptide_data |>\n  sample_n(10)\n\n# A tibble: 10 √ó 7\n   Experiment CDR3b             V_gene     J_gene     peptide  k_CDR3b k_peptide\n   <chr>      <chr>             <chr>      <chr>      <chr>      <int>     <int>\n 1 eOX49      CASSQLGPAGDGTEQFF TCRBV14-01 TCRBJ02-01 TVLSFCA‚Ä¶      17         9\n 2 eLH54      CASSSPPKGAAYEQYF  TCRBV07-09 TCRBJ02-07 FLQSINF‚Ä¶      16        10\n 3 eQD119     CASSYAGEEYEQYF    TCRBV06-05 TCRBJ02-07 QSINFVR‚Ä¶      14         9\n 4 eEE224     CATGQGNEQFF       TCRBV19-01 TCRBJ02-01 IDFYLCF‚Ä¶      11        10\n 5 eEE240     CASERSGGVGTDTQYF  TCRBV02-01 TCRBJ02-03 SLIDFYL‚Ä¶      16        10\n 6 eMR17      CASSPTGQGIYGYTF   TCRBV06-05 TCRBJ01-02 QSINFVR‚Ä¶      15         9\n 7 eOX52      CASSVPGHGDQPQHF   TCRBV02-01 TCRBJ01-05 NYLYRLF‚Ä¶      15         9\n 8 ePD83      CASSQAQQETQYF     TCRBV04-01 TCRBJ02-05 SEHDYQI‚Ä¶      13        14\n 9 eXL27      CASSILGAPYEQYF    TCRBV28-01 TCRBJ02-07 LQSINFV‚Ä¶      14         9\n10 eEE228     CASSLVREGYNEQFF   TCRBV27-01 TCRBJ02-01 LLFVTVY‚Ä¶      15        10\n\n\n\nT21: Create a dplyr-pipeline, starting with the peptide_data, which joins it with the meta_data and remember to make sure that you get only unqiue observations of rows. Save this data into a new variable names peptide_meta_data (If you get a warning, discuss in your group what it means?)\n\n\n\n\n\n\n\nClick here for hint 1\n\n\nWhich family of functions do we use to join data? Also, perhaps here it would be prudent to start with working on a smaller data set, recall we could sample a number of rows yielding a smaller development data set\n\n\n\n\nClick here for hint 2\n\n\nYou should get a data set of around +3.000.000, take a moment to consider how that would have been to work with in Excel? Also, in case the servers are not liking this, you can consider subsetting the peptide_data prior to joining to e.g.¬†100,000 or 10,000 rows.\n\n\npeptide_meta_data |>\n  sample_n(10)\n\n# A tibble: 10 √ó 8\n   Experiment CDR3b            V_gene    J_gene peptide k_CDR3b k_peptide Allele\n   <chr>      <chr>            <chr>     <chr>  <chr>     <int>     <int> <chr> \n 1 eXL30      CASSEVKAGGLWTQYF TCRBV02-‚Ä¶ TCRBJ‚Ä¶ KEIIFL‚Ä¶      16        11 A01:01\n 2 ePD76      CASSGGIGELFF     TCRBV02-‚Ä¶ TCRBJ‚Ä¶ YFLQSI‚Ä¶      12        10 B40:01\n 3 eXL27      CASSVAGKETQYF    TCRBV02-‚Ä¶ TCRBJ‚Ä¶ APAHIS‚Ä¶      13         8 B27:05\n 4 eXL32      CSARGLQSSYEQYF   TCRBV20-X TCRBJ‚Ä¶ FLAFLL‚Ä¶      14         9 C04:01\n 5 eXL30      CATAGTGSSYEQYF   TCRBV27-‚Ä¶ TCRBJ‚Ä¶ SVLLFL‚Ä¶      14         9 C07:02\n 6 eLH42      CASSMGHGDTEAFF   TCRBV05-‚Ä¶ TCRBJ‚Ä¶ GVVFLH‚Ä¶      14         9 A02:01\n 7 eEE228     CAILDRVVNTEAFF   TCRBV06-‚Ä¶ TCRBJ‚Ä¶ FLQSIN‚Ä¶      14        10 C04:01\n 8 eXL30      CASSLNMAGDGYTF   TCRBV28-‚Ä¶ TCRBJ‚Ä¶ LIDFYL‚Ä¶      14         9 A01:01\n 9 eXL31      CASSYPEGGHNEQFF  TCRBV06-‚Ä¶ TCRBJ‚Ä¶ SLIDFY‚Ä¶      15        10 B07:02\n10 ePD83      CASRNRLAGVYNEQFF TCRBV10-‚Ä¶ TCRBJ‚Ä¶ SEHDYQ‚Ä¶      16        14 B13:01\n\n\n\n\n\nAnalysis\nNow, that we have the data in a prepared and ready-to-analyse format, let us return to the two burning questions we had:\n\nWhat characterises the peptides binding to the HLAs?\nWhat characterises T-cell Receptors binding to the pMHC-complexes?\n\n\nPeptides binding to HLA\nAs we have touched upon multiple times, R is very flexible and naturally you can also create sequence logos. Finally, let us create a binding motif using the package ggseqlogo (More info here).\n\nT22: Subset the final peptide_meta_data-data to A02:01 and unique observations of peptides of length 9 and re-create the below sequence logo\n\n\n\n\nClick here for hint\n\n\nYou can pipe a vector of peptides into ggseqlogo, but perhaps you first need to pull that vector from the relevant variable in your tibble? Also, consider before that, that you‚Äôll need to make sure, you are only looking at peptides of length 9\n\n\n\n\n\n\n\n\n\n\n\nT23: Repeat for e.g.¬†B07:02 or another of your favorite alleles\n\nNow, let‚Äôs take a closer look at the sequence logo:\n\nQ14: Which positions in the peptide determines binding to HLA?\n\n\n\n\nClick here for hint\n\n\nRecall your Introduction to Bioinformatics course? And/or perhaps ask your fellow group members if they know?\n\n\n\nCDR3b-sequences binding to pMHC\n\nT24: Subset the peptide_meta_data, such that the length of the CDR3b is 15, the allele is A02:01 and the peptide is LLFLVLIML and re-create the below sequence logo of the CDR3b sequences:\n\n\n\n\n\n\n\n\n\n\n\nQ15: In your group, discuss what you see?\nT25: Play around with other combinations of k_CDR3b, Allele, and peptide and inspect how the logo changes\n\nDisclaimer: In this data set, we only get: A given CDR3b was found to recognise a given peptide in a given subject and that subject had a given haplotype - Something‚Äôs missing‚Ä¶ Perhaps if you have had immunology, then you can spot it? There is a trick to get around this missing information, but that‚Äôs beyond scope of what we‚Äôre working with here."
  },
  {
    "objectID": "lab05.html#epilogue",
    "href": "lab05.html#epilogue",
    "title": "Lab 5: Data Wrangling II",
    "section": "Epilogue",
    "text": "Epilogue\nThat‚Äôs it for today - I know this overwhelming now, but commit to it and you WILL be plenty rewarded! I hope today was at least a glimpse into the flexibility and capabilities of using tidyverse for applied Bio Data Science\n‚Ä¶also, noticed something? We spend maybe 80% of the time here on dealing with data-wrangling and then once we‚Äôre good to go, the analysis wasn‚Äôt that time consuming - That‚Äôs often the way it ends up going, you‚Äôll spend a lot of time on data handling and getting the tidyverse toolbox in your toolbelt, will allow you to be so much more effecient in your data wrangling, so you can get to the fun part as quick as possible!"
  },
  {
    "objectID": "lab05.html#sec-assignment",
    "href": "lab05.html#sec-assignment",
    "title": "Lab 5: Data Wrangling II",
    "section": "Today‚Äôs Assignment",
    "text": "Today‚Äôs Assignment\nAfter today, we are halfway through the labs of the course, so now is a good time to spend some time recalling what we have been over and practising writing a reproducible Quarto-report.\nYour group assignment today is to condense the exercises into a group micro-report! Talk together and figure out how to destill the exercises from today into one small end-to-end runable reproducible micro-report. DO NOT include ALL of the exercises, but rather include as few steps as possible to arrive at your results. Be vey consise!\nBut WHY? WHY are you not specifying exactly what we need to hand in? Because we are training taking independent decisions, which is crucial in applied bio data science, so take a look at the combined group code, select relevant sections and condense - If you don‚Äôt make it all the way through the exercises, then condense and present what you were able to arrive at! What do you think is central/important/indispensable? Also, these hand ins are NOT for us to evaluate you, but for you to train creating products and the get feedback on your progress!\nIMPORTANT: Remember to check the ASSIGNMENT GUIDELINES\n‚Ä¶and as always - Have fun!"
  },
  {
    "objectID": "lab06.html",
    "href": "lab06.html",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "",
    "text": "broom\npurrr"
  },
  {
    "objectID": "lab06.html#schedule",
    "href": "lab06.html#schedule",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.45: Recap of Lab 5\n08.45 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab06.html#learning-materials",
    "href": "lab06.html#learning-materials",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nImportant: Questionnaire (brief, 5-10 min): Course Midway Evaluation\nBook (Note, this is intentionally 1.ed.): R4DS: Chapter 22: Introduction\nBook (Note, this is intentionally 1.ed.): Chapter 23: Model Basics\nBook (Note, this is intentionally 1.ed.): Chapter 24: Model Building\nBook (Note, this is intentionally 1.ed.): Chapter 25: Many models\nVideo: Broom: Converting Statistical Models to Tidy Data Frames\nVideo: Alex Hayes | Solving the model representation problem with broom | RStudio (2019)\nVideo: ‚ÄúThe Joy of Functional Programming (for Data Science)‚Äù with Hadley Wickham\nOptional: If you are completely new to statistical modelling, then click here for a primer"
  },
  {
    "objectID": "lab06.html#learning-objectives",
    "href": "lab06.html#learning-objectives",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nFit a simple linear model and interpret model parameters\nUnderstand and apply simple purrr-functions for element-wise function application\nUnderstand and apply grouped supervised models to form nested model objects\nUnderstand and apply the broom-functions for tidying various model objects\nOptional LO: Perform a basic principal component analysis for dimension reduction of high dimensional data\nOptional LO: Perform a basic unsupervised k-means clustering of high dimensional data"
  },
  {
    "objectID": "lab06.html#sec-exercises",
    "href": "lab06.html#sec-exercises",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Exercises",
    "text": "Exercises"
  },
  {
    "objectID": "lab06.html#throwback",
    "href": "lab06.html#throwback",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Throwback‚Ä¶",
    "text": "Throwback‚Ä¶\nUsing the tibble() function, re-create the data from this visualisation and then create your version of how this data could be visualised in a more informative manner:\n\n\n\n\n\nAlso‚Ä¶ If you still need to be convinced of the flexibility of ggplot, try running this code:\n\nxy <- seq(from = -3,\n          to = 3, \n          by = 0.01)\nexpand.grid(x = xy,\n            y = xy) |>\n  ggplot(\n    mapping = aes(\n      x = (1 - x - sin(y^2)),\n      y = (1 + y - cos(x^2)))) +\n  geom_point(alpha = 0.05,\n             shape = 20,\n             size = 0) +\n  theme_void() +\n  coord_polar()\n\nIf you are curious about what is going on here, try googling ‚ÄúGenerative art‚Äù‚Ä¶ Anyhoo‚Ä¶ Let us move on‚Ä¶"
  },
  {
    "objectID": "lab06.html#prologue",
    "href": "lab06.html#prologue",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Prologue",
    "text": "Prologue\n\n\n\nSo far, we have worked on\n\nLab 2: Genomis, the gravier-data from A prognostic DNA signature for T1T2 node-negative breast cancer patients\nLab 3: Metagenomics, the pitlatrine-data from ‚ÄúAssessment of the influence of intrinsic environmental and geographical factors on the bacterial ecology of pit latrines‚Äù\nLab 4: Clinical data, the diabetes-data from Prevalence of coronary heart disease risk factors among rural blacks: a community-based study and A trial of church-based smoking cessation interventions for rural African Americans\nLab 5: High throughput immunoinformatics data, the SARS-CoV-2-data A large-scale database of T-cell receptor beta (TCRŒ≤) sequences and binding associations from natural and synthetic exposure to SARS-CoV-2"
  },
  {
    "objectID": "lab06.html#getting-started",
    "href": "lab06.html#getting-started",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Getting Started",
    "text": "Getting Started\n\nOnce again, please go to the course cloud RStudio server and login\nCreate a new Quarto document and remember to save it\n\nToday, we will re-examine the data acquired in Lab 2 to further explore the research question:\n\nWhat genes are significantly up-/down-regulated between the patients with- and without early metastasis?‚Äú"
  },
  {
    "objectID": "lab06.html#load-data",
    "href": "lab06.html#load-data",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Load Data",
    "text": "Load Data\nRecall, so far we have adjusted the chunk setting to #| eval: true first time we get some data from an online repository and then subsequently set #| eval: false.\nLet‚Äôs do that in a bit nicer way.\n\nT1: Create a new ‚ÄúLoad Data‚Äù header in your document and add the below chunk:\n\n\nraw_dir <- \"data/_raw/\"\ndata_file <- \"gravier.RData\"\ndata_loc <- \"https://github.com/ramhiser/datamicroarray/raw/master/data/\"\n\nif( !dir.exists(raw_dir) ){\n  dir.create(path = raw_dir)\n}\nif( !file.exists(str_c(raw_dir, data_file)) ){\n  download.file(\n    url = str_c(data_loc, data_file),\n    destfile = str_c(raw_dir, data_file))\n}\nload(file = str_c(raw_dir, data_file))\n\n\nQ1: In your group, discuss, what is going on here? Make sure you follow the difference between the first time you run this and the second!\n\n\n\n\nClick here for hint\n\n\nWe have used the str_c()-function before, try running e.g.:\n\nx <- \"a\"\ny <- \"b\"\nstr_c(x, y)\n\nFor the functions dir.exists() and file.exists(), the hint is in the title, they return logicals. To get a better understanding of this, try running e.g.:\n\nx <- 2\nx == 2\nif( x == 2 ){\n  print(\"Yes!\")\n}\n\nThen change x == 2 to x != 2, re-run the code and see what happens"
  },
  {
    "objectID": "lab06.html#clean-data",
    "href": "lab06.html#clean-data",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Clean Data",
    "text": "Clean Data\nThe next step is to clean up the data:\n\nUse the ls()-function to see what objects you have in your environment\nUse the str()-function on the gravier data you retrieved to answer:\n\n\nQ2: Discuss in your group if this is tidy data?\nT2: Create a new ‚ÄúClean Data‚Äù header in your document and add the below chunk:\n\n\ngravier_clean <- gravier |>\n  bind_cols() |>\n  as_tibble()\n\n\nQ3: Discuss in your group if this is tidy data?\nQ4: In fact, specifically state what are the ‚Äúrules‚Äù for tidy data?\nQ5: In your group, discuss why bind_cols can by very very dangerous to use?\n\nNow, moving on, let‚Äôs write the clean data to disk:\n\nT3: In your ‚ÄúClean Data‚Äù-section, add a new chunk, where you write a tab-separated-values gzipped (i.e.¬†compressed) file called ‚Äú02_gravier_clean‚Äù (with the correct filetype specification) into your ‚Äúdata‚Äù-folder\n\n\n\n\nClick here for hint\n\n\nJust as you can read a file, you can of course also write a file. Note the filetype we want to write here is tab-separated-values. If you in the console type e.g.¬†readr::wr and then hit the tab-button, you will see the different functions for writing different filetypes. We previously did a trick to automatically gzip (compress) files?"
  },
  {
    "objectID": "lab06.html#augment-data",
    "href": "lab06.html#augment-data",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Augment Data",
    "text": "Augment Data\n\nT4: Create a new ‚ÄúAugment Data‚Äù header in your document and add the below chunk:\n\n\ngravier_clean_aug <- gravier_clean |>\n  mutate(y = case_when(y == \"poor\" ~ 1,\n                       y == \"good\" ~ 0)) |> \n  relocate(early_metastasis = y)\n\n\nQ6: In your group, discuss, what each step of the above work flow does, i.e.¬†what are the specifics of the dplyr-pipeline?\nT5: In your ‚ÄúAugment Data‚Äù-section, add a new chunk, where you write a tab-separated-values gzipped (i.e.¬†compressed) file called ‚Äú03_gravier_clean_aug‚Äù (with the correct filetype specification) into your ‚Äúdata‚Äù-folder"
  },
  {
    "objectID": "lab06.html#analysis",
    "href": "lab06.html#analysis",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Analysis",
    "text": "Analysis\n\nOne Gene, one model\n\nT6: Create a new ‚ÄúAnalysis‚Äù header in your document\n\nRecall, in the second lab, we were looking at ‚Äúour favourite gene‚Äù. In the following either look back to what was your favourite gene or choose a new ü§∑Ô∏è\nLet‚Äôs fit our first model! If the concept of models and linear regression is unfamiliar, consider checking out the primer on linear models in R before proceeding.\n\nT7: Use the lm-function to create your first model and save it to a new variable e.g.¬†‚Äúmy_first_model‚Äù\n\n\n\n\nClick here for hint\n\n\nUse the formula my_favourite_gene ~ early_metastasis and remember when you pipe into the lm-function, you have to specify data = _\n\n\nQ7: What are your coefficients? Mine are:\n\n\n\n     (Intercept) early_metastasis \n     -0.01616011      -0.03426164 \n\n\n\nT8: Use the group_by() \\(\\rightarrow\\) summarise()-workflow to calculate the mean values of the gene expression for your favourite gene stratified on early_metastasis\nQ8: What are your mean values? Mine are:\n\n\n\n# A tibble: 2 √ó 2\n  early_metastasis      mu\n             <dbl>   <dbl>\n1                0 -0.0162\n2                1 -0.0504\n\n\n\nQ9: Discuss in your group: How are your coefficients related to your mean expression values?\n\n\n\n\nClick here for hint\n\n\nRecall, we have two terms here, intercept and slope. The intercept is the y-value at x = 0 and the slope is the change in y-value, for one unit change in x\n\n\nQ10: Discuss in your group: Is your gene up- or down-regulated from early_metastasis = 0 to early_metastasis = 1 and use the summary()-function to see if is it statistically significant at a level of significance of \\(\\alpha = 0.05\\)?\n\n\n\n\nClick here for hint\n\n\nTry to run my_first_model |> summary() and look at the estimate for early_metastasis, i.e.¬†the slope and also see if you can find the p-value somewhere in this summary‚Ä¶\n\nExcellent! Now we have a feeling for working with the lm()-functions and a basic understanding of the meaning of the coefficients, when we are using linear regression to model a binary outcome. The reason that we use a linear regression model in this case is, that for these exercises, we want to investigate the relationship between variables rather than obtaining probability predictions, i.e.\n\nWhat genes are significantly up-/down-regulated between the patients with- and without early metastasis?\n\nSo, without further ado, let‚Äôs dive in!\n\n\nAll the Genes, all the models\nFirst, the recent couple of years have seen an immense development in unifying the modelling interface in R, which is notoriously inconsistent. You may be familiar with the caret-package, the developer of which has created tidymodels, (which I really wish we had time to explore in details). In the following we will work with some of the principles for tidying model object using broom, having object nested in tibbles and working with these using purrr.\nNow, you saw above how we could fit one model for one gene. So, we could repeat the procedure you worked through for each gene, but first consider:\n\nQ11: How many genes are there in the gravier data set?\n\nNow, we just have to make one model for each gene!\n\n\n\n\n\nHonestly, let‚Äôs not! Also, recall: ‚ÄúWe don‚Äôt loop, we func!‚Äù, so let‚Äôs see how that would work.\n\n\nModels, models everywhere‚Ä¶\nIn principle, you could overflow your environment with individual model objects, but that would require a lot of code-lines and a lot of hard-coding. But let us instead see if we can come up with something just a tad more clever.\n\nPreparing the Data\nFirst:\n\nQ12: Discuss in your group, if the gravier_clean_aug is a ‚Äúwide‚Äù or a ‚Äúlong‚Äù dataset?\n\nOnce you have agreed upon and understood why, this is a wide data set, proceed and:\n\nT9: Create this long version of your gravier_clean_aug data and save it in gravier_clean_aug_long\n\n\n\n# A tibble: 488,040 √ó 3\n   early_metastasis gene   log2_expr_level\n              <dbl> <chr>            <dbl>\n 1                0 g2E09         -0.00144\n 2                0 g7F07         -0.00144\n 3                0 g1A01         -0.0831 \n 4                0 g3C09         -0.0475 \n 5                0 g3H08          0.0158 \n 6                0 g1A08         -0.0336 \n 7                0 g1B01         -0.136  \n 8                0 g1int1         0.0180 \n 9                0 g1E11          0.0257 \n10                0 g8G02          0.00720\n# ‚Ñπ 488,030 more rows\n\n\n\n\n\nClick here for hint\n\n\nRecall which function took a dataset from ‚Äúwide‚Äù- to ‚Äúlong‚Äù-format and also the three parameters of interest for that function is cols, names_to and values_to. If you look at my example, you will see that the columns we have pivotted have types <chr> and <dbl>, these will map to the names_to- and values_to-parameters respectively. The cols can be used with the so-called ‚Äúhelper-functions‚Äù, that we have previously talked about. If you look at the gene-columns in the data, all the genes starts with a ‚Äúg‚Äù, so we can use the helper functions starts_with() and then give the argument ‚Äúg‚Äù to the match-parameter\n\nThe reason we want the ‚Äúlong‚Äù version of the data set is, that now we have ALL the genes defined in ONE gene-column. This means that we can use the group_by()-function to work per gene without looping (Recall: Don‚Äôt loop, only func!).\nNow:\n\nT10: Create a dplyr-pipeline, use the group_by()-function to group your gravier_clean_aug_long-dataset by gene and then add the nest()- and ungroup()-functions to your pipeline\n\n\n\n# A tibble: 2,905 √ó 2\n   gene   data              \n   <chr>  <list>            \n 1 g2E09  <tibble [168 √ó 2]>\n 2 g7F07  <tibble [168 √ó 2]>\n 3 g1A01  <tibble [168 √ó 2]>\n 4 g3C09  <tibble [168 √ó 2]>\n 5 g3H08  <tibble [168 √ó 2]>\n 6 g1A08  <tibble [168 √ó 2]>\n 7 g1B01  <tibble [168 √ó 2]>\n 8 g1int1 <tibble [168 √ó 2]>\n 9 g1E11  <tibble [168 √ó 2]>\n10 g8G02  <tibble [168 √ó 2]>\n# ‚Ñπ 2,895 more rows\n\n\nNote, this is conceptually a super-tricky data structure!\n\nQ13: Discuss in your group, what happened to the data?\n\n\n\n\nClick here for hint\n\n\nTry to run each of the following code chunks and examine what happens at each step:\n\ngravier_clean_aug_long_nested\n\n\ngravier_clean_aug_long_nested |>\n  filter(gene == \"g2E09\") # Replace \"g2E09\" with whatever was YOUR favourite gene!\n\n\ngravier_clean_aug_long_nested |>\n  filter(gene == \"g2E09\") |> # Replace \"g2E09\" with whatever was YOUR favourite gene!\n  pull(data)\n\n\n\nQ14: Moreover, discuss in your group, what does <tibble [168 √ó 2]> mean?\n\nNow, if you experiencing, that the server seems slow, consider if you want to proceed now with ALL the genes or just a subset. If you just want a subset, then use sample_n() to randomly select e.g.¬†100 genes for further analysis. Remember you may want to use the set.seed() function to create a reproducible work flow even when sampling.\n\n\n\n\n\nFitting Models\nNow, recall our research question:\n\nWhat genes are significantly up-/down-regulated between the patients with- and without early metastasis?\n\nTo investigate this, we want to fit a linear model to each gene, i.e.¬†as you did initially for your favourite gene, we want to do in a clever way per gene for ALL genes.\n\nT11: Use the group_by()-function to let R know, that we want to work per gene\n\n\n\n# A tibble: 2,905 √ó 2\n# Groups:   gene [2,905]\n   gene   data              \n   <chr>  <list>            \n 1 g2E09  <tibble [168 √ó 2]>\n 2 g7F07  <tibble [168 √ó 2]>\n 3 g1A01  <tibble [168 √ó 2]>\n 4 g3C09  <tibble [168 √ó 2]>\n 5 g3H08  <tibble [168 √ó 2]>\n 6 g1A08  <tibble [168 √ó 2]>\n 7 g1B01  <tibble [168 √ó 2]>\n 8 g1int1 <tibble [168 √ó 2]>\n 9 g1E11  <tibble [168 √ó 2]>\n10 g8G02  <tibble [168 √ó 2]>\n# ‚Ñπ 2,895 more rows\n\n\n\nT12: Then add a new line to your pipeline, where you add a new variable model_object to your gravier_clean_aug_long_nested-dataset, which R will compute per gene\n\nTo do this you will need the following syntax and then wrap that inside the relevant tidyverse-function for creating a new variable:\n\nmodel_object = map(.x = data,\n                   .f = ~lm(formula = log2_expr_level ~ early_metastasis,\n                            data = .x))\n\n\n\n\nMake sure to understand the map()-function here, it is completely central to functional programming with purrr:\n\nWe need the group_by() to define which variable holds the elements to each of which we want to map\nmodel_object is a new variable, we are creating, which will contain the result of our call to the map()-function\n.x to what exising (nested) variable are we mapping?\n.f which function do we want to map to each element in the existing (nested) variable?\nNote that log2_expr_level and early_metastasis are variables ‚Äúinside‚Äù the nested data-variable\n\nNote, once again, this is conceptually a super-tricky data structure, not only do we have a per gene nested tibble, but now we also have a per gene nested model object - So please do make sure to discuss in your group, what is going on here, e.g.¬†try running this and discuss what you see:\n\ngravier_clean_aug_long_nested |>\n  filter(gene == \"g2E09\") |> # Replace \"g2E09\" with whatever was YOUR favourite gene!\n  pull(model_object)\n\n\n\nTidying Models\nExcellent! Now we have a per gene model. Let us use the broom-package to extract some information from each of the models. First, to get a better understanding of what is going on when calling the tidy()-function, try running this:\n\ngravier_clean_aug_long_nested |>\n  \n  # Here, you should replace \"g2E09\" with whatever was YOUR favourite gene!\n  filter(gene == \"g2E09\") |> \n  \n  # Pull() on tibbles: This pulls out the model_object variable.\n  #   Note! This is a list, because we nested!\n  pull(model_object) |> \n  \n  # Pluck() on lists: From the list we got from the last step,\n  #   we \"pluck\" the first element\n  pluck(1) |>\n  \n  # The result of pluck, is a model object,\n  #   upon which we can call the tidy function\n  tidy(conf.int = TRUE,\n       conf.level = 0.95)\n\nNow, we want to apply this tidy()-function per model_object:\n\nT13: Scroll a bit back to where we created the model_object and see if you can translate that into mapping the tidy()-function to the model_object-variable, thereby creating a new variable model_object_tidy - This is tricky, so do make sure to discuss in your group how this can be done!\n\n\n\n\nClick here for hint\n\n\nRemember the parameters to the tidy()-functions, which gives us the confidence intervals and defines corresponding level. Also, everything you need is in the previous layout of the map()-function. Note, that the calls to the pull()- and pluck()-functions above, pertains to extracting from a tibble, which we do not want, on the contrary, we want all objects to be contained in our gravier_clean_aug_long_nested data\n\n\n\n# A tibble: 2,905 √ó 4\n# Groups:   gene [2,905]\n   gene   data               model_object model_object_tidy\n   <chr>  <list>             <list>       <list>           \n 1 g2E09  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n 2 g7F07  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n 3 g1A01  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n 4 g3C09  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n 5 g3H08  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n 6 g1A08  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n 7 g1B01  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n 8 g1int1 <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n 9 g1E11  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n10 g8G02  <tibble [168 √ó 2]> <lm>         <tibble [2 √ó 7]> \n# ‚Ñπ 2,895 more rows\n\n\nNote, once again, this is conceptually a super-tricky data structure, not only do we have a per gene nested tibble, but now we also have a per gene nested model object and now also a nested tibble of tidyed objects - So please again do make sure to discuss in your group, what is going on here, e.g.¬†try running this and discuss what you see:\n\ngravier_clean_aug_long_nested |>\n  filter(gene == \"g2E09\") |> # Replace \"g2E09\" with whatever was YOUR favourite gene!\n  pull(model_object_tidy)\n\n\n\nWrangling\nWe‚Äôre almost there - Just a bit of wrangling to go!\nJust as you saw that we could nest() on a variable (recall we did that for the gene-variable), you can do the opposite and lo and behold, that is the unnest()-function. Before we continue:\n\nT14: Create a dplyr-pipeline and save the result in a new variable called gravier_estimates: Use the unnest()-function to unpack the model_object_tidy\n\n\n\n# A tibble: 5,810 √ó 10\n# Groups:   gene [2,905]\n   gene  data     model_object term         estimate std.error statistic p.value\n   <chr> <list>   <list>       <chr>           <dbl>     <dbl>     <dbl>   <dbl>\n 1 g2E09 <tibble> <lm>         (Intercept)  -0.0162     0.0117    -1.38  1.69e-1\n 2 g2E09 <tibble> <lm>         early_metas‚Ä¶ -0.0343     0.0201    -1.71  8.99e-2\n 3 g7F07 <tibble> <lm>         (Intercept)   0.0604     0.0139     4.35  2.36e-5\n 4 g7F07 <tibble> <lm>         early_metas‚Ä¶ -0.0185     0.0238    -0.778 4.38e-1\n 5 g1A01 <tibble> <lm>         (Intercept)  -0.0290     0.0123    -2.35  1.99e-2\n 6 g1A01 <tibble> <lm>         early_metas‚Ä¶ -0.0367     0.0211    -1.73  8.47e-2\n 7 g3C09 <tibble> <lm>         (Intercept)   0.0518     0.0145     3.58  4.55e-4\n 8 g3C09 <tibble> <lm>         early_metas‚Ä¶ -0.0148     0.0249    -0.595 5.53e-1\n 9 g3H08 <tibble> <lm>         (Intercept)   0.0142     0.0128     1.11  2.69e-1\n10 g3H08 <tibble> <lm>         early_metas‚Ä¶  0.00247    0.0220     0.112 9.11e-1\n# ‚Ñπ 5,800 more rows\n# ‚Ñπ 2 more variables: conf.low <dbl>, conf.high <dbl>\n\n\n\nT15: The again, create a dplyr-pipeline and save the result in a the same gravier_estimates-variable: Subset the rows to only get the slope term and then choose variables as displayed below, finally end with un-grouping your data, as we no longer need the groups\n\n\n\n# A tibble: 2,905 √ó 5\n   gene   p.value estimate conf.low conf.high\n   <chr>    <dbl>    <dbl>    <dbl>     <dbl>\n 1 g2E09   0.0899 -0.0343   -0.0739   0.00539\n 2 g7F07   0.438  -0.0185   -0.0655   0.0285 \n 3 g1A01   0.0847 -0.0367   -0.0784   0.00508\n 4 g3C09   0.553  -0.0148   -0.0639   0.0343 \n 5 g3H08   0.911   0.00247  -0.0410   0.0459 \n 6 g1A08   0.859  -0.00363  -0.0438   0.0366 \n 7 g1B01   0.279  -0.0218   -0.0615   0.0178 \n 8 g1int1  0.666  -0.0113   -0.0627   0.0402 \n 9 g1E11   0.106  -0.0329   -0.0728   0.00703\n10 g8G02   0.633  -0.0108   -0.0555   0.0338 \n# ‚Ñπ 2,895 more rows\n\n\n\nT16: To your gravier_estimates-dataset, add a variable q.value, which is the result of calling the p.adjust()-function on your p.value-variable and also add an indicator variable denoting if a given gene is significant or not\n\n\n\n# A tibble: 2,905 √ó 7\n   gene   p.value estimate conf.low conf.high q.value is_significant\n   <chr>    <dbl>    <dbl>    <dbl>     <dbl>   <dbl> <chr>         \n 1 g2E09   0.0899 -0.0343   -0.0739   0.00539       1 no            \n 2 g7F07   0.438  -0.0185   -0.0655   0.0285        1 no            \n 3 g1A01   0.0847 -0.0367   -0.0784   0.00508       1 no            \n 4 g3C09   0.553  -0.0148   -0.0639   0.0343        1 no            \n 5 g3H08   0.911   0.00247  -0.0410   0.0459        1 no            \n 6 g1A08   0.859  -0.00363  -0.0438   0.0366        1 no            \n 7 g1B01   0.279  -0.0218   -0.0615   0.0178        1 no            \n 8 g1int1  0.666  -0.0113   -0.0627   0.0402        1 no            \n 9 g1E11   0.106  -0.0329   -0.0728   0.00703       1 no            \n10 g8G02   0.633  -0.0108   -0.0555   0.0338        1 no            \n# ‚Ñπ 2,895 more rows\n\n\nBut‚Ä¶ q.value??? What are you on about??? Here is a nice primer on ‚ÄúHow does multiple testing correction work?‚Äù\n\n\nRecap\nIf you understandbly by now, have lost a bit of overview of what is going on, let‚Äôs just re-iterate.\n\nWe have the gravier-dataset, with the log2-expression levels for 2,905 genes of 168 patients of whom 111 did not have early metastasis and 57 who did\nWe are interested in investigating what genes are significantly up-/down-regulated between the patients with- and without early metastasis\nFirst we retrieved the data from the data repository, cleaned and augmented it and saved it to disk\nThen pivotted the data, so we could work per gene (The gene-variable)\nNext, we grouped per gene and nested the data (The data-variable)\nThen, we fitted a linear model to each gene (The model_object-variable)\nNext, we used the broom-package to tidy the fitted model incl.¬†getting confidence intervals (The model_object_tidy-variable)\nLastly, we extracted the model parameters, corrected for multiple testing and added and indicator for significant findings\n\nNow, we actually have everything we need to answer:\n\nWhat genes are significantly up-/down-regulated between the patients with- and without early metastasis?‚Äú\n\nIn the following, we will use a level of significance of \\(\\alpha = 0.05\\) to provide this answer.\n\n\nVisualise\nRight, let‚Äôs get to it!\n\nT17: Re-create this forest-plot to finally reveal the results of your analysis GROUP ASSIGNMENT part I\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nHere, you will have to use your indicator variable to identify significant genes before plotting and then it would probably be prudent to take a look at the fct_reorder()-function and geom_errorbarh()-representation\n\n\nT18: Re-create this volcano-plot to finally reveal the results of your analysis GROUP ASSIGNMENT part II\n\n\n\n\n\n\n\n\n\nClick here for hint\n\n\nHere,before plotting you will have to create a new label-variable which takes value gene for significant genes and otherwise is simply an empty string, which we denote by \"\". Also, perhaps the ggrepel-package would be relevant for somehow adding text/labels"
  },
  {
    "objectID": "lab06.html#group-assignment",
    "href": "lab06.html#group-assignment",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "GROUP ASSIGNMENT",
    "text": "GROUP ASSIGNMENT\nFor the group assignment this time, you will use T17 and T18 to again create a reproducible micro-report and make sure to:\n\nRead the assignment instructions\nRead and follow the code styling guidelines"
  },
  {
    "objectID": "lab06.html#optional",
    "href": "lab06.html#optional",
    "title": "Lab 6: Applying Functional Programming with Purrr to Models",
    "section": "Optional",
    "text": "Optional\nThe following is only if you can find the time! But, perhaps this would be something interesting to revisit during the project period\n\nPCA\n\nGo and visit this blog post by Claus O. Wilke, Professor of Integrative Biology.\nYour task is to work through the blog post using the gravier-dataset to crate a PCA-analysis\n\n\n\nK-means\n\nGo and visit this K-means clustering with tidy data principles post\nYour task is to work through the blog post using the gravier-dataset to crate a kmeans-analysis"
  },
  {
    "objectID": "lab07.html",
    "href": "lab07.html",
    "title": "Lab 7: Collaborative Bio Data Science using GitHub via RStudio",
    "section": "",
    "text": "usethis\ngitcreds\ngit (Actually not an R-package this time)"
  },
  {
    "objectID": "lab07.html#schedule",
    "href": "lab07.html#schedule",
    "title": "Lab 7: Collaborative Bio Data Science using GitHub via RStudio",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.10: Midway evaluation\n08.10 - 08.30: Recap of Lab 6\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab07.html#learning-materials",
    "href": "lab07.html#learning-materials",
    "title": "Lab 7: Collaborative Bio Data Science using GitHub via RStudio",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: Happy Git and GitHub for the useR ‚Äì Read chapters 1 (intro), 20 (basic terms), 22 (branching), 23 (remotes). Do not pay much attention to syntax of specific commands, because we are not going to use them during the exercises, focus on the idea\nBook: Introduction to Data Science - Data Analysis and Prediction Algorithms with R by Rafael A. Irizarry: Chapter 40 Git and GitHub ‚Äì Some of the information here is redundant with the previous book, but very important thing is a visualization of basic git actions and screenshots of how to perform them using RStudio\nVideo: RStudio and Git - an Overview (Part 1) ‚Äì Basic git concepts, for those who prefer listen rather than read. Books, however, contain more information\nVideo: How to use Git and GitHub with R ‚Äì Basic operating on git in RStudio. Complementary to second book. You can skip to 2:50, we are not going to link to git manually either way"
  },
  {
    "objectID": "lab07.html#learning-objectives",
    "href": "lab07.html#learning-objectives",
    "title": "Lab 7: Collaborative Bio Data Science using GitHub via RStudio",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nExplain why reproducible data analysis is important\nIdentify associated relevant challenges\nDescribe the components of a reproducible data analysis\nUse RStudio and GitHub (git) for collaborative bio data science projects"
  },
  {
    "objectID": "lab07.html#sec-exercises",
    "href": "lab07.html#sec-exercises",
    "title": "Lab 7: Collaborative Bio Data Science using GitHub via RStudio",
    "section": "Exercises",
    "text": "Exercises\n\nPrologue\n\n\n\n\n\nUsing git is completely central for collaborative bio data science. You can use git not only for R, but for any code-based collaboration. All bioinformatics departments in major players in Danish Pharma, are using git - Learning git is a key bio data science skill! Today, you will take your first steps - Happy Learning!\n\nT1: Find the GitHub repository for the ggplot2 R-package\nQ1: How many Contributors are there?\nT1: Find the GitHub repository for the Linux kernel\nQ2: How many Contributors are there?\nQ3: Discuss in your group why having an organised approach to version control is central? And consider the simple contrast of the challenges you were facing when doing the course assignments.\n\n\n\nGetting Started\nThe following exercises have to be done in your groups! You must move at the same pace and progress together as a team through the exercises!\nGitHub is the place for collaborative coding and different group members will have to do different tasks in a specific order, to make it through the exercises together, so‚Ä¶ Team Up and don‚Äôt rush it!\nFirst, select a team Captain, that person will have to carry out specific tasks. If Team is stated, then that refers to all group members and lastly if Crew is stated, then that is everyone but the Captain. Please note, that tasks are sequential, so if a task is assigned to the Captain, then the Crew has to await completion before proceeding!\n\nTeam\n\nGo to GitHub and login\nIn the upper left corner, click Repositories\n\n\n\nCaptain\n\nThe default owner of the repository should be your GitHub username. Keep that, please.\nName the repository groupXX, where XX is your group number, e.g.¬†02\nSelect Public\nTick Add a README file\nClick Create repository\nClick Settings in the menu line starting with <> Code\nUnder Access, click Collaborators and teams\nClick Add people\nWrite a group members username\nClick the correct suggestion\nSelect Add username to this repository\nRepeat for all group members\n\n\n\nCrew\n\nCheck your mail and accept your invitation to join the repository\n\n\n\nTeam\n\nClick here to go to the course RStudio cloud server and login\nIn the upper right corner, where it should say r_for_bio_data_science, click\nChoose  New Project...\nSelect  Version Control\nSelect  Git\nUnder Repository URL:, enter https://github.com/CAPTAIN_USERNAME/groupXX, where again you replace XX with your group number, e.g.¬†02. NOTE: Replace CAPTAIN_USERNAME with the captain‚Äôs GitHub username.\nUnder Project directory name:, enter lab07_git_exercises\nUnder Create project as a subdirectory of:, make sure that is says ~/projects\nClick Create Project\n\nCongratulations! You have just cloned your first GitHub repository!\nIn your Files-pane, you should now see:\n\n\n\n\n\n‚Ä¶and now in your Environment-pane, you should see a new git-tab:\n\n\n\n\n\nIf you click the git-tab, you should see:\n\n\n\n\n\n\n\n\nYour first collaboration\n\nTeam\n\nCreate a new Quarto document, title it student_id and save id as student_id.qmd, where student_id is your‚Ä¶ You guessed it!\nIn the environment-pane, click the git-tab\nTick the 3 boxes under staged\nClick commit\nIn the upper right corner, add a Commit message, e.g ‚ÄúFirst commit by student_id‚Äù\nClick the Commit-button\nA pop-up, will give you details on your commit, look through them and then click Close\nNow, very important ALWAYS click the  Pull-button BEFORE clicking the  Push-button\nClicking Pull, you should see ‚ÄúAlready up to date.‚Äù\nThen click Push\n\n\nQ4: Discuss in your group, what happened and why? Try copying the message and paste it into google, did you get any hits?\n\nDo not start solving this issue just yet, simply realise, that git is being used by literally millions of people on a daily basis, the chance that you will encounter a unique error, that no one else have ever seen is slim-to-nothing! So if you end up in trouble, googl‚Äôing the message, is not a bad initial stab at the problem!\n\n\nSetting up your Credentials and Personal Access Token (Team)\nGitHub doesn‚Äôt know who you are, you‚Äôre just someone who cloned your first GitHub repository and now you want to do all sorts of stuff! We can‚Äôt have that, so let‚Äôs fix that GitHub doesn‚Äôt know who you are!\n\nIn the Console, run the command:\n\n\nusethis::use_git_config(user.name = \"YOUR_GITHUB_USERNAME\", user.email = \"THE_EMAIL_YOU_USED_TO_CREATE_YOUR_GITHUB_ACCOUNT\")\n\n\nIn the Console, run the command:\n\n\nusethis::create_github_token()\n\n\nLogin\nUnder Note, where it says DESCRIBE THE TOKEN'S USE CASE, delete the text and write e.g.R for Bio Data Science lab 7 git exercises\nUnder Expiration, where it says 30 days, change that to 90 days\nDo not change any other setting, but simply scroll down and click Generate token\nA new Personal access tokens (classic)-page will appear, stating your personal access token, which starts with ghp_, go ahead and copy it\nStore the PAT somewhere save, e.g.¬†in a password management tool\nAgain in the console, run the below command and enter your PAT when prompted:\n\n\ngitcreds::gitcreds_set()\n\n\nIn the Environment-pane, make sure you are in the git-tab and again click Pull (We always pull before push)\nClick Push\nGo to your groups GitHub-project page https://github.com/CAPTAIN_USERNAME/groupXX\nConfirm, that you see your files\n\nCongratulations! You just took your first step towards true collaborative bio data science!\nStop, wait a minute and make sure, that ALL group members are at this stage of the exercises.\n\n\nMaking your Credentials and Personal Access Token stick around (Team)\nThe thing is, we‚Äôre actually working on a Linux server her, this entails a that you‚Äôre credentials and PAT exists in a cache, which is cleared every 15 minutes. This. Is. Annoying! So let‚Äôs fix it!\nFirst of all RStudio is a (G)raphical (U)ser (I)nterface, meaning, that it allows you to do pointy-clicky stuff, but at the end of the day, everything happens at the commandline. That also goes for git, so your button-pushing simply gets converted into commands, which are executed at the commandline. If you are not comfortable with the commandline, don‚Äôt worry, the pointy-clicky stuff will be sufficient for now! If you are comfortable with the commandline, I advice to get comfortable with git on the commandline as well, you do get some extra bang-for-the-buck!\nNow, a brief visit of the commandline:\n\nIn the Console-pane, click the Terminal-tab, which gains you access to the system underlying RStudio\nYou should see something like user@pupilX:/net/pupilX/home/people/user/projects/lab07_git_exercises$\nEnter ls -a, this will list all the files, compare with your Files-pane, you will see that RStudio is ‚Äúhiding‚Äù something from you, namely the .git folder\nEnter ls -a .git and you will see the content of the folder, this is where all the git-magic happens\nSee the config file? That holds the configuration, enter git config --global --list\nRecall you entered your GitHub username and mail? This is where it ended up! Note the credential.helper=cache, which tells us, that the credentials are being cached. Now, enter git config --global credential.helper 'cache --timeout=86400'\nRe-run the command git config --global --list and confirm the change\nGo back to your Console and run the command gitcreds::gitcreds_set() and re-enter your PAT\n\nCongratulations! You have now used the commandline and you will forever be part of an elite few, who know that everything you see in hacker-movies is BS, except perhaps for Wargames and Mr.¬†Robot! Also, your PAT will now be good for 24h\n\n\n\nYour next collaboration\nThat first collaboration was easy right!? Well, you were all working on different files‚Ä¶\n\nCaptain\n\nCreate a new Quarto Document, title it ‚ÄúGroup Document‚Äù and save it as group_document.qmd\nUsing markdown headers, create one section for each group member, incl.¬†yourself. Here, you can use your names, student ids or whatever you deem appropriate\nAgain in the Environment-pane, make sure you are in the git-tab and then tick the box next to group_document.qmd\nClick Commit\nAdd a commit message, e.g.¬†‚ÄúAdd group document‚Äù\nClick the Commit-button\nClick Pull (You should be ‚ÄúAlready up to date.‚Äù)\nClick Push\nClick Close\nAgain, go to the group GitHub and confirm that you see the new document you just created\n\n\n\nCrew\n\nClick Pull and confirm that you now also have the file group_document.qmd\nOpen group_document.qmd and find your assigned section\nIn your section and your section only, enter some text, add a few code chunks with some R-code\nMake sure to save the document\nNow again, find the group_document.qmd in the git-tab of the Environment-pane and tick the box under Staged\nClick Commit\nNote how your changes to the document are highlighted in green\nAdd a commit message, e.g.¬†‚ÄúUpdate the STUDENT_ID section‚Äù and click Commit\nClick Close\nClick Pull\nClick Push\nGo to the group GitHub, find the group_document.qmd and click it, do you see your changes?\n\nOnce everyone has added to their assigned section, everyone should do a pull/push, so everyone has the complete version of the group_document.qmd.\n\n\n\nYour first branching\nOk that‚Äôs pretty great so far - Right? The thing is‚Ä¶ Consider, the ggplot2 repository, that you found in T1. Thousands of companies and even more thousands of people rely on ggplot for advanced data visualisation. What would happen, if you wanted to add a new feature or wanted to optimise an existing one, while people were actively installing your package? They would get what stage your code was in, which may or may not be functional - Enter branching!\nBelow here, is an illustration, consider the Master the stable version people can download and use and then Your Work will be the feature or update that you personally are working on. Someone Else‚Äôs Work will be another team and that persons work on a new feature or update. Before the last green circle, note how both the Your Work- and Someone Else‚Äôs Work-branches are merged onto the Master-branch.\n\n\n\n\n\nThere can be even more branches\n\nQ4: Again, find the ggplot GitHub site and see if you can find how many branches there are?\n\n\nTeam\n\nGo to your RStudio session and in the git-tab of the Environment-pane, click  New Branch\nIn branch name, enter your student id\nYou should see a pop-up Branch 'STUDENT_ID' set up to track remote branch 'STUDENT_ID' from 'origin'., go ahead and click Close\nNext to where you clicked New Branch just before, it should now say STUDENT_ID, click it and confirm that you see STUDENT_ID and Main\nLook at the illustration above and make sure you are on par with that from the original branch main, which is equivalent with Master, you have created a new branch STUDENT_ID, which is equivalent to Your Work\nClick STUDENT_ID and you will get a confirmation that you are already on that branch and that you are up-to-date, click Close\nIn the group_document.qmd under your section, using markdown, enter a new sub-header and name it e.g.¬†New feature or New analysis\nEnter some text, a few code chunks with a bit of R-code of your choosing and save the document\nAgain, in the git-tab, tick the box under staged and click Commit\nAdd a commit message, e.g.¬†New feature from STUDENT_ID and click Commit\nClick Close, Pull, Close, Push and Close and then close the commit window\nGo to your group GitHub and confirm that you now have at least 2 branches\nMake sure you are in the main branch and then click the group_document.qmd, you should now not see your new feature/analysis that you added\nOn the left, where it says main, click and select your STUDENT_ID, you should now see your new feature/analysis that you added\n\nCongratulations! You have now succesfully done your first branching!\n\n\n\nYour first branch merging\n\nGo to your groups GitHub page, at the top it should say STUDENT_ID had recent pushes..., click the Compare & pull request\nYour commit message will appear and where it says Leave a comment, add a comment like e.g.¬†I'm done, all seems to be working now! or similar\nClick Create pull request\nIt should now say This branch have no conflicts with the base branch, confirm and click Merge pull request\nClick Confirm merge after which it should now say Pull request successfully merged and closed\nYou have now fully merged, so go ahead and click Delete branch\nRevisit the previous illustration and compare with your branch workflow, make sure that everyone in the groups are on par here\nFinally return to your RStudio session and make sure you switch to the main-branch\n\nCongratulations! You have now succesfully done your first branch merge!\nBut wait, what was this Pull request??? What you actually did, was: 1. Created a new branch 1. Completed a new feature/analysis 1. Push‚Äôed the new feature/analysis to GitHub 1. Created a Pull request for merging your branch STUDENT_ID into the main branch 1. Approved and completed the Pull request\nThink about e.g.¬†again the ggplot2-repo, if anyone could create a new branch and then do as described above, then there would be no way of making quality control. Therefore, typically such pull requests will have to be approved by someone. This can either be someone who is close-to-the-code e.g.¬†in the case of ggplot2, that‚Äôd be someone like Hadley. In a company, then that might be some senior developer approving junior developers pull requests. At one point you might have seen something like ‚ÄúThe min branch is unprotected‚Äù, this is exactly that!\n\n\nYour first merge conflict\nOkay, that‚Äôs all good an well. Seems easy and straight forward, right? Well, that is as long as we don‚Äôt have a conflict. A conflict is when two or more changes to one file are not compatible, consider:\n\nTop 10 Data Science Languages of ALL Time:\n10. It is \n9. Impossible\n8. to rank them\n7. Because\n6. programming is subjective\n5. and everyone\n4. has\n3. different\n2. tastes\n1. R\n\nIn fact, let‚Äôs screw things up!\n\nCaptain\n\nGo to the RStudio session and where you usually click Quarto Document..., this time let‚Äôs just create a simple Text File\nCopy/paste the Top 10 Data Science Languages of ALL Time into the file\nSave the file as best_ds_langs.txt\nMake sure you are in the main-branch\nDo a commit/pull/push and check the file ended at your group github\n\n\n\nCrew\nLet‚Äôs commit mutiny! (pun intended)\n\nMake sure you are in the main-branch and then hit Pull\nOpen the best_ds_langs.txt\nEach of you separatly (wrongfully) replace R with an (inferior) data science language of your choice (you can really write anything)\nNow, this time we do not pull first, simply commit and then push\nNow you can pull\nYou will now get notified about a merge conflict\nClose and re-open the best_ds_langs.txt-file, it should look something like:\n\n\nTop 10 Data Science Languages of ALL Time:\n10. It is \n9. Impossible\n8. to rank them\n7. Because\n6. programming is subjective\n5. and everyone\n4. has\n3. different\n2. tastes\n<<<<<<< HEAD\n1. C++\n=======\n1. Python\n>>>>>>> 85283ca3246b7f7462ab633085f92ac5f173d3e7\n\n\n\nCaptain\nGet your Crew in order!\n\nSimply edit the file so that you delete everything below 2. and add the final line, which emphasises that 1. R is superiour!\nThe box under Staged looks a bit odd right, just click it\nClick Commit and add a commit message, e.g.¬†Fixed merge conflict, got crew back in order!\nClick Commit, Close, Pull, Close, Push, Close and close the commit window\nReturn to your GitHub group and confirm all is in order and you can continue with confidence knowing that R is superiour! (Check the file to make sure)\n\nCongratulations! You have now fixed your first merge conflict!\nLastly, click the History-button next to the Push-button and explore the history of how your branches and commits have changed through these exercises.\n\n\n\nThe .gitignore file\nYou may have noticed the .gitignore-file?\n\nTeam\n\nGo to your RStudio session, find the .gitignore-file and click it\n\nIt contains a list of files and folders, which should not end up at your git-repo, i.e.¬†files which should be ignored. An important aspect of working with GitHub is that GitHub is meant for code, not data! Let‚Äôs say we had a data and a data/_raw/, we could add those to the .gitignore-file to avoid the data being included in our commits.\n\nUpdate the .gitignore-file and get the updated version to GitHub\n\n\n\n\nSummary\nYou have not gotten to play around with collaborative coding using git - Well done!\nIf you are more curious for more, please feel free to play around with a new file, edit commit/pull/push etc. Perhaps also take an extra look at the GitHub, explore and learn more üëç\n\n\nGROUP ASSIGNMENT\nFirst of all, here we have just scratched the surface. You can read much more here.\nYour assignment this time, will be to:\n\nGo to this post on ‚ÄúPCA tidyverse style‚Äù by Claus O. Wilke, Professor of Integrative Biology\nAgain, create a reproducible micro-report, this time, where you do a code-along with either the data in the post or e.g.¬†the gravier-data or ?\nUse your GitHub group repository to collaborate\nYour hand in will be a link to your micro-report in your GitHub group repository\nMake sure to check the Assignment Guidelines\nAnd also follow the Course Code Styling\nHOW TO HAND IN: Go to https://github.com/CAPTAIN_USERNAME/groupXX, copy the link and paste it into an empty text (.txt) file. Hand in this text file.\n\nNote, focus here is on the git learning objectives and doing a code-along, which is not a copy/paste of the code, but using it as inspiration to create your own nice concise tidy micro-report!"
  },
  {
    "objectID": "lab08.html",
    "href": "lab08.html",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "",
    "text": "devtools\nusethis\nroxygen2\ntestthat\ngert"
  },
  {
    "objectID": "lab08.html#schedule",
    "href": "lab08.html#schedule",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.15: Recap of Lab 7\n08.15 - 08.45: Lecture\n08.45 - 09.00: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab08.html#learning-materials",
    "href": "lab08.html#learning-materials",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials:\n\nPrimer on R Packages\nCheatsheet: Package development with devtools ‚Äì When in doubt, check here first\nWeb: R Packages: A Beginner‚Äôs Tutorial ‚Äì Read this before class\nWeb: Developing Packages with RStudio ‚Äì Creating an R Package with RStudio\nWeb: R package primer a minimal tutorial ‚Äì Brief but comprehensive R Package tutorial\nBook: R Packages - 1 Introduction ‚Äì Everything you need to know about R Packages"
  },
  {
    "objectID": "lab08.html#learning-objectives",
    "href": "lab08.html#learning-objectives",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nPrepare a simple R package for distributing documented functions\nExplain the terms Repository, Dependencies, and Namespace\nImplement testing in an R package\nCollaboratively work on an R package on GitHub"
  },
  {
    "objectID": "lab08.html#sec-exercises",
    "href": "lab08.html#sec-exercises",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "Exercises",
    "text": "Exercises\nRead the steps of this exercises carefully while completing them\n\nIntroduction\nThe aim of these exercises is to set up a collaborative coding project using GitHub and collaborate on creating a simple R package that replicates the central dogma of molecular biology.\nThe exercises will build upon what you learned in Lab 7 and its exercises. But don‚Äôt worry too much, the setup steps will be given here as well.\n\n\n\nTASKS\nIf you haven‚Äôt read the Primer on R packages yet, consider reading it now.\n\n\nTask 1 - Setting up the R package\nIn the first task, one team member (you decide who) will initiate a package project and push it to Github. Then, when instructed, the remaining group members will connect to that repository, and the real package building will begin.\n\nTask 1.1 - Create R Package\n\nGo to R for Bio Data Science RStudio Server and login.\nClick Create a project in the top left and choose New Directory.\nSelect R Package and pick a fitting Package name.\n\nThe package you will create will replicate the central dogma of molecular biology. Let that inspire you\nLook up naming rules here\nThe most important rule: _, -, and ' are not allowed.\n\nTick the ‚ÄúCreate git repository‚Äù box.\nClick ‚ÄúCreate project‚Äù.\n\nRStudio will now create an R project for you that contains all necessary files and folders.\n\nOpen the DESCRIPTION file and write a title for your package, a small description, and add authors.\n\n\nWhen adding authors, the format is:\n\n\nAuthors@R:\n  c(person(given = \"firstname\",\n           family = \"lastname\",\n           role = c(\"aut\", \"cre\"), # There must be a \"cre\", but there can only be one\n           email = \"your@email.com\"), \n    person(given = \"firstname\",\n           family = \"lastname\",\n           role = \"aut\",\n           email = \"your@email.com\"))\n\n\nCreate an MIT license (from the console) usethis::use_mit_license(\"Group name\")\nIf you didn‚Äôt tick the ‚ÄúCreate git repository‚Äù: Run usethis::use_git(). If the ‚ÄúGit‚Äù tab is not in the lower left panel: reopen the R project.\n\n\nOptional setup steps (generally good, but not essential for this course)\n\n\n\nAdd a Readme file usethis::use_readme_rmd( open = FALSE ). You will do that in the Group Assignment later\n\n\nAdd a lifecycle badge: usethis::use_lifecycle_badge( \"Experimental\" )\n\n\nWrite vignettes: usethis::use_vignette(\"Vignette name\")\n\n\n\n\n\n\nTask 1.2 - Setup GitHub Repository for your group‚Äôs R package\nDuring the previous class, you have been using a combination of console and terminal to set up GitHub. This week, you will solely be using the console.\nStill only the first team member\n\nGo to https://github.com/rforbiodatascience23\nClick the green New-button\nCreate a new repository called group_X_package. Remember to replace the X\nSelect rforbiodatascience23 as owner\nMake the repository Public\nClick the green Create repository-button\nIn this new repository, click the settings tab\nClick Collaborators and Team\nClick the green Add people-button\nInvite the remaining group members\nAll other team members should now have access to the repository, but do not create your own project yet.\n\n\n\nTask 1.3 - Connect your RStudio Server Project to the GitHub Repository\nStill only the first team member\n\nFind your PAT key or create a new\n\n\nHow to create a new one\n\n\n\nType in the console usethis::create_github_token(). You‚Äôre going to be redirected to GitHub website.\n\nIn case you did that step manually, remember to give permission to access your repositories.\n\nYou need to type your password. Don‚Äôt change the default settings of creating the token except for the description ‚Äì set ‚ÄòRStudio Server‚Äô or something similar. Then hit Generate token.\n\nCopy the generated token (should start with ghp_) and store it securely (e.g., in a password manager). Do not keep it in a plain file.\n\n\nGo back to the RStudio Server project\nType in the console gitcreds::gitcreds_set() and paste the PAT key\nStage all files (in the git window in the top right) by ticking all boxes under Staged\nStill in the console, run the following commands. Replace your group number and use your GitHub username and email. You run these to link your project to the GitHub repository you created. Remember to replace the X.\n\n\nusethis::use_git_remote(name = \"origin\", \"https://github.com/rforbiodatascience23/group_X_package.git\", overwrite = TRUE) # Remember to replace the X\nusethis::use_git_config(user.name = \"USERNAME\", user.email = \"USEREMAIL@EXAMPLE.ORG\")\ngert::git_commit_all(\"Package setup\")\ngert::git_push(\"origin\")\n\nAll other team members\nAfter your teammate has pushed to GitHub.\n\nIn the RStudio Server, create a new project based on the GitHub Repository just created by your teammate.\n\nClick Create a project in the top left.\nChoose Version Control and then Git.\nPaste in the repository URL: https://github.com/rforbiodatascience23/group_X_package.git. Remember to replace the X.\nGive the directory a fitting name and click Create Project.\n\nFind your PAT key or create a new\n\n\nHow to create a new one\n\n\n\nType in the console usethis::create_github_token(). You‚Äôre going to be redirected to GitHub website.\n\nIn case you did it manually, remember to give permission to access your repositories.\n\nYou need to type your password. Don‚Äôt change the default settings of creating the token except for the description ‚Äì set ‚ÄòRStudio Server‚Äô or something similar. Then hit Generate token.\n\nCopy the generated token (should start with ghp_) and store it securely (e.g.¬†in password manager). Do not keep it in a plain file.\n\n\nGo back to the RStudio Server project\nType in the console gitcreds::gitcreds_set() and paste the PAT key\nStill in the console, run the following commands. Replace your GitHub username and email.\n\n\nusethis::use_git_config(user.name = \"USERNAME\", user.email = \"USEREMAIL@EXAMPLE.ORG\")\n\nIf RStudio at any point asks you to log in to GitHub, redo step 4 and 5.\nNow you are ready to work on the R package.\n\n\n\nTask 2 - Build the package\n\nEach team member should build and implement their own function. The code will be given, but you will be asked to come up with a name for your function and many of the variables, so discuss in the team what naming convention you want to use. Do you want to use snake_case (common in tidyverse) or camelCase (common in Bioconductor)? It doesn‚Äôt really matter, but be consistent.\n\n\nTask 2.1 - Incorporate data\nIn this task you will include the following codon table in your package.\nThis task should be done by one team member. The rest should follow along.\n\nBelow is a standard codon table. Store it in an object with a name of your own choosing.\n\n\nc(\"UUU\" = \"F\", \"UCU\" = \"S\", \"UAU\" = \"Y\", \"UGU\" = \"C\",\n  \"UUC\" = \"F\", \"UCC\" = \"S\", \"UAC\" = \"Y\", \"UGC\" = \"C\",\n  \"UUA\" = \"L\", \"UCA\" = \"S\", \"UAA\" = \"_\", \"UGA\" = \"_\",\n  \"UUG\" = \"L\", \"UCG\" = \"S\", \"UAG\" = \"_\", \"UGG\" = \"W\",\n  \"CUU\" = \"L\", \"CCU\" = \"P\", \"CAU\" = \"H\", \"CGU\" = \"R\",\n  \"CUC\" = \"L\", \"CCC\" = \"P\", \"CAC\" = \"H\", \"CGC\" = \"R\",\n  \"CUA\" = \"L\", \"CCA\" = \"P\", \"CAA\" = \"Q\", \"CGA\" = \"R\",\n  \"CUG\" = \"L\", \"CCG\" = \"P\", \"CAG\" = \"Q\", \"CGG\" = \"R\",\n  \"AUU\" = \"I\", \"ACU\" = \"T\", \"AAU\" = \"N\", \"AGU\" = \"S\",\n  \"AUC\" = \"I\", \"ACC\" = \"T\", \"AAC\" = \"N\", \"AGC\" = \"S\",\n  \"AUA\" = \"I\", \"ACA\" = \"T\", \"AAA\" = \"K\", \"AGA\" = \"R\",\n  \"AUG\" = \"M\", \"ACG\" = \"T\", \"AAG\" = \"K\", \"AGG\" = \"R\",\n  \"GUU\" = \"V\", \"GCU\" = \"A\", \"GAU\" = \"D\", \"GGU\" = \"G\",\n  \"GUC\" = \"V\", \"GCC\" = \"A\", \"GAC\" = \"D\", \"GGC\" = \"G\",\n  \"GUA\" = \"V\", \"GCA\" = \"A\", \"GAA\" = \"E\", \"GGA\" = \"G\",\n  \"GUG\" = \"V\", \"GCG\" = \"A\", \"GAG\" = \"E\", \"GGG\" = \"G\")\n\n\nRun usethis::use_data(NAME_OF_YOUR_OBJECT, overwrite = TRUE, internal = TRUE)\nYou have now made the data available to our functions, but we also want to make it visible for our users.\n\nRun usethis::use_data(NAME_OF_YOUR_OBJECT, overwrite = TRUE).\n\nWrite a data manual (document the data).\n\nAll non-internal data should be documented in a data.R file in the R folder. Create it with usethis::use_r(\"data\")\nAdd the following scaffold to R/data.R and write a very brief description of the data (see an example here). Don‚Äôt spend a lot of time here.\n\n\n\n#' Title\n#' \n#' Description\n#' \n#' \n#' @source \\url{https://www.ncbi.nlm.nih.gov/Taxonomy/Utils/wprintgc.cgi?chapter=tgencodes#SG1}\n\"NAME_OF_YOUR_OBJECT\"\n\nNormally, you should also describe how the raw data was cleaned. You would do that in the file that opens after running usethis::use_data_raw( name = \"NAME_OF_YOUR_OBJECT\", open = TRUE ), but that is less relevant here, so we will skip that part.\nYour package now includes some data.\nRestart R, clean your Environment (small broom in the Environment tab), run the three lines of code from The Package Workflow section:\n\nrstudioapi::documentSaveAll()  # Saves all you files\ndevtools::document()  # Writes all your manuals for you\ndevtools::load_all()  # Simulates library(\"your package\"), allowing you to use your functions\n\nAnd run ?NAME_OF_YOUR_OBJECT. OBS For some unknown reason, this does not work on the current R version (4.3.1). Instead, you can click the ‚ÄúBuild‚Äù tab next to the ‚ÄúGit‚Äù tab and then ‚Äúinstall‚Äù. After installing and attaching, your manual should pop up. Try printing the object as well to see what it looks like print(NAME_OF_YOUR_OBJECT).\nPush your changes to GitHub. The other team members pull the changes to have the data available to you as well.\n\n\n\nTask 2.2 - Implement functions\nIn this task you will be working individually to implement a function each. If you are fewer in the team than the number of functions. The quickest to finish can implement the remaining, or separate them out as you see fit.\nIf you want an additional challenge, each team member can create their own Git branch gert::git_branch_checkout(\"NAME_OF_BRANCH\") and work there. When done, merge your branch with the main branch. That is a more clean and ‚Äòproper‚Äô workflow, but completely optional.\nIf you are in doubt what the underlying functions do, run ?function_name to get a hint about their purpose. OBS For some unknown reason, this does not work on the current R version (4.3.1). As mentioned with the data, you can install your package, and then load the manual. Also, remember the functions are replicating the central dogma. Let that inspire you, when naming the functions and variables. If you get stuck, ask your teammates about their functions.\nFunction five is a bit more involved. Do it together or help your teammate out if it causes problems.\nFor each function (found below), complete the following steps:\n\nLook carefully at your function. Choose a fitting name for it\nRun usethis::use_r(\"function_name\") to create an .R file for the function\nPaste in the function and rename all places it says name_me with fitting names\nClick somewhere in the function. In the toolbar at the very top of the page, choose Code and click Insert Roxygen Skeleton\nFill out the function documentation\n\nGive the function a title\nDo not write anything after @export\nThe parameters should have the format: @param param_name description of parameter\nThe return has the format: @return description of function output\nExamples can span multiple lines and what you write will be run, so make it runnable.\nImportant! Either fill out everything or delete what you don‚Äôt. Otherwise, the package check will fail (Do not delete @export for these functions).\n\nRun the three lines of codes from The Package Workflow section\n\n\nIf at this point, you get a warning that NAMESPACE already exists, delete it and try again.\n\n\nrstudioapi::documentSaveAll()  # Saves all you files\ndevtools::document()  # Writes all your manuals for you\ndevtools::load_all()  # Simulates library(\"your package\"), allowing you to use your functions\n\n\nView your function documentation with ?function_name\nDefining a test or a series of tests around your newly created function ensures future corrections will not yield undesired results. Create such a test for your function. Run usethis::use_test(\"function_name\") and write a test. Draw some inspiration from here or from running ?testthat::expect_equal.\n\nSkip this step for function five. Instead, write inline code comments for each chunk of code. Press Cmd+Shift+c / Ctrl+Shift+C to comment your currently selected line.\n\nRerun the three lines from The Package Workflow section and check that the package works with devtools::check()\n\n\n\nBriefly about check\n\ndevtools::check() will check every aspect of your package and run the tests you created. You will likely get some warnings or notes like ‚ÄòNo visible binding for global variables‚Äô. They are often harmless, but, if you like, you can get rid of them as described here. The check will tell you what might cause problems with your package and often also how to fix it. If there are any errors, fix those. Warnings and notes are also good to address. Feel free to do that if any pops up.\n\n\nIf it succeeds without errors, push your changes to GitHub\n\nIf you decided to create branches review last week‚Äôs exercises for guidance\nAlways pull before pushing\nUse the RStudio GUI if you prefer\nRemember to pull first\nIf you chose to create your own branch, merge it with master/main.\n\n\n\n\nFunction one\n\n\nname_me1 <- function(name_me2){\n  name_me3 <- sample(c(\"A\", \"T\", \"G\", \"C\"), size = name_me2, replace = TRUE)\n  name_me4 <- paste0(name_me3, collapse = \"\")\n  return(name_me4)\n}\n\n\n\n\nFunction two\n\n\nname_me1 <- function(name_me2){\n  name_me3 <- gsub(\"T\", \"U\", name_me2)\n  return(name_me3)\n}\n\n\n\n\nFunction three\n\n\nname_me1 <- function(name_me2, start = 1){\n  name_me3 <- nchar(name_me2)\n  codons <- substring(name_me2,\n                      first = seq(from = start, to = name_me3-3+1, by = 3),\n                      last = seq(from = 3+start-1, to = name_me3, by = 3))\n  return(codons)\n}\n\n\n\n\nFunction four\n\nNAME_OF_YOUR_OBJECT refers to the codon table you stored in Task 2.\n\nname_me <- function(codons){\n  name_me2 <- paste0(NAME_OF_YOUR_OBJECT[codons], collapse = \"\")\n  return(name_me2)\n}\n\n\n\n\nFunction five\n\nThis function will be the first to use dependencies. As a reminder, a dependency is a package that your package depends on. In this case, it will be stringr and ggplot2. They are already installed, so you don‚Äôt need to do that. The best way to add these packages to your own is to first add them to your package dependencies with usethis::use_package(\"package_name\"). If you care about reproducibility, you can add min_version = TRUE to the function call to specify a required minimum package version of the dependency. Run usethis::use_package for both dependencies.\nFor the ggplot2 functions, we will use ggplot2::function_name everywhere a ggplot2 function is used. Also add @import ggplot2 to the function description (this is often done with ggplot2 because it has a lot of useful plotting functions with only rare name overlaps). If you want to be more precise, add @importFrom ggplot2 ggplot aes geom_col theme_bw theme instead. The same applies for stringr, but since we only use a few functions, add @importFrom stringr str_split boundary str_count to the function description.\n\nname_me1 <- function(name_me2){\n  name_me3 <- name_me2 |>  \n    stringr::str_split(pattern = stringr::boundary(\"character\"), simplify = TRUE) |>\n    as.character() |> \n    unique()\n  \n  counts <- sapply(name_me3, function(amino_acid) stringr::str_count(string = name_me2, pattern =  amino_acid)) |>  \n    as.data.frame()\n  \n  colnames(counts) <- c(\"Counts\")\n  counts[[\"Name_me2\"]] <- rownames(counts)\n  \n  name_me4 <- counts |>  \n    ggplot2::ggplot(ggplot2::aes(x = Name_me2, y = Counts, fill = Name_me2)) +\n    ggplot2::geom_col() +\n    ggplot2::theme_bw() +\n    ggplot2::theme(legend.position = \"none\")\n  \n  return(name_me4)\n}\n\n\n\n\n\n\nTask 3 - Group discussion\n\nDescribe each function to each other in order - both what it does and which names you gave them and their variables.\nThe person(s) responsible for function five, describe how you added the two packages as dependencies.\nDiscuss why it is a good idea to limit the number of dependencies your package has. When can‚Äôt it always be avoided?\nDiscuss the difference between adding an @importFrom package function tag to a function description compared to using package::function(). Read this section if you are not sure or just want to learn more."
  },
  {
    "objectID": "lab08.html#group-assignment",
    "href": "lab08.html#group-assignment",
    "title": "Lab 8: Creating a Simple R-package",
    "section": "GROUP ASSIGNMENT",
    "text": "GROUP ASSIGNMENT\n\nFor this week‚Äôs group assignment, write a vignette (user guide) for your package (max 2 pages). The vignette should include a brief description of what the package is about and demonstrate how each function in the package is used (individually and in conjunction with each other). As a final section, discuss use cases for the package and what other functions could be included. Also include the main points from your discussion in Task 3.\nInclude a link to your group‚Äôs GitHub repository at the top of the vignette. Hand it in as a pdf in DTU Learn.\n\nCreate a vignette with usethis::use_vignette(\"your_package_name\").\nWhen you are done writing it, run devtools::build_vignettes().\nAt the top line of your vignette, change rmarkdown::html_vignette to rmarkdown::pdf_document\nRerun devtools::build_vignettes() - the created pdf-file in the doc-folder is the document to hand it.\nLastly, duplicate the vignette as the GitHub README\n\nCreate a README with usethis::use_readme_rmd( open = TRUE ).\nCopy the content of the vignette Rmarkdown into the readme Keep the top part of the README as is. If you overwrote it anyway, change rmarkdown::pdf_document to rmarkdown::github_document.\nRun devtools::build_readme()\nPush the changes to GitHub.\n\n\n\nLast tip on packages\n\nNext week, I will introduce the golem package for building production-grade Shiny apps. However, I personally also use it to quickly get going with packages.\nWhen starting out with an R package, it may seem complicated with a lot of things to remember. golem remembers these things for you. When setting up your package / Shiny app with golem::create_golem(\"Name of your awesome package/app\"), it creates a dev folder with a few files listing all you need to get started with an R package.\nIt does also give you a lot of other things that you will rarely use, and it also sets up some basic structures for Shiny apps. These can simply be deleted if you are not also building a Shiny app (which you will next week)."
  },
  {
    "objectID": "lab09.html",
    "href": "lab09.html",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "",
    "text": "shiny\ngolem"
  },
  {
    "objectID": "lab09.html#schedule",
    "href": "lab09.html#schedule",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Schedule",
    "text": "Schedule\n\n08.00 - 08.30: Recap of Lab 8\n08.30 - 09.00: Lecture\n09.00 - 09.15: Break\n09.00 - 12.00: Exercises"
  },
  {
    "objectID": "lab09.html#learning-materials",
    "href": "lab09.html#learning-materials",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Learning Materials",
    "text": "Learning Materials\nPlease prepare the following materials\n\nBook: Mastering Shiny by Hadley Wickham ‚Äì Read Chapter 1 (Chapter 2 and 3 are good to read as well, if you want).\nBook: Engineering Production-Grade Shiny Apps ‚Äì Read Chapter 2 - 5 (they are fairly short, but if you don‚Äôt find Shiny Apps super cool, feel free to skip Chapter 3 and 5 and Sections 2.2.2 and 4.2.3 - 4.2.4), the rest are quite important for the exercises.\nCheatsheet: Shiny ‚Äì This cheatsheet is a bit cluttered, but useful\nCheatsheet: Golem ‚Äì Look through this after reading the chapters in ‚ÄúEngineering Production-Grade Shiny Apps‚Äù - the exercises will remind you to look at the cheatsheet as well.\n\nNote: The following are suggested learning materials, i.e., do not go over everything, but poke around. You will use these materials as a point of reference for the group exercises\n\nShiny Input Gallery\nWeb: Shiny from RStudio\nWeb: RStudio tutorials on Shiny\nVideo: Playlist: Web Apps in R: Building your First Web Application in R | Shiny Tutorial\nExample: nnvizRt\nMore inspiration: Shiny Gallery"
  },
  {
    "objectID": "lab09.html#learning-objectives",
    "href": "lab09.html#learning-objectives",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nA student who has met the objectives of the session will be able to:\n\nPrepare a simple shiny application\nUsing relevant online resources to autonomously identify and obtain new and expand on existing knowledge of R"
  },
  {
    "objectID": "lab09.html#sec-exercises",
    "href": "lab09.html#sec-exercises",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "Exercises",
    "text": "Exercises\n\nIntroduction\n\nIn today‚Äôs lab, you will use the package you developed last time as basis for a Shiny App.\nDo not worry if you didn‚Äôt finish it, you will be able to install a backup. But please do try to use your own package first.\nYou will be using the golem production-grade app development framework, which has a bit of a learning curve, but it is a solid starting point. Please hang tight, as I walk you through how to get started.\nWhen building a Shiny App with golem, you need everything you learned about creating an R package, because a golem Shiny App is a package, and all the same rules apply.\n\n\nTASKS\n\n\nTask 1 - Setting up the Shiny App\n\nAs last week, the first tasks are only done by one team member, until instructed otherwise. Let the other team members follow along with what you are doing and do the following:\n\nTask 1.1 - Create the App\n\nGo to R for Bio Data Science RStudio Server and login.\nCreate a folder in home called ShinyApps - You need this for the server to find your app.\nRun golem::create_golem(\"~/ShinyApps/YOUR_APP_NAME\") in the console, putting in a name of your own choosing.\n\n\nAs last week, the theme is the central dogma of molecular biology. Let that inspire you\nLook up naming rules here\nThe most important rule: _, -, and ' are not allowed.\n\n\nRun usethis::use_git() to ensure git is initialized.\n\n\n\nTask 1.2 - Make a GitHub repository for your group‚Äôs app\n\nGo to https://github.com/rforbiodatascience23\nClick the green New-button\nCreate a new repository called group_X_shiny. Remember to replace the X\nSelect rforbiodatascience23 as owner\nMake the repository Public\nClick the green Create repository-button\nIn this new repository, click the settings tab\nClick Collaborators and Team\nClick the green Add people-button\nInvite the remaining group members\nAll other team members should now have access to the repository, but do not create your own project yet.\n\n\n\nTask 1.3 - Connect your RStudio Project to the GitHub Repository\nStill only the first team member\n\nFind your PAT key or create a new\n\n\nHow to create a new one\n\n\n\nType in the console usethis::create_github_token(). You‚Äôre going to be redirected to GitHub website.\n\nIn case you do it manually, remember to give permission to access your repositories.\n\nYou need to type your password. Don‚Äôt change the default settings of creating the token except for the description ‚Äì set ‚ÄòRStudio Server‚Äô or something similar. Then hit Generate token.\n\nCopy the generated token (should start with ghp_) and store it securely (e.g.¬†in password manager). Do not keep it in a plain file.\n\n\nGo back to the RStudio Server project.\nType in the console gitcreds::gitcreds_set() and paste the PAT key.\nIf there are files in the git window, stage them by ticking all boxes under Staged.\nStill in the console, run the following commands. Replace your group number and use your GitHub username and email. You run these to link your project to the GitHub repository you created. Remember to replace the X.\n\n\nusethis::use_git_remote(name = \"origin\", \"https://github.com/rforbiodatascience23/group_X_shiny.git\", overwrite = TRUE)\nusethis::use_git_config(user.name = \"USERNAME\", user.email = \"USEREMAIL@EXAMPLE.ORG\")\ngert::git_commit_all(\"App setup\") # Only run if there were files to stage\ngert::git_push(\"origin\")\n\nAll other team members\nAfter your teammate has pushed to GitHub.\n\nYou also need to create a ShinyApps folder.\nIn the RStudio Server, create a new project based on the GitHub Repository just created by your teammate.\n\nClick Create a project in the top left.\nChoose Version Control and then Git.\nPaste in the repository URL: https://github.com/rforbiodatascience23/group_X_shiny.git. Remember to replace the X.\nMake sure to put it in the ShinyApps folder.\nGive the directory a fitting name and click Create Project.\n\nFind your PAT key or create a new\n\n\nHow to create a new one\n\n\n\nType in the console usethis::create_github_token(). You‚Äôre going to be redirected to GitHub website.\n\nIn case you did it manually, remember to give permission to access your repositories.\n\nYou need to type your password. Don‚Äôt change the default settings of creating the token except for the description ‚Äì set ‚ÄòRStudio Server‚Äô or something similar. Then hit Generate token.\n\nCopy the generated token (should start with ghp_) and store it securely (e.g.¬†in password manager). Do not keep it in a plain file.\n\n\nGo back to the RStudio Server project\nType in the console gitcreds::gitcreds_set() and paste the PAT key\nStill in the console, run the following commands. Replace your GitHub username and email.\n\n\nusethis::use_git_config(user.name = \"USERNAME\", user.email = \"USEREMAIL@EXAMPLE.ORG\")\n\nIf RStudio at any point asks you to log in to GitHub, redo step 6 and 7.\n\n\n\nTask 2 - Create a production-grade Shiny app\n\nIt may seem daunting, but golem holds your hand much of the way.\n\nTask 2.1 - The dev folder\ngolem creates a dev folder in your project. The files there are extremely helpful and you should make yourself familiar with the content. Read this short chapter to get a feeling for what each helper function does. Also take a look at the golem cheatsheet.\n\nGo through dev/01_start.R and run the setup functions you think will be useful. Not everything is.\n\nRun them in order. Don‚Äôt run line 36, and if you are prompted to install packages, please decline.\nEverything on and above line 42 is recommended, everything else is optional. Feel free to see what they do. None of them should cause problems.\nAfter running the function for modifying the DESCRIPTION file, add more authors to the DESCRIPTION-file using this layout:\n\n\n\nAuthors@R:\n  c(person(given = \"firstname\",\n           family = \"lastname\",\n           role = c(\"aut\", \"cre\"), # There must be a \"cre\", but there can only be one\n           email = \"your@email.com\"), \n    person(given = \"firstname\",\n           family = \"lastname\",\n           role = \"aut\",\n           email = \"your@email.com\"))\n\n\nGo through dev/02_dev.R to get an idea of what helper functions are available. You will likely use some of these functions quite frequently.\nLook through dev/03_deploy.R to get an idea of how to deploy your app either in shinapps.io, as the app I showed in class, or on CRAN.\n\n\nBecause we are operating on a server, we cannot run the app directly in our R session. Therefore, we should prepare the app for deployment. This will allow the shiny server hosted on the Health Tech Server to launch your apps.\nRun line 32 in dev/03_deploy.R, i.e., golem::add_shinyserver_file()\n\n\nLook through dev/run_dev.R and run the first four commands. If you were not on a server, you could launch your app by running the last command run_app(). However, we are on a server, so we have implemented a small trick.\n\n\nWhen you ran line 32 previously, an app.R file was created. This allows the Health Tech Server to run your Shiny app from this url: https://teaching.healthtech.dtu.dk/shiny/username/app_name/ - Edit username and app_name appropriately.\nYou should see an empty app with your title. Now you just need to put stuff in it.\nOBS Sometimes, for unknown reasons, the browser removes the ‚Äúshiny‚Äù part of the url. If you get a ‚ÄúNot Found‚Äù error, check the url.\nAnother OBS For changes to take effect, you will have to close the app session, which means either closing the tab or removing the app name from the url. The latter will list all apps you have created - likely just this one so far. The url would then look like: https://teaching.healthtech.dtu.dk/shiny/username/.\n\n\n\nThe R folder\ngolem sets up a framework for your Shiny app. This framework is probably a little different from what you normally see when reading/watching guides on Shiny Apps. It may look overly complicated, and perhaps it is for this exercise. But it is a robust framework that makes maintaining even large apps easy(ier). Building a small app is very easy as well.\nYou are welcome to ignore the R/run_app.R (a wrapper function for shiny::shinyApp(), which runs the app) and R/app_config.R (some golem configuration) for now.\nThe first places you want to look are R/app_ui.R and R/app_server.R. They are responsible for the user interface and the app back end, respectively. You will not be working here directly as much, although you could. But it is strongly recommended to work in Shiny modules - created with golem::add_module( name = \"module_name\" ), as you saw in class.\n\nShiny Modules\nShiny modules may appear to make your life more complicated at first, but they will save you a lot of time down the road - especially from bug-fixing and GitHub merging issues.\nAfter creating a module, its ui and server functions should be copied into the app_ui.R and app_server.R scripts, as you will be reminded in the bottom of the module scripts. There, you can control how the module should interact with the main app - should each module have their own tab in the app or should they be called depending on which button the user clicks on.\nAnother useful feature of Shiny modules is that you can test them independently from the main app. I suggest, each time you create a module, to copy something like the following in the bottom of the created module script (renaming the relevant parts to fit the module). This way you always have a quick test environment at hand. But bear in mind the layout will be different from your main app, as it will not transfer the layout from app_ui.R.\n\nif(FALSE){ # Testing\n  golem::detach_all_attached()\n  golem::document_and_reload()\n  ui <- mod_NAME_ui(\"NAME_ui_1\") # replace NAME here\n  server <- function( input,output,session){\n    mod_NAME_server(\"NAME_ui_1\") # and here\n  }\n  shiny::shinyApp(ui, server)\n}\n\nIf you want to learn how to make the modules share information, please read here. I personally prefer approach B.\nThis is not necessary for the exercises, but I wanted to make you aware that it is possible. Feel free to use it in the Group Assignment.\n\n\n\n\nTask 3 - Populate your Shiny App\n\nLast week, you created a package that simulates the central dogma of molecular biology. Today, you will use that package to create an app around it.\nPlease work together on these tasks.\n\nTask 3.1 - Install the package you created last week\n\ndevtools::install_github(\"rforbiodatascience23/group_X_package\"). Remember to replace the X\n\n\n\nInstall backup\n\nPlease only use this if your own package does not work!\nInstall it with: devtools::install_github(\"r4bds/centralDogma\")\nThe function names are as follows:\n\nFunction one: centralDogma::random_dna()\nFunction two: centralDogma::transcribe()\nFunction three: centralDogma::codon_split()\nFunction four: centralDogma::translate()\nFunction five: centralDogma::plot_abundance()\n\n\nYou have already done the initial setup steps, so let‚Äôs get started with building the app!\n\n\nTask 3.2 - Design your primary layout\nFeel free to use your imagination. Otherwise, follow my suggestion for a tabsetPanel-layout:\nIf you make your own layout, make space for the two modules, as I did with the two tabPanels with temporary titles and module names below.\n\nAdd a tabsetPanel with 2 panels in the fluidPage() function within the app_ui function definition in R/app_ui.R. The tabsetPanel should be inserted after the header (written as h1(‚Äúheader‚Äù) ), separated by a comma.\n\n\ntabsetPanel(\n  tabPanel(title = \"panel1\",\n           \"module1\"),\n  tabPanel(title = \"panel2\",\n           \"module2\")\n)\n\nI have added placeholder names, you should replace these with relevant module functions as the app is build.\nYou will start by implementing the plotting module (‚Äúmodule2‚Äù) because it is a lot simpler, with only a single text input and a plot.\n\n\nTask 3.3 - Create the plotting shiny module\nYour plotting module should take in a peptide sequence and plot the relative absolute abundance of each amino acid.\n\nGo to dev/02_dev.R and find the function to add a module. For now, set with_test = FALSE - you will not be creating tests for the app in this exercise, unless you want to. Remember to name the module something sensible related to the description above instead of just ‚Äúmodule2‚Äù. Run the function.\nScroll to the bottom of the module and copy the commented out server function and add it to R/app_server.R inside the app_server function.\nRedo step 2, but for the ui function. Importantly, place it in the second tabPanel in R/app_ui.R, instead of the ‚Äúmodule2‚Äù. Give it a sensible title. If you made your own layout, place the module where you imagine it should go.\nGo back to the module script. Use my suggested module layout below. If you want, you are more than welcome to design it yourself. See the layout guide here or cheat sheet for inspiration.\n\nFill your desired layout in the tagsList of the ui function in the module.\n\n\n\n\nIf you want, draw inspiration for the layout I chose\n\n\nsidebarLayout(\n      sidebarPanel(\n        \"peptide_sequence\"\n      ),\n      mainPanel(\n        \"plot\"\n      )\n    )\n\nThe names in the example are not actual shiny stuff, just names I put in as a placeholder. Feel free to use them as your inputIDs and outputIDs or come up with your own.\n\n\n\nRun the app to see what it looks like (by refreshing the app url you created on Task 2.1).\n\n\nRemember if you get a ‚ÄúNot Found‚Äù error, the ‚Äú/shiny/‚Äù might be missing from the url.\n\n\nAdd an input field for the peptide_sequence, replace the string with the input you are adding. Please try it out first.\n\n\nRemember ns() around input/output IDs\n\n\n\nPossible solution\n\n\ntextAreaInput(\n          inputId = ns(\"peptide\"),\n          label = \"Peptide sequence\",\n          width = 300,\n          height = 100,\n          placeholder = \"Insert peptide sequence\"\n        )\n\n\n\n\nAdd an output field for the plot, replacing the placeholder string. Please try it out first.\n\n\n\nPossible solution\n\n\nplotOutput(\n          outputId = ns(\"abundance\")\n          )\n\n\n\n\nFull possible solution\n\n\nmod_abundance_ui <- function(id){\n  ns <- NS(id)\n  tagList(\n    shiny::sidebarLayout(\n      shiny::sidebarPanel(\n        shiny::textAreaInput(\n          inputId = ns(\"peptide\"),\n          label = \"Peptide sequence\",\n          width = 300,\n          height = 100,\n          placeholder = \"Insert peptide sequence\"\n        )\n      ),\n      shiny::mainPanel(\n        shiny::plotOutput(\n          outputId = ns(\"abundance\")\n          )\n\n      )\n    )\n  )\n}\n\n\n\n\nTask 3.4 - Plotting module server logic\n\nThere are several ways to solve this task, but keep in mind that if an empty string is given to function_five, it will give an error and the app will crash. Prevent that. Please try to make the server logic on your own. Also feel free to add an additional button to control when the plotting is happening (see my slide on reactivity for inspiration).\n\nGo the the module server function. Make the server logic for handling the text input and render a plot using function_five from your package, remember to rename the function call appropriately (yourpackage::function_five).\n\n\nEnsure the IDs match.\nIt does not matter where in the module server function you put it, as long as you put it inside the function.\n\n\n\nPossible solution\n\n\noutput$abundance <- renderPlot({\n      if(input$peptide == \"\"){\n        NULL\n      } else{\n        input$peptide |>\n          yourpackage::function_five() +\n          ggplot2::theme(legend.position = \"none\")\n        }\n    })\n\n\n\n\nDiscuss in your group what is going on in my solution. Why does it work, and what other ways could you get to the same solution (check my slide on reactivity for inspiration). As a small note, I chose to hide the legend, as that looked better in the app.\nAdd @importFrom ggplot2 theme and @import yourpackage to the module server function description.\nGo to dev/02_dev.R and run the first function to add ggplot2 and yourpackage to your package dependencies: attachment::att_amend_desc(). Or add it to the DESCRIPTION manually.\nRun every line but the last in dev/run_dev.R and reopen your app to see that it works (You can just type in random amino acids in the field).\n\n\nIf you run the last line, a popup will tell you the connection is refused - it would work locally, but doesn‚Äôt on the server.\nRemember if you get a ‚ÄúNot Found‚Äù error, the ‚Äú/shiny/‚Äù might be missing from the url.\nIf no changes appear, close the tab for a minute and try again.\n\n\n\nTask 3.5 - Create shiny ‚Äúmodule1‚Äù\nThe ‚Äúmodule1‚Äù module (please pick a sensible name) should take in a DNA strand and convert it into a peptide sequence. There should also be an option to create a random DNA strand, in case the user does not have one of their own.\n\nGo to dev/02_dev.R and find the function to add a module. Again, set with_test = FALSE. Remember to name the module something sensible related to the description above. Run the function.\nIn the created module file, scroll to the bottom of the module and copy the commented out module server function and add it to R/app_server.R in the app_server function.\nDo the same for the ui function. Importantly, place it in the first tabPanel in R/app_ui.R, replacing the ‚Äúmodule1‚Äù string. Give it a sensible title. If you made your own layout, place the module where you imagine it should go.\nGo back to the module script. Use my suggested module layout below. If you want, you are more than welcome to design it yourself. See the layout guide here or cheat sheet for inspiration.\n\nFill your desired layout in the tagsList of the ui function in the module.\n\n\n\nfluidRow(\n  column(8, \"DNA_sequence\"),\n  column(4, \"random_dna_length\", \"generate_dna_button\")\n),\n\"peptide_sequence\"\n\nAgain, the names in the example are not actual shiny stuff, just names I put in as a placeholder. Feel free to use them as your inputIDs and outputIDs or come up with your own.\n\nBefore we continue, try to run the app: Run every line but the last in dev/run_dev.R and reopen your app.\n\n\nRemember if you get a ‚ÄúNot Found‚Äù error, the ‚Äú/shiny/‚Äù might be missing from the url.\nIf no changes appear, close the tab for a minute and try again.\n\n\nDiscuss in your group which parts of the UI needs to be reactive (updateable) and which don‚Äôt. Of course, when you type a letter, the letter should appear where you typed it - that part happens automatically. But what needs to update when a button is clicked or an input changes?\nLet‚Äôs add some inputs (buttons, text, etc.):\n\nGo here for inspiration for inputs.\nLook at the shiny cheat sheet for inspiration for input and output.\nRemember to wrap your input/output IDs with ns() (required in modules).\n\n\n\n\nHint for DNA_sequence\n\nFor the DNA input, I wanted the sequence to be updated when the user clicks the ‚Äúgenerate_dna_button‚Äù button. The simplest way to make UI inputs reactive is with an uiOutput(). My ‚ÄúDNA_sequence‚Äù look like: uiOutput(ns(\"DNA\")). The content of the field will be defined in the server.\n\n\n\nHint for random_dna_length and generate_dna_button\n\nI chose a numeric input for the ‚Äúrandom_dna_length‚Äù and an actionButton for the ‚Äúgenerate_dna_button‚Äù. They look like:\n\nnumericInput(\n    inputId = ns(\"dna_length\"),\n    value = 6000,\n    min = 3,\n    max = 100000,\n    step = 3,\n    label = \"Random DNA length\"\n  ),\nactionButton(\n    inputId = ns(\"generate_dna\"),\n    label = \"Generate random DNA\", style = \"margin-top: 18px;\"\n  )\n\nI added the style to adjust the position of the button. That is in no way required.\n\n\n\nHint for peptide_sequence\n\nFor ‚Äúpeptide_sequence‚Äù, I chose a verbatimTextOutput. You may not have seen it before; it is in the shiny cheat sheet, but it simply allows for long-form text output in a grey box. The tagAppendAttributes allow you to add CSS tags (you don‚Äôt need to know what that means). In this case, I wanted the text to wrap. By googling a bit, I found the solution seen below (feel free to see how it looks with and without it - but remember you will first see something, when you have made the server logic in the next task).\n\nverbatimTextOutput(outputId = ns(\"peptide\")) |> \n  tagAppendAttributes(style = \"white-space: pre-wrap;\")\n\n\n\n\nFull possible solution\n\n\nmod_dna_expression_ui <- function(id){\n  ns <- NS(id)\n  tagList(\n    fluidRow(\n      column(8, shiny::uiOutput(ns(\"DNA\"))),\n      column(4, shiny::numericInput(\n        inputId = ns(\"dna_length\"),\n        value = 6000,\n        min = 3,\n        max = 100000,\n        step = 3,\n        label = \"Random DNA length\"\n      ),\n      shiny::actionButton(\n        inputId = ns(\"generate_dna\"),\n        label = \"Generate random DNA\", style = \"margin-top: 18px;\"\n      ))\n    ),\n    shiny::verbatimTextOutput(outputId = ns(\"peptide\")) |> \n      shiny::tagAppendAttributes(style = \"white-space: pre-wrap;\")\n\n  )\n}\n\n\n\n\nReopen your app to see what it looks like. Remember, all output stuff will not appear before the server outputs something to it.\n\n\n\nTask 3.6 - Adding server logic\nThis task might seem a bit confusing, but hang tight! Let me take you through this one step at a time.\n\nFirst, the DNA sequence should be an object that reacts to user input. Therefore, it should be a reactive value.\n\nAs the first line to add in the server function in your module, write dna <- reactiveVal().\nRead more about reactive values here if you like. The important thing to have in mind is that every time you want to get the value run dna(), and when you update it run dna(new_value) with your new value - you will see an example of this in a minute.\n\nIf you, like me, wanted your DNA input strand to be updateable in the input field, add the following somewhere in the server:\n\n\noutput$DNA <- renderUI({\n      textAreaInput(\n        inputId = ns(\"DNA\"),\n        label = \"DNA sequence\",\n        placeholder = \"Insert DNA sequence\",\n        value = dna(),\n        height = 100,\n        width = 600\n        )\n    })\n\n\nMake sure the IDs match.\nDiscuss in the group what is going on there.\n\n\n\nAnswer\n\nThe output$DNA informs shiny to update the output with DNA as ID from the UI. In this case the uiOutput you added earlier. The renderUI creates a UI field. This function is complementary to the uiOutput. textAreaInput is a large text input field - suitable for large text inputs, such as DNA. The ns() should always be called when adding input/output fields in modules. Note that it is not necessary when assigning to outputs, such as output$DNA. The dna() adds the DNA strand as the value in the field, this ensures that the updated DNA value is displayed in the input field, when it is generated with the ‚Äòrandom DNA‚Äô function.\nAs an added note, notice that the input and output IDs can be the same, as with the ns(\"DNA\") ID for both uiOutput and textAreaInput in this case. I find it handy to name them the same, so I know which input and output interacts. If you find it confusing, you can name them something else.\n\n\n\nThe next step is to add a reaction when the ‚Äúgenerate random DNA‚Äù button is clicked.\n\nThere are a few ways to handle this. My favorite is the following:\n\n\n\nobserveEvent(input$generate_dna, {\n      dna(\n        yourpackage::function_one(input$dna_length)\n      )\n    })\n\n\nWrite the name of your package instead of yourpackage and add the name you gave function one from last week‚Äôs exercise.\n\nMake sure the input names are the same as for your app (generate_dna and dna_length)\nDiscuss in your group what is going on. Before looking at the answer, consider reading this section.\n\n\n\n\nAnswer\n\nThe observeEvent function observes when a change occurs to the generate_dna input. The reaction to a change then involves updating the dna value to the output of function_one() - or what you chose to call the function.\n\n\n\nLastly, we should address what happens when there is DNA to be translated into a peptide:\n\nFeel free to play around with a solution, based on what you have done previously.\nThe challenging part is to make the peptide update automatically without initial loading problems or nuisance for the user (the latter refers to a problem where an observer may deselect the text input field such that the user has to press the field again to continue typing.)\nOtherwise, use my solution below:\n\n\n\n\nPossible solution\n\nThis may not work as intended if you did not choose the uiOutput for the DNA input field. Then you may have to use dna() or add an actionButton. Ask if you run into issues. Please note that I am using input$DNA and not dna(). You will discuss why in a minute, but feel free to try it out both ways and see if you notice the difference.\n\noutput$peptide <- renderText({\n      # Ensure input is not NULL and is longer than 2 characters\n      if(is.null(input$DNA)){\n        NULL\n      } else if(nchar(input$DNA) < 3){\n        NULL\n      } else{\n        input$DNA |> \n          toupper() |> \n          yourpackage::function_two() |> \n          yourpackage::function_three() |> \n          yourpackage::function_four()\n      }\n    })\n\n\n\n\nRegardless of wether you used my solution or not, discuss in your group what goes on in my solution\n\n\n\nAnswer\n\nWith a direct renderText on the input$DNA, the output will update whenever the input field changes, without causing the nuisance described above, which would be introduced with an observeEvent without an associated actionButton. Recall input$DNA gets updated when generate_dna is clicked. The first test checks that the input is not NULL, which it is upon loading the app, causing an error message in the console, but doesn‚Äôt necessarily crash the app. An output of NULL renders nothing (this fact can be useful when building apps in the future). The second test ensures an output is only generated when there is more than 3 characters to interpret. The toupper() function ensures the input is upper case, which is required for the codon table. renderText does as you would expect and renders some text for the UI. You might have guessed from the row of functions in the renderText, it takes the dna strand and converts it using the package you created last week into a peptide sequence. This peptide sequence is then stored in the peptide output, which links to the verbatimTextOutput you likely have in your ui function\n\n\n\nPlease make sure all input and output IDs match in the UI and Server functions.\nGo to dev/02_dev.R and run the first function to add dependencies: attachment::att_amend_desc() or do it manually.\nReopen the app and see if it acts as expected.\n\n\nRemember if you get a ‚ÄúNot Found‚Äù error, the ‚Äú/shiny/‚Äù might be missing from the url.\nIf no changes appear, close the tab for a minute and try again.\n\n\nPush it to GitHub\n\n\nAwesomely done! You have now created a fully functional R shiny application that takes in a DNA strand or creates a random one, converts it into a peptide sequence, and then is able to plot the abundance of each amino acid in the sequence."
  },
  {
    "objectID": "lab09.html#group-assignment",
    "href": "lab09.html#group-assignment",
    "title": "Lab 9 Creating a Simple Shiny Application",
    "section": "GROUP ASSIGNMENT",
    "text": "GROUP ASSIGNMENT\n\nThis week‚Äôs assignment is a short group discussion (1/4 - 1/2 page). Please cover these topics:\n\nWhat did you find most challenging about creating a shiny app?\nWhat do you feel are the benefits and challenges with using golem for app development?\nMaintaining reproducibility when using shiny apps is a challenge. As an example, some pharma-companies display databases to medical doctors for pointy-clicky data exploration. What are your thoughts on maintaining reproducibility in that scenario? What if the database is updated or the app is updated with new or changed functionality?\nWhat other challenges and limitations do you see shiny apps may have? Please elaborate.\nHand in a pdf with you discussion points and links to your shiny app GitHub page and the app itself.\n\nAs an optional extra challenge, try to expand your shiny app with a third module. You decide what the content of the module should be. Remember to put the shiny module functions somewhere in R/app_ui.R in the app_ui function and R/app_server.R in the app_server function. If you made one, add a brief description of the module in the hand-in."
  },
  {
    "objectID": "lab10.html",
    "href": "lab10.html",
    "title": "Lab 10 Project Startup & Industry Talks",
    "section": "",
    "text": "08.00 - 08.15: Recap of Lab 9\n08.15 - 08.45: Introduction to Project Period and Exam\n08.45 - 09.00: Break\n09.00 - 12.00: Mini Symposium"
  },
  {
    "objectID": "lab10.html#sec-symposium",
    "href": "lab10.html#sec-symposium",
    "title": "Lab 10 Project Startup & Industry Talks",
    "section": "Mini Symposium: Applications of R for Bio Data Science in Industry",
    "text": "Mini Symposium: Applications of R for Bio Data Science in Industry\nThis mini symposium on Applications of R for Bio Data Science in Industry aims to give students a glimpse into how modern bio data science is transforming value creation in the pharmaceutical industry and healthcare sector.\n\n2023\n\nCompanies Represented\n\nClinical Microbiomics\nGubra\nLundbeck\nNovo Nordisk\nZS\n\n\n\nSymposium Programme\n\n09.00 ‚Äì 09:25: ‚ÄúHarnessing R Packages for Microbiome Analysis and Reporting‚Äù by Maria Novosolov, Data Scientist, Clinical Microbiomics\n09.25 ‚Äì 09:50: ‚ÄúPlotly and Shiny apps for exploratory analysis of transcriptomics data‚Äù by Christina Bligaard Pedersen, Senior Bioinformatician & Mikhail Osipovitch, Lead Bioinformatician, ZS\n09.50 ‚Äì 10:00: Break\n10.00 ‚Äì 10:25: ‚ÄúAn ADA monitoring tool developed with Rshiny‚Äù by Carlotta Porcelli, Senior Statistical Programmer, Novo Nordisk\n10.25 ‚Äì 10:50: ‚ÄúThe R in R&D efficiency‚Äù by Victor A. O. Carmelo, Principal Bioinformatician, Lundbeck\n10.50 ‚Äì 11:00: Break\n11.00 ‚Äì 11:25: ‚ÄúFrom data analysis to infrastructure: A journey with R‚Äù by Sebastian T√∏lb√∏l Thrane, Senior Scientist, Gubra\n11.25 ‚Äì 11:50: ‚ÄúThe long and winding road to use R in a pharmaceutical company‚Äù by Claus Dethlefsen, Statistical Director, Novo Nordisk\n11.50 ‚Äì 12:00: Symposium round-off\n\n\n\n\n2022\n\nCompanies Represented\n\nAbzu\nBristol Myers Squibb\nClinical Microbiomics\nNovo Nordisk Bioinformatics\nNovo Nordisk Biostatistics\nSteno Diabetes Center Copenhagen\n\n\n\nSpeaker List\n\nAndrea Marquard, Head of Data Science and Automation, Clinical Microbiomics: ‚ÄúHow we use internal R packages, and why you should learn to make one‚Äù\nAnders Gorst-Rasmussen, Statistical Director, Novo Nordisk Biostatistics, Novo Nordisk: ‚ÄúHow we use R in Novo Nordisk Biostatistics‚Äù\nAnne-Mette Bjerregaard, Senior Scientist in Computational Biology, Novo Nordisk: ‚ÄúThe Evolution of a Computational Biologist‚Äù\nSam Demharter, Bioinformatics Specialist, Abzu: ‚ÄúExplainable AI in Precision Medicine - Closing the Loop from Patient to Drug‚Äù\nLykke Pedersen, Head of RNA Therapeutics, Abzu: ‚ÄúExplainable AI in Precision Medicine - Closing the Loop from Patient to Drug‚Äù\nSimon Papillon-Cavanagh, Principal Scientist, Bristol Myers Squibb (USA): ‚ÄúA Career Built on Mistakes‚Äù\nSofie Olund Villumsen, Bioinformatician, Steno Diabetes Center Copenhagen: ‚ÄúUsing R for Data Analysis in Clinical Studies‚Äù\nTarun Veer Singh Ahluwalia, Assoc. Prof., Steno Diabetes Center Copenhagen: ‚ÄúUsing R for Data Analysis in Clinical Studies‚Äù\n\n\n\n\n2021\n\nCompanies Represented\n\nAbzu\nALK\nChr. Hansen\nLundbeck\nNovozymes\nTeraData\n\n\n\nSpeaker List\n\nSamuel Demharter, Senior Bioinformatician, Abzu\nLykke Pedersen, Senior Bioinformatician, Abzu\nMarie-Catherine Le Bihan, Senior Data Scientist, Chr. Hansen\nThomas Strantzl, Head of Bioinformatics, ALK\nMaria Dalby, Postdoctoral Researcher, Lundbeck\nThomas Poulsen, Science Manager, Novozymes\nMikkel Freltoft Krogsholm, Team lead, Senior Data Scientist, TeraData"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Various resources pertaining to the course"
  },
  {
    "objectID": "guide_for_working_locally.html",
    "href": "guide_for_working_locally.html",
    "title": "Guide for Working Locally",
    "section": "",
    "text": "Go to the R for Bio Data Science RStudio Cloud Server\nLogin using your credentials\nIn the file pane, navigate to your projects-directory, where (if you followed course instructions) you will find your r_for_bio_data_science-directory\nTick the box to the left of your r_for_bio_data_science-directory\nClick the gear cog, which says More\nClick Export...\nYou should now be prompted to download r_for_bio_data_science.zip\nClick Download"
  },
  {
    "objectID": "guide_for_working_locally.html#download-course-slides",
    "href": "guide_for_working_locally.html#download-course-slides",
    "title": "Guide for Working Locally",
    "section": "Download Course Slides",
    "text": "Download Course Slides\nThe course slides are available via the course site. They are self-contained .html-files and can be opened offline in a browser like e.g.¬†google chrome. The slides can be downloaded as follows:\n\nGo to the Course Site\nIn the menu on the left, choose a course lab and click it\nUnder Schedule, find and click the link to the lecture\nHit command+s (mac) or ctrl+s (windows) or choose file and then save depending on your OS\nPut the downloaded .html-files where you want to save them.\nRight click and choose open with or similar and then choose your favourite browser"
  },
  {
    "objectID": "guide_for_working_locally.html#installing-r-on-your-computer",
    "href": "guide_for_working_locally.html#installing-r-on-your-computer",
    "title": "Guide for Working Locally",
    "section": "Installing R on your computer",
    "text": "Installing R on your computer\n\nGo to The R Project for Statistical Computing\nUnder Getting Started it says To download R, please..., click the download R-part\nScroll down and find Denmark and click\nClick the Aalborg University-mirror link https://mirrors.dotsrc.org/cran/\nUnder Download and Install R, click your appropriate OS Linux, macOS or Windows\n\nIf you have Linux, you‚Äôll know what to do\nFor mac, be aware that you have to choose either R-4.3.2-arm64.pkg or R-4.3.2-x86_64.pkg (versions as of 30-11-23) depending on whether you have a M- or an Intel-mac. This you can find out by clicking the apple in the upper left corner of your screen and select About This Mac and look at what is says next to Chip\nFor Windows, there is a install R for the first time-guide you will have to follow.\n\n\nIf any of this is unclear, I‚Äôm absolutely sure, there will be tons of how-to-install-R tutorials on youtube"
  },
  {
    "objectID": "guide_for_working_locally.html#installing-rstudio",
    "href": "guide_for_working_locally.html#installing-rstudio",
    "title": "Guide for Working Locally",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nGo to the Posit site\nClick DOWNLOAD RSTUDIO in the upper right corner\nUnder RStudio Desktop, click DOWNLOAD RSTUDIO\nThe first point 1: Install R is covered above\nUnder 2: Install RStudio, click the blue box below, it will vary depending on your OS, mine says DOWNLOAD RSTUDIO DESKTOP FOR MACOS 11+\nOpen the downloaded installer and follow the instructions"
  },
  {
    "objectID": "guide_for_working_locally.html#installing-packages",
    "href": "guide_for_working_locally.html#installing-packages",
    "title": "Guide for Working Locally",
    "section": "Installing packages",
    "text": "Installing packages\nOnce you have downloaded and installed R and RStudio, you can start installing packages. E.g. we can install Tidyverse using the command:\n\ninstall.packages(\"tidyverse\")\n\nLook through the different exercise labs at the top, the packages you have been working with are stated."
  },
  {
    "objectID": "guide_for_working_locally.html#open-course-materials",
    "href": "guide_for_working_locally.html#open-course-materials",
    "title": "Guide for Working Locally",
    "section": "Open course materials",
    "text": "Open course materials\nThe downloaded course materials can simply be opened directly in RStudio, so be sure to organise them, so you can easily access them on your own computer."
  },
  {
    "objectID": "guide_for_building_r4ds2e_locally.html",
    "href": "guide_for_building_r4ds2e_locally.html",
    "title": "Guide for Building the R4DS2e Book Locally",
    "section": "",
    "text": "In the upper right corner of RStudio, click the current project\nChoose New Project...\nChoose Version Control\nChoose Git\nUnder Repository URL:, enter https://github.com/hadley/r4ds\nUnder Project directory name:, enter r4ds\nUnder Create project as subdirectory of: uge the Browse...-button to navigate to where you want to place this project\nClick Create Project\n\nA Clone Repository process now starts, wait for it to finish\n\nIn the RStudio Files pane of your new project, find and open the file index.qmd\nClick the render-button above the index.qmd-file you just opened\nIt probably will not run on the first try! Make sure to check the errors, e.g.¬†Error in library(ggthemes) : there is no package called 'ggthemes' and then run e.g.¬†install.packages(\"ggthemes\")\nAgain, click the render-button above the index.qmd-file (Be aware that RStudio will show you the file, where the missing package is mentioned, so you will have to make sure, that you click the index.qmd-file tab) and redo installation of any missing packages (For Restart R error, see below)\nOnce all missing packages have been installed, the rendered book will appear in the Viewer pane. Click the small white and lightblue icon with and arrow\nThe book will now open locally in your browser on e.g.¬†http://localhost:7829/index.html\n\nCongratulations! You have now sucessfully cloned a GitHub repository and build an entire Quarto book from scratch!\nIf you get stuck in a loop where RStudio says ‚ÄúRestart R prior to install‚Äù on the same package. Select Cancel, quit the project, re-open the project and as the first thing, type in the install.packages(\"PACKAGE_NAME\")-command and then select No, when it prompts for restart.\nStop for a moment and consider that you get all of this is for free! All the tons of hours being put in by the 287 contributors have done so in the name of open source and open learning! That‚Äôs nothing short of awesome!"
  },
  {
    "objectID": "paths_and_projects.html",
    "href": "paths_and_projects.html",
    "title": "Paths & Projects",
    "section": "",
    "text": "You step onto the road, and if you don‚Äôt keep your feet, there‚Äôs no knowing where you might be swept off to\nIn context of R, a path is a way to tell R, where to look for a file. First, let us familiarise us a bit with paths in RStudio."
  },
  {
    "objectID": "paths_and_projects.html#getting-familiar-with-paths-in-rstudio",
    "href": "paths_and_projects.html#getting-familiar-with-paths-in-rstudio",
    "title": "Paths & Projects",
    "section": "Getting Familiar with Paths in RStudio",
    "text": "Getting Familiar with Paths in RStudio\nLog on to the RStudio Cloud Server and in case you do so for the first time, your files-pane should look something like:\n Here,  Home defines your home, that is where you ‚Äúlive‚Äù on the server. Now, click the button  New Folder and create a new folder called projects. Hereafter, your files-pane should look something like:\n\nNow, click the projects folder you created and then you should see:\n\nNote how the Home now is extended to  Home > projects. This signifies, that you are looking at the projects folder in your home.\nTry to click on the 2 dots in .. (the green arrow won‚Äôt do it, so hit those dots!), this will take you one level up, so you again see:\n\nNote, that you are now back in  Home"
  },
  {
    "objectID": "paths_and_projects.html#intermezzo-creating-a-project",
    "href": "paths_and_projects.html#intermezzo-creating-a-project",
    "title": "Paths & Projects",
    "section": "Intermezzo: Creating a Project",
    "text": "Intermezzo: Creating a Project\nOk, so far so good. Now again click into projects and click  Project: (None) and from here select  New Project.... Now you will se a dialogue window open, i.e.¬†RStudio requires input from you:\n\nClick  New Directory and you should see:\n\nClick  New Project:\n\nIn the Directory name:, enter e.g.¬†r_for_bio_data_science and note how we are creating the folder (In this case Directory name, which is equivalent) as a sub-folder of projects. The funny wavy symbol followed by a slash ~/ is simply a short hand for ‚Äúin this users home folder‚Äù. So let us proceed:\n\nand then click Create Project. Now, depending on your choice of directory name, you should see:\n\nBriefly, the created r_for_bio_data_science.Rproj file contains the settings for your project. You can verify this, by clicking on the file and you should see:\n\nWe will leave this as is for now, so go ahead and click OK.\nNow, back to folders and paths - note how you now see  Home > projects > r_for_bio_data_science, this means that you are now in the r_for_bio_data_science folder, which is inside the projects folder which in turn is inside the Home folder. If you take a look at the  Home > projects > r_for_bio_data_science, you can in fact also click directly on e.g.¬†projects - Try it!\nBut why? Don‚Äôt worry, we will return to why RStudio Projects are indispensable when working with reproducible Bio Data Science"
  },
  {
    "objectID": "paths_and_projects.html#locating-data",
    "href": "paths_and_projects.html#locating-data",
    "title": "Paths & Projects",
    "section": "Locating Data",
    "text": "Locating Data\nLet‚Äôs move on. Now, again click the  New Folder-button and create a data-folder, make sure it ends up in the r_for_bio_data_science-folder. This should result in:\n\nNote here how we now have  data and  r_for_bio_data_science.Rproj. These are different, one is a folder, data, and the other is a file, r_for_bio_data_science.Rproj, containing settings for your RStudio Project.\nLet us try to put som data into the data-folder. In the console window, run the following command\n\nwrite.table(x = datasets::Puromycin, file = \"data/puromycin.tsv\", sep = \"\\t\")\n\nThis should look like so:\n\nThis command write a table containing the Puromycin from an R-package named datasets, this is done using the x-parameter. The next paramter is file and we set that to indicate, that the file should go into the data-folder we created and that we would like the file to be named puromycin.tsv, where tsv is an abbreviation for tab-separated-values and then the last parameter sep is set to \"\\t\" indicating, that we want the values to be separated by a tab, as indicated, when we named the file.\nOnce you have run the command, click the data-folder and you should now see:\n\nAgain, click .., this will take you one level up, so you again see:\n\nNow, we have a data file called puromycin.tsv. Let us read that file into R. We can do that like so:\n\nmy_data <- read.table(file = \"puromycin.tsv\")\n\nEnter the command into the console and run it like we did before. You will now see the following:\n\nSo, what happend? The blue writing is your command and the red is an error message from R. Always read error messages carefully, they will inform you what went wrong. In this case, we can see that cannot open file 'puromycin.tsv': No such file or directory.\nThis happened because we forgot to specify where the puromycin.tsv-file is located. R is very picky here, you have to specify exactly where R should find things. Recall, that we decided to create a data-folder and that we placed the puromycin.tsv-file into that folder. This we have to specify, when we use the file parameter in the read.table()-function. So, let us fix that:\n\nmy_data <- read.table(file = \"data/puromycin.tsv\")\n\nThis data/puromycin.tsv is the path to the file and now, that we have got that correct, you will see no error message and furthermore, you will see in the environment-pane, that we now have an object called my_data, containing 23 obs. of 3 varibles, i.e.¬†a data set with 23 rows and 3 columns."
  },
  {
    "objectID": "paths_and_projects.html#absolute-versus-relative-paths",
    "href": "paths_and_projects.html#absolute-versus-relative-paths",
    "title": "Paths & Projects",
    "section": "Absolute versus Relative Paths",
    "text": "Absolute versus Relative Paths\nLet us get back to why we have to work using RStudio Projects, recall we created the r_for_bio_data_science.Rproj-file, defining out project. You can verify, that we are indeed working in that project, by looking in the upper right corner of the RStudio IDE and you should see  r_for_bio_data_science.\nGood, now in the console, enter the command:\n\ngetwd()\n\nYou should see something along the lines of:\n\n\"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science\"\n\nSo, when we read the puromycin.tsv-file using the path data/puromycin.tsv, we specify, that R should look for the file puromycin.tsv in the data folder. So why did we not have to specify /net/pupilx/home/people/student_id/projects/r_for_bio_data_science? Well indeed, we could have specified the full location of the puromycin.tsv-file, which would be:\n\n\"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science/data/puromycin.tsv\"\n\nThis is called the absolute path and here you should note, that it begins with a /. But let us say, that we had indeed in our code stated:\n\nmy_data <- read.table(file = \"/net/pupilx/home/people/student_id/projects/r_for_bio_data_science/data/puromycin.tsv\")\n\nThen that would work‚Ä¶ On OUR computer. If we were to share our code to a colleague or a collaborator, then that would not work, because that person would have a different path, e.g.¬†a different student_id. The code would break! Imagine that you have thousands of line of code with hundreds of absolute paths - You would spend hours-and-hours on fixing all the absolute paths, so they matched that particular computer. Then every time we would want to share the analysis project, we would have to redo this tedious proces!\nThis is why we work in RStudio Projects! RStudio Projects allows us to specifiy where everything is located relative to where the .Rproj-file is. So in our case, the r_for_bio_data_science.Rproj-file is located in the same place as the data-folder, namely in the folder containing our entire project, the r_for_bio_data_science-folder, which in turn is located in the projects-folder.\nThis means, that all paths in the analysis project, can be stated relative to the location of the .Rproj-file and hence we have relative paths, meaning that anyone can receive the project and run it straight-out-of-the-box!"
  },
  {
    "objectID": "paths_and_projects.html#working-in-multiple-projects",
    "href": "paths_and_projects.html#working-in-multiple-projects",
    "title": "Paths & Projects",
    "section": "Working in Multiple Projects",
    "text": "Working in Multiple Projects\nNow, we did add that plural s, when we created the projects-folder. When you have completed this course, perhaps you want to attend the ‚ÄúIntroduction to Systems Biology‚Äù-course. In that case, we would setup a new project, so use the Files-pane to navigate to the projects-folder:\n\nThen, we simply repeat the proces: Click  r_for_bio_data_science in the upper right corner and from here select  New Project.... Now you will se a dialogue window open, i.e.¬†RStudio requires input from you:\n\nClick  New Directory and you should see:\n\nClick  New Project:\n\nIn the Directory name:, enter e.g.¬†introduction_to_systems_biology:\n\nand then click Create Project. Now, you should see:\n\nNow, note how you now see  Home > projects > introduction_to_systems_biology, meaning that you are now in your Home and then in your folder containing projects, one of which is your introduction_to_systems_biology project.\nClick .. and you will see:\n\nThis is now your two project folders and you can add others, such as yet another course or e.g.¬†special_course, bachelor_thesis or master_thesis.\nNow, we can easily switch between different projects. In the upper right corner you will see, that you are currently in the introduction_to_systems_biology project, meaning that R will look for all files relative to the introduction_to_systems_biology.Rproj-file. Naturally, we would want to switch back to the project, we created for the ‚ÄúR for Bio Data Science‚Äù-course. To do this, we simply click  introduction_to_systems_biology and if you look at the drop-down menu, you should see r_for_bio_data_science - Click it! Notice how R automatically restarts and you are moved to the correct folder for this project."
  },
  {
    "objectID": "paths_and_projects.html#learning-objectives",
    "href": "paths_and_projects.html#learning-objectives",
    "title": "Paths & Projects",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIf you made it this far, you should now be able to:\n\nNavigate the RStudio IDE in context of folders and projects\nCreate a new project\nCreate a new folder\nRead and write data files from relative paths\nWork in and switch between different projects\nExplain the difference between a folder, a data file and a Rproj-file\nExplain the difference between absolute and relative paths\nExplain why RStudio Projects are an essential part of doing reproducible bio data science"
  },
  {
    "objectID": "paths_and_projects.html#epilogue",
    "href": "paths_and_projects.html#epilogue",
    "title": "Paths & Projects",
    "section": "Epilogue",
    "text": "Epilogue\nThat‚Äôs all folks! I hope this cleared up some things - Please feel free to revisit this chapter, as needed!\nRemember - Have fun! No one ever got really good at something they didn‚Äôt think was fun!"
  },
  {
    "objectID": "variable_assignment_in_r.html",
    "href": "variable_assignment_in_r.html",
    "title": "Variable Assignment in R",
    "section": "",
    "text": "In R we operate with variables. A variable can be seen as a container for a value. To get a better conceptual understanding of this, you can go through the following and code-along in your own R-session."
  },
  {
    "objectID": "variable_assignment_in_r.html#assigning-a-value-to-a-variable",
    "href": "variable_assignment_in_r.html#assigning-a-value-to-a-variable",
    "title": "Variable Assignment in R",
    "section": "Assigning a Value to a Variable",
    "text": "Assigning a Value to a Variable\n\nIn R, we state values directly in the chunk or the console, e.g.:\n\n\n3\n\n[1] 3\n\n\n\nHere, we just state 3, so R simply ‚Äúthrows‚Äù that right back at you!\nNow, if want to ‚Äúcatch‚Äù that 3 we have to assign it to a variable, e.g.:\n\n\nx <- 3\n\n\nNotice how now we ‚Äúcatch‚Äù the 3 and nothing is ‚Äúthrown‚Äù back to you, because we now have the 3 stored in x:\n\n\nx\n\n[1] 3"
  },
  {
    "objectID": "variable_assignment_in_r.html#updating-the-value-of-a-variable",
    "href": "variable_assignment_in_r.html#updating-the-value-of-a-variable",
    "title": "Variable Assignment in R",
    "section": "Updating the Value of a Variable",
    "text": "Updating the Value of a Variable\n\nNow, we can of course use x moving forward, e.g.¬†by adding 2:\n\n\nx + 2\n\n[1] 5\n\n\n\nNotice how this does not change x and the result is simply ‚Äúthrown‚Äù right-back-at-ya\n\n\nx\n\n[1] 3\n\n\n\nIf we wanted to update x by adding 2, we would have to ‚Äúcatch‚Äù the result as before:\n\n\nx <- x + 2\n\n\nNow, we have updated x:\n\n\nx\n\n[1] 5"
  },
  {
    "objectID": "variable_assignment_in_r.html#use-one-variable-in-the-creation-of-another",
    "href": "variable_assignment_in_r.html#use-one-variable-in-the-creation-of-another",
    "title": "Variable Assignment in R",
    "section": "Use one Variable in the Creation of Another",
    "text": "Use one Variable in the Creation of Another\n\nAnalogue, we can create a new variable using x:\n\n\ny <- x + 3\n\n\nAgain, this does not change x\n\n\nx\n\n[1] 5\n\n\n\nBut rather the result is now stored in y\n\n\ny\n\n[1] 8"
  },
  {
    "objectID": "variable_assignment_in_r.html#summary",
    "href": "variable_assignment_in_r.html#summary",
    "title": "Variable Assignment in R",
    "section": "Summary",
    "text": "Summary\n\nIn R, we use the assignment operator <- to perform assignment\nVariables are not change in place, but needs to be stored\nNote, this also applies to running e.g.¬†a dplyr-pipeline, where we do not change the dataset by running the pipeline, but we must store the result of the pipeline\n\nBefore continuing, make sure that you are on track with the above concepts!\n\nCreate a new variable my_age containing‚Ä¶ You guessed it!\nAdd 0.5 to the variable (I.e. your age, when you‚Äôre done with this course)\nCheck the value of my_age, did you remember to assign, thereby updating?"
  },
  {
    "objectID": "variable_assignment_in_r.html#pipeline-example",
    "href": "variable_assignment_in_r.html#pipeline-example",
    "title": "Variable Assignment in R",
    "section": "Pipeline Example",
    "text": "Pipeline Example\n\n\n\n\nLet us create some example sequence data:\n\n\ntibble(sequence = c(\"aggtgtgag\", \"tggaatgaaccgcctacc\",\n                    \"aagaatgga\", \"tct\", \"tgtatt\", \"tgg\",\n                    \"accttcaacgagtcccactgt\", \"cgt\",\n                    \"gaggctgagctggttgta\", \"ggggaacag\"))\n\n# A tibble: 10 √ó 1\n   sequence             \n   <chr>                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nNotice, that our data creation is just ‚Äúthrown‚Äù back at us, we forgot something!\n\n\nmy_dna_data <- tibble(sequence = c(\"aggtgtgag\", \"tggaatgaaccgcctacc\",\n                                   \"aagaatgga\", \"tct\", \"tgtatt\", \"tgg\",\n                                   \"accttcaacgagtcccactgt\", \"cgt\",\n                                   \"gaggctgagctggttgta\", \"ggggaacag\"))\n\n\nNow, we have stored the data in the variable my_dna_data\n\n\nmy_dna_data\n\n# A tibble: 10 √ó 1\n   sequence             \n   <chr>                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nNote here, that a variable can as we saw before with x and y store a single value, e.g.¬†2, but here, we are storing a tibble-object in the variable my_dna_data and in that tibble-object, we have a variable sequence, which contains some randomly generated dna.\nBut what if we wanted to add a new variable to the tibble-object, which is the lenght of each of the dna-sequences?\n\n\nmy_dna_data |>\n  mutate(dna_length = str_length(sequence))\n\n# A tibble: 10 √ó 2\n   sequence              dna_length\n   <chr>                      <int>\n 1 aggtgtgag                      9\n 2 tggaatgaaccgcctacc            18\n 3 aagaatgga                      9\n 4 tct                            3\n 5 tgtatt                         6\n 6 tgg                            3\n 7 accttcaacgagtcccactgt         21\n 8 cgt                            3\n 9 gaggctgagctggttgta            18\n10 ggggaacag                      9\n\n\nNice! Let‚Äôs see that data again then:\n\nmy_dna_data\n\n# A tibble: 10 √ó 1\n   sequence             \n   <chr>                \n 1 aggtgtgag            \n 2 tggaatgaaccgcctacc   \n 3 aagaatgga            \n 4 tct                  \n 5 tgtatt               \n 6 tgg                  \n 7 accttcaacgagtcccactgt\n 8 cgt                  \n 9 gaggctgagctggttgta   \n10 ggggaacag            \n\n\n\nWait! What? Where is the variable we literally just created?\nWe forgot something‚Ä¶ We did not update the my_dna_data, let‚Äôs fix that:\n\n\nmy_dna_data <- my_dna_data |>\n  mutate(dna_length = str_length(sequence))\n\n\nNote, nothing is ‚Äútrown‚Äù back at us! Let‚Äôs verify, that we did indeed update the my_dna_data:\n\n\nmy_dna_data\n\n# A tibble: 10 √ó 2\n   sequence              dna_length\n   <chr>                      <int>\n 1 aggtgtgag                      9\n 2 tggaatgaaccgcctacc            18\n 3 aagaatgga                      9\n 4 tct                            3\n 5 tgtatt                         6\n 6 tgg                            3\n 7 accttcaacgagtcccactgt         21\n 8 cgt                            3\n 9 gaggctgagctggttgta            18\n10 ggggaacag                      9\n\n\nDid it make sense? Check yourself, add a new variable to my_dna_data called sequence_capital by using the function str_to_upper()\nThat‚Äôs it - Hope it helped and remember‚Ä¶ Bio data science in R is really fun!"
  },
  {
    "objectID": "code_styling.html",
    "href": "code_styling.html",
    "title": "Code Styling",
    "section": "",
    "text": "This is a condensed primer on code styling, based on The tidyverse style guide"
  },
  {
    "objectID": "code_styling.html#why",
    "href": "code_styling.html#why",
    "title": "Code Styling",
    "section": "Why?",
    "text": "Why?\n‚ÄúGood coding style is like correct punctuation: you can manage without it, butitsuremakesthingseasiertoread‚Äù source\nIt is not uncommon in a bio data science industry setting, where there is an aspect of production code, i.e.¬†the code you are writing is being used professionally by other users either in an organisation or as an actual software product, that you will have to adhere to a certain style, when coding. This facilitates lower maintanence on code. Imagine the case, where you are perhaps 10 bio data scientists working on e.g.¬†an R-package, which will used downstream by another department. You really need to make sure, that everything is top notch, so you decide to do code reviewing. You write your code up and then your colleague will go over the code to check it. But you and your colleague have completely different opinions on how the code should be styled. This will result in an unnecessary time overhead. Another case, could be a colleague leaving for another position, where you then have to take over that colleague‚Äôs code base and you end up re-styling the code, so it matches your preferences, again this will increase maintenance unnecessarily. The solution is to agree on a set of rules and principles on how to style your code - This is code styling!\nBelow follows the code styling you will have to adhere to in this course"
  },
  {
    "objectID": "code_styling.html#so-how-should-we-style-the-code",
    "href": "code_styling.html#so-how-should-we-style-the-code",
    "title": "Code Styling",
    "section": "So, how should we style the code?",
    "text": "So, how should we style the code?\nIn this course, we will use principles from the The tidyverse style guide. At first, it may seem constraining, but you will quickly get used to it and then it will be easier moving forward. Note, for your project, being able to review consistant code, will save you valuable time!\nRecall the cancer_data (gravier) dataset, we worked with:\n\nlibrary(\"tidyverse\")\nlibrary(\"curl\")\nbase_url <- \"https://github.com/\"\ntarget_file <- \"ramhiser/datamicroarray/raw/master/data/gravier.RData\"\noutput_file <- \"data/gravier.RData\"\ncurl_download(url = str_c(base_url,\n                          target_file),\n              destfile = output_file)\n\nPrinciples:\n\nQuote the packages you load using the library-function\nUse the proper variable assignment in R, namely <-\nStay within 80 characters width. Note, you can set a ‚Äúhelp line‚Äù: Tools \\(\\rightarrow\\) Global Options... \\(\\rightarrow\\) Code \\(\\rightarrow\\) Display \\(\\rightarrow\\) Show margin \\(\\rightarrow\\) Margin column: 80\nUse double quotes. This is for consistency with other languages\nTab out parameters of functions\nDo line breaks after commas\nDo one space on each side of =, when assigning arguments to parameters, e.g.¬†my_function(parameter_1 = argument_1), etc.\nMatch indentations, this RStudio will do for you in most cases\n\nOk, a bit of data wrangling:\n\nload(file = \"data/gravier.RData\")\ncancer_data <- gravier |> \n  bind_cols() |> \n  rename(early_metastasis = y) |> \n  mutate(pt_id = str_c(\"pt_\", row_number()),\n         pt_has_early_metastasis = case_when(\n    early_metastasis == \"good\" ~ \"No\",\n    early_metastasis == \"poor\" ~ \"Yes\"))\n\nPrinciples:\n\nLine break after each pipe |>, recall we prononuce the pipe as ‚Äúthen‚Äù\nLine break after commas inside functions, such as here with mutate()\nIf the line in mutate() becomes wide, then consider using a single linebreak after opening the function call\nuse proper descriptive variable names in snake case like pt_has_early_metastasis, there is no overhead in understanding what this variable means\n\nLet‚Äôs do a simple plot:\n\ncancer_data |>\n  ggplot(aes(x = early_metastasis,\n             y = g8A08)) +\n  geom_hline(yintercept = 0,\n             linetype = \"dashed\") +\n  geom_boxplot() +\n  scale_y_continuous(limits = c(-0.5, 0.5)) +\n  theme_bw() +\n  labs(x = \"Early Metastasis\")\n\n\n\n\nPrinciples:\n\nLinebreak after +, just like with the pipe\nSpace after comma inside a vector, like when we define the limits\n\nDo it - It‚Äôll be fun and it does not take a long time to adapt!"
  },
  {
    "objectID": "primer_on_linear_models_in_r.html",
    "href": "primer_on_linear_models_in_r.html",
    "title": "Primer on Linear Models in R",
    "section": "",
    "text": "In a basic linear regression model, one independent variable and one dependent variable are involved. The terms independent and dependent are literal meaning, that one variable does depend on the value of the other, whereas for the other the value is independent of the other. In a basic linear regression, we find the line that best fit the data. We will illustrate this, with the following example."
  },
  {
    "objectID": "primer_on_linear_models_in_r.html#example",
    "href": "primer_on_linear_models_in_r.html#example",
    "title": "Primer on Linear Models in R",
    "section": "Example",
    "text": "Example\n\n\n\n\nBackground\nLet‚Äôs say we wanted to study the genetic mechanism protecting a plant from heat shock, then:\n\nIndependent: Environmental Condition (temperature)\nDependent: Gene Expression Level (related to heat shock protection)\n\nHere, the independent variable is the temperature and the dependent variable is the gene expression level. It is clear, that the temperature, does not rely on the gene expression level, but the gene expression level of heat shock related genes, does rely on the temperature.\nSo, we keep plants under different temperatures and collect samples, from which we can extract RNA and run a transcriptomics analysis uncovering gene expression levels.\n\n\nData\nFor the data here, we are going to simulate the relationship between gene expression levels and temperature, as a function in R:\n\nrun_simulation <- function(temp){\n  measurement_error <- rnorm(n = length(temp), mean = 0, sd = 3)\n  gene_expression_level <- 2 * temp + 3 + measurement_error\n  return( gene_expression_level )\n}\n\nNote, how we‚Äôre adding some measurement error to our simulation, otherwise we would get a perfect relationship, which we all know never happens.\nNow, we can easily run simulations:\n\nrun_simulation(temp = c(15, 20, 25, 30, 35))\n\n[1] 29.63799 40.28852 51.26510 61.26683 73.78605\n\n\nLet‚Äôs just go ahead and create some data, we can work with. For this example, we take samples starting at 5 degree celsius and then in increments of 1 up to 50 degrees:\n\nset.seed(806017)\nexperiment_data <- tibble(\n  temperature = seq(from = 5, to = 50, by = 1),\n  gene_expression_level = run_simulation(temp = temperature)\n)\nexperiment_data |> \n  sample_n(10) |> \n  arrange(temperature)\n\n# A tibble: 10 √ó 2\n   temperature gene_expression_level\n         <dbl>                 <dbl>\n 1          12                  28.7\n 2          13                  30.2\n 3          16                  35.6\n 4          19                  44.0\n 5          22                  48.8\n 6          27                  64.6\n 7          32                  61.1\n 8          33                  73.9\n 9          35                  76.7\n10          40                  84.9\n\n\n\n\nVisualising\nNow, that we have the data, we can visualise the relationship between the temperature- and gene_expression_level-variables:\n\nmy_viz <- experiment_data |> \n  ggplot(aes(x = temperature,\n             y = gene_expression_level)) +\n  geom_point() +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0)\nmy_viz\n\n\n\n\n\n\n\n\nNow, we can easily add the best fit line using the geom_smooth()-function, where we specify that we want to use method = \"lm\" and for now, we exclude the confidence interval, by setting se = FALSE:\n\nmy_viz +\n  geom_smooth(method = \"lm\",\n              se = FALSE)\n\n\n\n\n\n\n\n\nWhat happens here, is that a best-fit line is added to the plot by calculating the line, such that the sum of the squared errors is as small as possible, where the error is the distance from the line to a given point. This is a basic linear regression and is known as Ordinary Least Squares (OLS). But what if we want to work with this regression model, beyond just adding a line to a plot?\n\n\nModelling\nOne of the super powers of R is the build in capability to do modelling. Because we simulated the data (see above), we know that the true intercept is 3 and the true slope of the temperature variable is 2. Let see what we get, if we run a linear model:\n\nmy_lm_mdl <- lm(formula = gene_expression_level ~ temperature,\n   data = experiment_data)\nmy_lm_mdl\n\n\nCall:\nlm(formula = gene_expression_level ~ temperature, data = experiment_data)\n\nCoefficients:\n(Intercept)  temperature  \n      2.816        2.021  \n\n\nImportant, the formula notation gene_expression_level ~ temperature is central to R and should be read as: ‚Äúgene_expression_level modelled as a function of temperature‚Äù, i.e.¬†gene_expression_level is the dependent variable often denoted y and temperature is the independendt variable often denoted x.\nOkay that‚Äôs pretty close! Recall the reason for the difference is, that we are adding measurement error, when we run the simulation (see above).\nIn other words our model says, that:\n\\[gene\\_expression\\_level = 2.816 + 2.021 \\cdot temperature\\]\nI.e. the estimate of the intercept is 2.816 and the estimate of the slope is 2.021, meaning that when the temperature = 0, we estimate that the gene_expression_level is 2.816 and for each 1 degree increase in temperature, we estimate, that the increase in gene_expression_level is 2.021.\nThese estimates are pretty close to the true model underlying our simulation:\n\\[gene\\_expression\\_level = 3 + 2 \\cdot temperature\\]\nIn general form, such a linear model can be written like so:\n\\[y = \\beta_{0} + \\beta_{1} \\cdot x_{1}\\]\nWhere the \\(\\beta\\)-coefficients are termed estimates, because that is exactly what we do, given the observed data, we estimate their values.\n\n\nWorking with a lm-object:\nThe model format you saw above, is a bit quirky, but luckily, there is a really nice way to get these kind of model object into a more tidy-format:\n\nlibrary(\"broom\")\nmy_lm_mdl |> \n  tidy()\n\n# A tibble: 2 √ó 5\n  term        estimate std.error statistic  p.value\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)     2.82    1.16        2.44 1.89e- 2\n2 temperature     2.02    0.0378     53.4  1.16e-41\n\n\nBriefly, here we term, estimate, std.error, statistic and p.value. We discussed the term and estimate. The std.error pertains to the estimate and the statistic is used to calculate the p.value.\n\nThe P-value\nNow, because we now have a tidy object, we can simply plug-‚Äòn‚Äô-play with other tidyverse tools, so let us visualise the p.value. Note, because of the often vary large differences in p.values, we use a -log10-transformation, this means that larger values are ‚Äúmore significant‚Äù. Below, the dashed line signifies \\(p=0.05\\), so anything above that line is considered ‚Äústatistically significant‚Äù:\n\nmy_lm_mdl |> \n  tidy() |> \n  ggplot(aes(x = term,\n             y = -log10(p.value))) +\n  geom_point() +\n  geom_hline(yintercept = -log10(0.05),\n             linetype = \"dashed\")\n\n\n\n\n\n\n\n\nNow, as mentioned the p-values are computed based on the statistic and are defined as: ‚ÄúThe probability of observing a statistic as or more extreme given, that the null-hypothesis is true‚Äù. Where the null-hypothesis it that there is no effect, i.e.¬†the estimate for the term is zero.\nFrom this, it is quite clear, that there very likely is a relationship between the gene_expression_level and temperature. In fact, we know there is, because we simulated the data.\n\n\nThe Confidence Intervals\nWe can further easily include the confidence intervals of the estimates:\n\nmy_lm_mdl_tidy <- my_lm_mdl |> \n  tidy(conf.int = TRUE,\n       conf.level = 0.95)\nmy_lm_mdl_tidy\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)     2.82    1.16        2.44 1.89e- 2    0.488      5.14\n2 temperature     2.02    0.0378     53.4  1.16e-41    1.94       2.10\n\n\n‚Ä¶and as before easily do a plug‚Äôn‚Äôplay into ggplot:\n\nmy_lm_mdl_tidy |> \n  ggplot(aes(x = estimate,\n             y = term,\n             xmin = conf.low,\n             xmax = conf.high)) +\n  geom_errorbarh(height = 0.1) +\n  geom_point()\n\n\n\n\n\n\n\n\nNote, what the 0.95 = 95% confidence intervals means is that: ‚ÄúIf we were to repeat this experiment 100 times, then 95 of the times, the generated confidence interval would contain the true value‚Äù.\n\n\n\nSummary\nWhat we have gone through here is a basic linear regression, where we are aiming to model the continuous variable gene_expression_level as a function of yet another continous variable temperature. We simulated data, where the true intercept and slope were 3 and 2 respectively and by fitting a linear regression model, the estimates of the intercept and slope respectively were 2.82 [0.49;5.14] and 2.02 [1.94;2.10].\nLinear models allow us to gain insights into data, by modelling relationships."
  },
  {
    "objectID": "primer_on_r_packages.html",
    "href": "primer_on_r_packages.html",
    "title": "Primer on R package development",
    "section": "",
    "text": "There are a few things to know when creating a package before you jump in. These are not strict rules, but they make your life easier when bug-fixing and make the package much easier to use for the users. Learn about the dos and don‚Äôts in the following.\n\n\nThe one strict rule is Never use library(\"package\") within a package! \nInstead, add the packages your are going to use to the DESCRIPTION file and in the function descriptions. This is done by running usethis::use_package(\"packageName\") in the console and adding @import package (OK) or @importFrom package function1 function2 ... (Best). Using the functions in your package is then done with package::function() (e.g., dplyr::mutate()) or omitting the package::.\nThis way, it is easy to read what functions are from your package, and your package namespace does not get cluttered. Read more in the Namespace section.\nIt should also be a goal to make your package depend on as few other packages as possible. The user will need to install all packages your package depends on, but then also every package those depends on - that list quickly becomes quite long if you are not careful.\n\n\n\nA package is typically a collections of functions.\nThese functions are stored in .R files in the R folder. A good starting point is to create an .R file for each function. But, as the package becomes bigger, it often makes sense to combine related functions into bigger files.\nYou can quickly create a new .R file with usethis::use_r(\"function_name\"). Or do it manually, as you are used to.\n\n\n\nTry running ?mean in the Console.\nIf you have every wondered how to write a manual like the one that pops up, please click here and read - if not, consider reading it anyway, as you will use it later.\nWhen you have made a function, or have at least defined one, you should describe what it does and how to use it. The manual you write for your function is the function documentation, and it should describe the purpose of the function and how to use it.\nYou can read extensively about it here, but I will give you the most essential information to get you started.\nThe R package roxygen2 makes this easy as 1-2-3. It is part of devtools and is already installed. It uses #' comments above the function. @ tags lets you add specific details to the function documentation.\nCreate an roxygen skeleton by clicking somewhere in your function. Go to the ‚ÄòCode‚Äô tab in the top of your window and select ‚ÄòInsert Roxygen Skeleton‚Äô.\nThis will look something like this:\n\n#' Title\n#'\n#' @param foo \n#' @param bar \n#'\n#' @return\n#' @export\n#'\n#' @examples\nmyFunction <- function(foo, bar){\n  # Do stuff with foo and bar\n  foobar <- (foo * bar) / (foo + bar)\n  return(foobar)\n}\n\nThis allows you to add the most basic descriptions. To begin with, the Title, @param, and @export are the most important, you may remove the other tags for now. A more detailed example is given here. There, you can also read about documenting datasets and other object types - even the package itself.\n\n\n\nYour package namespace can quickly become very cluttered, if you are not careful.\nTherefore, follow these rules:\n\nOnly @export the functions the users will use. Removing the tag makes the function internal and hides it from your package namespace. It can still be freely used within your package and accessed outside your package with package:::internal_function()\nMake your code explicit with package::function().\n\nThis step is not mandatory, but makes reading the code easier.\n\nAdd your dependencies in the DESCRIPTION file with usethis::use_package(\"packageName\")\nOnly very rarely use the @import tag. Aim to use the @importFrom tags in your function descriptions instead.\n\nYou can read more extensively about namespace here.\n\n\n\nTesting is essential\n\nto ensure your package runs smoothly and that no bugs are introduced when you make a seemingly minor change. It is handled with the testthat package, which is also installed with devtools.\nI will not go into too much detail here, but know that testing is an important, but often neglected, part of building a package. You can read more about it here.\nEvery time you run the usethis::use_r() function to create a new script, the function encourages you to create a test alongside the new function. I recommend you follow that advise.\nYou create a test by running usethis::use_test(\"function name\").\nThe function creates a new folder tests and creates a test script for the function. The good R package creator writes a few tests for every function.\nThe exercises will ask you to make a simple test for every function, introducing you to the concept.\n\n\n\nWhen creating a package, it is important to test your work along the way.\nYou can do that in many ways, but I recommend the following workflow:\n\nWrite a function / make a change\n\nIf it is a new function, document it\n\nSave your files: rstudioapi::documentSaveAll()\nCreate package documentation: devtools::document()\n\n\nIf at this point, you get a warning that NAMESPACE already exists, delete it and try again.\n\n\nLoad package: devtools::load_all()\nYour package is now loaded, and you can test that it works as intended.\n\nOptionally, you can save the three lines of code in dev/load.R and run the lines with source(\"dev/load.R\"). If you do, add the dev folder to the .Rbuildignore file."
  },
  {
    "objectID": "external_resources.html",
    "href": "external_resources.html",
    "title": "External Resources",
    "section": "",
    "text": "Here, you will find various valuable resources, to aid your bio data science workflow"
  },
  {
    "objectID": "external_resources.html#a-few-quick-ones",
    "href": "external_resources.html#a-few-quick-ones",
    "title": "External Resources",
    "section": "A few quick ones‚Ä¶",
    "text": "A few quick ones‚Ä¶\n\nA very handy ggplot cheat-sheet can be found here\nSo which plot to choose? Check this handy guide\nExplore ways of plotting here\nThere is a nice tool to aid in choosing colours for visualisations here\nThe Posit community pages is a very nice place to get help if you‚Äôre stuck"
  },
  {
    "objectID": "external_resources.html#open-source-data-science-books",
    "href": "external_resources.html#open-source-data-science-books",
    "title": "External Resources",
    "section": "Open source data science books",
    "text": "Open source data science books\n\nHands-On Programming with R by Garrett Grolemund\nStatistical Inference via Data Science - A moderndive into R and the tidyverse by Chester Ismay and Albert Y. Kim\nIntroduction to Data Science, Data Analysis and Prediction Algorithms with R by Rafael A. Irizarry\nMastering Shiny by Hadley Wickham\nAn Introduction to Statistical Learning with Applications in R by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\nSTAT 545 - Data wrangling, exploration, and analysis with R by Jenny Bryan\nHappy Git and GitHub for the useR by Jenny Bryan, the STAT 545 TAs, Jim Hester"
  },
  {
    "objectID": "external_resources.html#software-links",
    "href": "external_resources.html#software-links",
    "title": "External Resources",
    "section": "Software Links",
    "text": "Software Links\n\nThe R Project for Statistical Computing\nRStudio - Open Source and Enterprise-ready professional software for R\nTidyverse website"
  },
  {
    "objectID": "external_resources.html#some-useful-links",
    "href": "external_resources.html#some-useful-links",
    "title": "External Resources",
    "section": "Some Useful Links",
    "text": "Some Useful Links\n\nFrom data to Viz: Find the graphic you need\nR for Data Science: Exercise Solutions\nR colour guide by Tian Zheng\nThe tidyverse style guide - By Hadley Wickham\nRStudio Primers - Learn data science basics with the interactive tutorials\nThe tidyverse style guide\nswirl - Learn R, in R\nRStudio Cheat Sheets\nRStudio Community - Stuck? Ask a question and get help moving on\nHarvardX Biomedical Data Science Open Online Training\nData Analysis Playlist"
  },
  {
    "objectID": "external_resources.html#on-data-science",
    "href": "external_resources.html#on-data-science",
    "title": "External Resources",
    "section": "On Data Science",
    "text": "On Data Science\n\nThe Role of Academia in Data Science Education"
  },
  {
    "objectID": "external_resources.html#guides-on-good-data-practices",
    "href": "external_resources.html#guides-on-good-data-practices",
    "title": "External Resources",
    "section": "Guides on Good Data Practices",
    "text": "Guides on Good Data Practices\n\nA Guide to Reproducible Code by the British Ecology Society\nA Quick Guide to Organizing Computational Biology Projects\nHow to pick more beautiful colors for your data visualizations\nTalk: Steps toward reproducible research by Karl Broman"
  },
  {
    "objectID": "course_elements.html",
    "href": "course_elements.html",
    "title": "Course Elements",
    "section": "",
    "text": "Various elements central to the course"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "As part of this course, starting from Lab 2, there will be a weekly hand in. Here, follows the instructions for creating and handing in course assignments."
  },
  {
    "objectID": "assignments.html#assignment-instructions",
    "href": "assignments.html#assignment-instructions",
    "title": "Assignments",
    "section": "Assignment Instructions",
    "text": "Assignment Instructions\nThe assignment will be marked ‚ÄúGroup Assignment‚Äù with red font in the exercises. In your group, you are to prepare an answer to just this one question. Think about reproducibility from the get-go, so include in your assignment, what is needed to re-create your micro-report, e.g.¬†if you‚Äôre using external data, from where and how did you get it? You will then receive group feedback on your assignment, which you should make sure to go over in your group.\nPlease note, that in order to include all elements in a self-contained html file, you will have to use the following YAML-header in your Quarto document:\n\n---\ntitle: \"Lab 2 Assignment: Group 02\"\nformat:\n  html:\n    embed-resources: true\neditor: visual\n---\n\nOr at least one similar, format title and other elements as you see fit, key here is the html-format with embedded resources."
  },
  {
    "objectID": "assignments.html#how-to-hand-in",
    "href": "assignments.html#how-to-hand-in",
    "title": "Assignments",
    "section": "How to hand in",
    "text": "How to hand in\n\nCheck that your assignment conforms to the assignment checklist, as defined in the next section\nGo to DTU Learn\nFind and click your R for Bio Data Science course\nMake sure you are enrolled in the correct group, as defined by the Group Formation Sheet (see the Getting Started Section)\nIn case groups consists of a mix of BSc. and MSc. students, then enroll under your respective courses\nOnly hand in one assignment, meaning either under the BSc. or the MSc. course, not both\nYou should hand in the rendered html-file, make sure to compress it to a zip-file prior to upload"
  },
  {
    "objectID": "assignments.html#assignment-checklist",
    "href": "assignments.html#assignment-checklist",
    "title": "Assignments",
    "section": "Assignment Checklist",
    "text": "Assignment Checklist\nDid you?\n\nMatch your group number with DTU Learn groups\nAdd Group number, names and student ids?\nOnly answer what is defined as ‚ÄúGroup Assignment‚Äù\nCreate the assignment using Quarto\nModify the YAML-header to ensure encapsulation\nInclude Group number, names and student ids\nUse proper markdown headers as defined by #, ##, ###\nWrite a few sentences under each section\nSeparate text clearly from code chunks\nFollow course code styling\nThink about reproducibility with respect to data and libraries?\nRender to html\nCompress the html-file to a zip-file, before upload to DTU Learn\nMake sure to download the upload‚Äôed zip-file and check that everything looks right?"
  },
  {
    "objectID": "project_description.html",
    "href": "project_description.html",
    "title": "Project Description",
    "section": "",
    "text": "Make sure you have read the exam description"
  },
  {
    "objectID": "project_description.html#project-groups",
    "href": "project_description.html#project-groups",
    "title": "Project Description",
    "section": "Project Groups",
    "text": "Project Groups\n\nYou will be working in the groups assigned from the beginning of the course\nIMPORTANT: All groups members are responsible for all parts of the project!\nNote, completing the project is considered your exam preparation"
  },
  {
    "objectID": "project_description.html#project-as-a-collaborative-effort",
    "href": "project_description.html#project-as-a-collaborative-effort",
    "title": "Project Description",
    "section": "Project as a Collaborative effort",
    "text": "Project as a Collaborative effort\nAs per the course description at the DTU course base: Active participation in the group work and timely submission of project and code base are both indispensable prerequisites for exam participation.\nThis means, that each group member is expected to:\n\nGenerally participate actively in the group project\nTake responsibility for solving assigned tasks within the project\nWrite and review code and perform commit-/push-/pulls to the project GitHub repository\nMeet and discuss actively with the group members\nSpend 9-10h per week on the project for the full 3 week project period\n\nBio Data Science is a collaborative effort, which is reflected in the design of the project module of this course!"
  },
  {
    "objectID": "project_description.html#expected-time-usage",
    "href": "project_description.html#expected-time-usage",
    "title": "Project Description",
    "section": "Expected time usage",
    "text": "Expected time usage\nAs per the rules for the European Credit Transfer System (ECTS) points, 1 credit equals 28 hours. Therefore, for a 5-person group working for 3 weeks, the expected total project hours is ~150. Setup your collaborative project and this will be more than sufficient to create a full bio data science project. Note, for lab 13, you workload will be a ~10min. presentation therefore the hours for this lab is included."
  },
  {
    "objectID": "project_description.html#project-description",
    "href": "project_description.html#project-description",
    "title": "Project Description",
    "section": "Project Description",
    "text": "Project Description\n\nAim\nThe aim with the Project module of the course is to allow you to independently work with the course elements, you have been exposed to during the first 9 weeks of teaching. Here, you will synthesise the entire bio data science cycle, thereby internalising the course elements. Moreover, you are to:\n\nUse the tools you have learned in the course and ‚Äúdesign and execute a bio data science project focusing on collaborative coding and reproducibility, incl.¬†independently using online resources to seek information about application and technical details of state-of-the-art data science tools‚Äù\n\nRecall the ‚ÄúData Science Cycle‚Äù"
  },
  {
    "objectID": "project_description.html#project-requirements",
    "href": "project_description.html#project-requirements",
    "title": "Project Description",
    "section": "Project Requirements",
    "text": "Project Requirements\n\nLocation\nThe project must be placed on the course GitHub organisation and named e.g.¬†group_03_project, replacing 03, with your group number.\n\n\nGitHub README\n\nIMPORTANT: First header in the README has to be ‚ÄúProject Contributors‚Äù and then please state the student ids and matching GitHub usernames, so we know who-is-who\nSince we are not putting data on GitHub, use the README to supply information on data retrieval\n\n\n\nOrganisation\nYour project must strictly adhere to the organisation illustrated below:\n\n\n\n\n\nWhere directories left-to-right, starting with data and corresponding files are:\n\n\n\ndata: Your data\n\n\n\n_raw: Directory inside data, containing your raw data file(s), which must never be edited\n\nraw_data.xlsx: This need not necessarily be an Excel-file, but should be seen as a placeholder for whichever raw data, your project is based on\n\n01_dat_load.tsv: Your loaded data, e.g.¬†combining multiple sheets from an excel file\n02_dat_clean.tsv: Your data cleaned per your specifications and in tidy format\n03_dat_aug.tsv: Your data with added variables, e.g.¬†like computing a BMI\n\n\n\n\n\nR: Your code\n\n\n\n01_load.qmd: Loads your data, e.g.¬†combining multiple sheets from an excel file\n02_clean.qmd: Cleans your data per your specifications and converts to tidy format\n03_augment.qmd: Adds new variables, like e.g.¬†computing a BMI\n04_describe.qmd: Descriptive statistics, how many in each group, etc.\n05_analysis_1.qmd: Here goes your first analysis\n06_analysis_2.qmd: Here goes your second analysis and so on\n99_proj_func.qmd: DRY: Don‚Äôt Repeat Yourself, repeated code goes into a function\n00_all.qmd: The master document, capable of running the entire project in one go\n\n\n\n\n\nresults: Your results\n\n\n\n*.html: The output from your *.qmds in your R-directory\nkey_plots.png: Whichever of all the nice plots you made, which should end up in your final presentation\n\n\n\n\n\ndoc: Your documents\n\n\n\npresentation.qmd: Your final Quarto Presentation of your project\npresentation.html: Self-contained HTML5 presentation (just as the course slides)\n\n\nImportant: This entails, that the entire project as put on GitHub, can be cloned and then executed end-to-end. Since, we are not putting data on GitHub, if possible included programmatic retrieval of data and/or data instructions in your GitHub README file\n\nBut How???\nYes, we are not putting the data on GitHub, so either include programmatic retrieval of the data or state in your GitHub README file, how you retrieved the data forming the base of the project.\n\n\n\nCode\nYour project must strictly adhere to the Course Code Styling Guide\n\n\nData\nFirst and foremost, you must find a data set you can work with in the project!\n\nIt must be based on a bio data set\nStart out as ‚Äúdirty‚Äù i.e.¬†a completely clean / tidy and analysis-ready data set will not allow you to demonstrate, that you have met the course learning objectives\nIt is advisable, that the data is of limited size, so you do not risk time waste due to long runtimes\nNote, you should demonstrate ability to extract biological insights, but at the same time mind that the focus should be on demonstrating that you master the data science toolbox according to course aim and learning objectives\nRemember, the process is the product!\nNaturally, you cannot reuse the data we have worked with during the exercise labs\n\n\n\nPresentation\n\nA 10-slides-in-10-mins presentation, possibly followed by a few questions\nFollow the IMRAD standard scientific structure:\n\nIntroduction\nMaterials and Methods\nResults (And)\nDiscussion\n\nWith a technical focus, but minding to communicate which-ever biological insights you arrived at\nShould not include all your code, but rather focus on the broader picture and include data summaries and visualisations\nCreated using Quarto Presentation, just as the course slides are\nNOTE: This final presentation in HTML-format must be zipped and uploaded to DTU Learn before deadline, so we can check the GitHub version is identical"
  },
  {
    "objectID": "project_description.html#project-supervision",
    "href": "project_description.html#project-supervision",
    "title": "Project Description",
    "section": "Project Supervision",
    "text": "Project Supervision\n\nAs a point of reference, this project is part of the overall assessment and you will therefore have limited access to supervision\nThink of the project, as a long take-home-assignment\nI highly encourage the use of Piazza for questions, which will be monitored by the teaching team\nAlso, a rather comprehensive list of Project FAQ have been compiled, so be sure to check that out!\nI recommend using your course Quarto documents for reference\nAlso, perhaps you can get input on the Posit Community Pages\n\nIf any aspects of the above is not clear, please reach out to the teaching team!"
  },
  {
    "objectID": "project_faq.html",
    "href": "project_faq.html",
    "title": "Project FAQ",
    "section": "",
    "text": "The emphasis for the communication of the course is on the presentation. Hence, think of the report text as ‚ÄúWhat information would I like to have / need in order to follow the code and project?‚Äù\n\n\n\n\n\nDo not think of the text as constituting a report in a classical sense, as you have likely done in other courses. Rather, the text constitutes the underpinnings of a technical report, i.e.¬†the aim is to support the code and clarify what is going on enabling the reader to easily follow the code"
  },
  {
    "objectID": "project_faq.html#data",
    "href": "project_faq.html#data",
    "title": "Project FAQ",
    "section": "Data",
    "text": "Data\n\nIs this data set ok to use?\n\nWhere can I find data? Rethink the question: Discuss in your group what are your interests and then find data related to your problem, there are literally terabytes of publicly available data, i.e.¬†don‚Äôt just google ‚Äúdata‚Äù, be specific\nIn order for you to demonstrate that you master the entire bio data science cycle, you must choose a bio data set, which requires cleaning and tidying\nIt would be good, if the data set requires joining, if not, consider artificially splitting it, to demonstrate that you master joining\nOne approach could be to work on reproducing results from a paper. Here, you can even consider improving the data visualisations or adding something extra\nDo not use a data set from one of the course exercises. Doing so would not allow you to demonstrate that you are independently capable of performing a bio data science analysis\nYou are 100% free to choose bio data from any resource as long as it meets the above requirements\nBe sure to understand that the purpose of the project is for you to synthesise an end-to-end Bio Data Science project demonstrating, that you have met the learning objectives of the course, as stated at the individual labs and on the DTU course base. Hence and importantly, your data must allow you to do so!\n\n\n\nHow should we handle NAs in the data?\n\nDepends, does your data set come with a readme, which allows you to make a decision regarding NAs?\nBe careful not to simply drop all NAs as you might drop observations, where columns of interest have complete data\nWhether to remove NAs or not cannot be universally answered, as it is specific to the challenge / data at hand. If it‚Äôs meaningful for you to keep NAs, then do so, if not, then remove them.\n\n\n\nHow should we handle binary variables?\n\nIn case your variable contains categories, i.e.¬†values, where the question ‚ÄúIs one larger than the other‚Äù is nonsensical, then encode as factors\n\n\n\nHow do we handle nonsensical data points?\n\nIf you believe the data points are nonsensical, e.g.¬†manual typing errors, then exclude them, but be very clear about which data (groups of) points were excluded and why\n\n\n\nCan we subset the data?\n\nYes, if you are particularly interested in a subset of the data, then that is perfectly fine. Just be aware, that any choice you take with the data, you must be able to account for why you took that choice\nYou should NOT just sample 100 observations as was done in the exercises to reduce runtime. However, if your data set is very large, consider down-sampling either randomly or by some metric of association. For the latter e.g.¬†perform a test and identify the top X observations, save those to file and continue from there.\nFor large data, try appending ‚Äú.gz‚Äù to your files, when you write them using write_tsv(), this will invoke gzip-compression"
  },
  {
    "objectID": "project_faq.html#inputoutput-files",
    "href": "project_faq.html#inputoutput-files",
    "title": "Project FAQ",
    "section": "Input/output files",
    "text": "Input/output files\n\nHow can we get a better overview of the data file flow?\n\nConsider creating a flow chart of ‚Äúyour data journey‚Äù. It will force you to think about how files are connected and how input/output flows, check out e.g.¬†draw.io for this\n\n\n\nHow can we write a file with e.g.¬†factor encoding or a nested tibble?\n\nThis can only be done by writing an R-object. However, this is for future reference - in this course if at all possible, stick to flat text files, e.g.¬†.tsv or .csv\n\n\n\nHow do we handle different naming conventions?\n\nDO NOT do a manual search and replace in Excel. That defeats the whole purpose of this course\nFix names using a programmatic approach. The stringr-package is extremely useful in this context\n\n\n\nWhere should we place external files?\n\nConsider creating an ‚Äúimages‚Äù folder in the ‚Äúdoc‚Äù folder, from where you can input images to your presentation"
  },
  {
    "objectID": "project_faq.html#modelling",
    "href": "project_faq.html#modelling",
    "title": "Project FAQ",
    "section": "Modelling",
    "text": "Modelling\n\nWhat should we include in the modelling part?\n\nWe have worked with fitting a linear regression and we have done a PCA. You could do something along those lines or something of similar complexity\nThis is not a modelling course. We briefly visited modelling to bridge the process of going from the raw data to analysis ready and then communication via data visualisation. Therefore, do not start fitting a full fledged machine/deep learning model, it is a time-void, which you cannot ‚Äúafford‚Äù to get sucked into\nAlso, mind that the focus of this course is to make you realise that even though you have been trained in delivering results, then the process of arriving at the results in a reproducible manner is equally important - The process is the product!\n\n\n\nDo we HAVE to do a PCA?\n\nDoes it make sense to do a PCA-analysis in your project? If you have group labels and you want to visualise to see if there is a separation, then a PCA is a good first step\n\n\n\nOur model is not ‚Äúperforming very well‚Äù. What should we do?\n\nLeave it as is. The focus of this course is not on the results, but on the reproducible process of arriving and communicating said results"
  },
  {
    "objectID": "project_faq.html#coding",
    "href": "project_faq.html#coding",
    "title": "Project FAQ",
    "section": "Coding",
    "text": "Coding\n\nWhat goes in ‚Äúaugment‚Äù?\n\nIf you google ‚ÄúAugment‚Äú, you will get ‚Äúmake (something) greater by adding to it; increase‚Äù, in other words, when you add e.g.¬†variables to your data, which was not there initially, e.g.¬†like we did, when we calculated the BMI from existing information on weight and height\nYou should think of the augment script as the place, where you create your ‚Äúdatabase‚Äù for everything that happens afterwards, i.e.¬†all your analyses and ideas\nCreate that ‚Äúdatabase‚Äù and then load it into your downstream scripts\nAugment == Adding something\nBe aware, that if you downstream discover a need to create a variable, you will need to go back to the augment and update\n\n\n\nCan we mix base R and tidyverse?\n\nNo, you might as well get used to it - Tidyverse all the way! (This is a tidyverse course)\nIf you use base, like e.g.¬†my_data$my_var <- 1:4 or my_data$my_var[1] or similar, then you will get points deducted in the evaluation of your project\nAgain, the process is the product and it matters much if you are using the course correct dialect of R. What you choose to do after the course is your choice\nUsing base R, where course content contains describes the tidyverse alternative is no different from using python and I suppose you wouldn‚Äôe expect to be able to hand in python code as the final product in an R course\n\n\n\nWhat if we can‚Äôt make it work in tidyverse but only in base R?\n\nIf you can make it work in base R, you can make it work in tidyverse\n\n\n\nIs it okay to use e.g.¬†the base function sum()?\n\nYes, think of it this way: R has as core functionality to do statistics. Tidyverse is for performing the data manipulation surrounding these calculations. Therefore, using functions such as sum(), mean(), sd(), etc. is naturally fine\nE.g. recall, when we did the group_by() \\(\\rightarrow\\) summarise()-workflow, we used the descriptive statistics functions\nAlso, we used the lm()-function for fitting a linear regression.\nTidyverse aims to replace the inconsistent and low legibility of base R with respect to data manipulation and visualisation - It does not replace the core functionality of doing statistics\n\n\n\nHow do we know if there is a tidyverse function we should use rather than base?\n\nIdentify the general area of what you‚Äôre working with. E.g. if strings, then go to your RStudio session and find the console and type stringr:: and hit tab, then you can look through the functions\nLook and search in the R4DS2e-book\nAsk on the Posit community pages\nAlso generally base will use e.g.¬†read.table(), where tidyverse will use read_table(), i.e.¬†a period vs.¬†an underscore\n\n\n\nCan we use package X for analysis/visualisation?\n\nIf the package performs a lot of the work, which you are to demonstrate that you can do, then absolutely no. E.g. DO NOT use packages, where you call a plot-my-data-function and the ggplot-magick happens that will NOT allow you to demonstrate that you have met the course learning objectives\nFor an example of and-then-magic-happens, see e.g.¬†the corrplot-package\nBasically, any package, which automatically does ‚Äúthings‚Äù, that are part of the learning objectives, will not allow you to demonstrate, that you have met the learning objectives!\n\n\n\nHow do we run the entire project incl.¬†the presentation?\n\nMake sure to include a programmatic call to render a file, i.e.¬†in your run-all qmd file, when you include sub-documents and run the run-all file, the sub-files will not be generated, hence you need to include a programmatic call in your run-all file, which will also render the seperate sub-documents\nNote, you can include one Quarto document in another\n\n\n\nShould we create a Shiny app?\n\nThe project deliverables are the GitHub repo and your presentation\nRemember, you do this project not for me as ‚Äúyour teacher‚Äù, but for you to internalize the knowledge you have been exposed to during the initial 10 weeks of teaching\nIf you find Shiny interesting/fun, then by all means, please do create an app\nNote, Shiny apps are optional and will not give extra credit - Be careful with your time!\n\n\n\nShould we create a package?\n\nIf you find Rpackages interesting/fun, then by all means, please do create an Rpackage\nNote, Rpackages are optional and will not give extra credit - Be careful with your time!\n\n\n\nWhen should we create functions?\n\nRemember DRY (Don‚Äôt-Repeat-Yourself), so generally, if you do something more than once, it is a function\nHowever, we do not focus much on functions in this course, so don‚Äôt put too much effort into creating functions\nPlace functions in 99_proj_functions.R as illustrated in the overview\nDO NOT hide your code away in functions, so that your main script just becomes 10 function calls. For this process-oriented course, show the code in your main qmd-documents, i.e.¬†01_, 02_, ‚Ä¶\n\n\n\nCan we use loops?\n\nNo, you should instead embrace functional programming and use functions from the purrr-package. Revisit lab 6, if needed\n\n\n\nCan we directly copy/paste a code chunk from an online source?\n\nNo, that would be plagiarism. Understand the steps in the chunk and make the code your own\nI acknowledge that for coding this is not completely black and white, but please refrain from a direct copy/paste\n\n\n\nCan we use chatGPT or similar for coding?\n\nNo, of course not, the coding has to be done by the students, not an AI\nHow would you know if we used an AI? I may not be able to tell or I may be, I wouldn‚Äôt recommend taking the risk\nOnce through this course, AIs can be a powerful allied, but it requires skills and experience to use an AI productively, otherwise you will end up in ‚Äútraps‚Äù"
  },
  {
    "objectID": "project_faq.html#coding-style",
    "href": "project_faq.html#coding-style",
    "title": "Project FAQ",
    "section": "Coding style",
    "text": "Coding style\n\nHow should we comment on our code?\n\nRemember, the point of writing verbose tidyverse code is that the code-becomes-the-comments. Think of it this way: The pipeline is the text on a page in a book, so before your pipeline, put a header/title on what is happening below, just as a title/header in a book\nThe title/header will be what is generally going on, e.g.¬†‚ÄúNormalise all gene expression values using standard score approach‚Äù and the pipeline will be how that is actually done\nThe pipe ‚Äú%>%‚Äù is pronounced as ‚Äúthen‚Äù, when you ‚Äúread‚Äù your code\n\n\n\nHow should we style our code?\n\nStrictly adhere to the Course Style Guide as introduced in the course\nMake sure that all scripts are styled the same way and be consistent in your coding style\nYour code should not be >80 chars wide, follow the vertical line in your editor\n\n\n\nHow should we style our plots?\n\nBe concise, ‚Äúless is more‚Äù\nMake some nice and relevant plots. Show us that you‚Äôve learned data visualisation. Remember legends, titles etc.\nDo not put 3 messages in 1 plot. Instead put each message into a different plot\nBe very careful with matching the text size in your plots to that of your presentation (trial-and-error)"
  },
  {
    "objectID": "project_faq.html#github",
    "href": "project_faq.html#github",
    "title": "Project FAQ",
    "section": "GitHub",
    "text": "GitHub\n\nWhere should we place the project?\n\nThe project must be placed on the course GitHub rforbiodatascienceXX, where XX is the course year\n\n\n\nShould our GitHub be public or private?\n\nPublic, show the world what you can do!\n\n\n\nShould we keep all the data on GitHub?\n\nNo, GitHub is meant for code, not data, see the project organisation\n\n\n\nWhat goes in the ‚Äúdoc‚Äù folder?\n\nThe project organisation chart is a generic figure. You should not hand in a report. Your deliverable/product outcome for the project period is: 1) Your GitHub repository and 2) A presentation. Therefore, the ‚Äúdoc‚Äù folder would in this case contain your presentation\n\n\n\nWhy are there multiple ‚Äúqmd-reports‚Äù in the ‚Äúresults‚Äù folder?\n\nIn case you have a computationally intensive part of the project it can be an advantage to split into sub-reports and then collect in a master report (think e.g.¬†large LaTeX reports)\nOverview, even if your report is not computationally intensive, then it may be long and splitting into sub-reports facilitate better overview\n\n\n\nShould we include a README on GitHub?\n\nThat would be a good way to briefly introduce what the contents of this repo is and something one would usually do\n\n\n\nShould everything be on Github, also plots we don‚Äôt present?\n\nYes, everything!"
  },
  {
    "objectID": "project_faq.html#examhand-in-deliverables",
    "href": "project_faq.html#examhand-in-deliverables",
    "title": "Project FAQ",
    "section": "Exam/hand-in deliverables",
    "text": "Exam/hand-in deliverables\n\nWhat does ‚ÄúIndependently identify and adapt relevant novel state-of-the-art data science tools‚Äù mean?\n\nThis is the exact result of completing the project work and my reason for not doing hand-holding in the project period. You are training your competences in that exact learning objective, so that once you leave this course, you will be able to do just that. This is absolute key as a modern bio data scientist. As an example, I programmed in a programming language called Perl, when I did my bioinformatics PhD, in fact the majority of my fellow phd students did that. I spend a lot of time getting really good at Perl‚Ä¶ Today, I never use it‚Ä¶ As in ever-never! Technology is under constants development and those with the capabilities stated in this LO will have a competitive edge and remain forefront in their working life\n\n\n\nWhat is the final product?\n\nA GitHub repository organised according to principles of reproducible data analysis\nA Quarto HTML-presentation following the IMRAD structure as elaborated in the project description\nThis final presentation in HTML-format should be uploaded to DTU Learn\n\n\n\nDo we need to hand in a report?\n\nNo, no report is required!"
  },
  {
    "objectID": "project_faq.html#presentation",
    "href": "project_faq.html#presentation",
    "title": "Project FAQ",
    "section": "Presentation",
    "text": "Presentation\n\nShould we include code in our presentation?\n\nYour presentation should follow the standard scientific IMRAD-structure, i.e.¬†introduction, materials-and-methods, results, and discussion.\nInclude certain decisions you took with the data and your reasoning herfore.\nIf you found a particular challenging problem in coding, to which you found an elegant solution, include that in your presentation as an example\nThe presentation is your chance to practice communicating insights to stakeholders\nYour audience will be same-level bioinformaticians\nPerhaps consider a graphical representation of your process going from raw to analysis-ready data\n\n\n\nShould we include tables or plots?\n\nWhat makes sense? If you only have two numerical values, then perhaps a table is fine. If you have 100 observations, then a plot is likely better\nRemember, we are as humans evolutionary encoded to interpret visual information, not numbers\n\n\n\nHow do we add plots to the Quarto presentation?\n\nOutput the plot as a png file using the function ggsave and include that png in your presentation. Think dynamically, we don‚Äôt want to type anything manually in our presentation\n\n\n\nHow do we get the presentation in wide-/full screen?\n\nHit w and f while the HTML presentation is loaded\n\n\n\nShould we split our presentation into sub-presentations?\n\nNo, this is not necessary given the extend/size of this project. Just create one qmd-file, which outputs an HTML-presentation\n\n\n\nWe are working on re-creating paper results. Should we re-create the exact same plots?\n\nIdeally, you re-create the plots and then you create your own improved version of the visualisation\n\n\n\nWhere should we ‚Äúplace‚Äù our Shiny app in our presentation?\n\nDemo it briefly at the end\n\n\n\nHow can we illustrate our data handling?\n\nConsider creating a flow chart, what are input files and how are they connected to final output?\n\n\n\nHow much code should we include in the presentation?\n\nThe exam deliverables are two-fold. Code is the GitHub repo and your ability to communicate biological insights is in the presentation. Therefore, code could be included in the presentation if you faced and solved a particularly challenging problem in a clever way\n\n\n\nHow do we include data numbers in the presentation?\n\nYou could from your script output a results table with relevant numbers as a .tsv file and then read that into the presentation and extract the numbers\nRemember, think dynamical reporting, what if your input data changes and you manually entered the numbers in your presentation? Then you wouldn‚Äôt be much farther than a powerpoint\n\n\n\nWhat goes into the materials and methods section?\n\nMaterials: What data did you use and where did you get it from?\nMethods: Which modelling did you use? Think of the methods section as a recipe for how to go from raw to results => Flow chart?\n\n\n\nShould we interpret our results?\n\nYes, you have to think about it as a ‚Äúnormal‚Äù project presentation\n\n\n\nWhat about all the stuff we tried, which did not work?\n\nIn the presentation, you should focus on what worked and what results you arrived at\nExclude dead-ends from the presentation, but‚Ä¶ Leave them in the repo, perhaps with an initial comment signifying that the following turned out to be a dead-end\nThink about this - Scenario: You‚Äôre working in a company and you and your team of 3 spend 6 months on a project, which turns out to be a dead-end. For future reference: Would the company be interested in knowing that this project was a dead end or should you delete everything and never speak of it again?"
  },
  {
    "objectID": "project_checklist.html",
    "href": "project_checklist.html",
    "title": "Project Checklist",
    "section": "",
    "text": "‚ÄúHave we remembered to look through the FAQ and description‚Äù\n\nThis is essential, this is where all the information on the project is collected\n\n\n\n‚ÄúHave we included a project README on the GitHub Repository?‚Äù\n\nIMPORTANT: First header in the README has to be ‚ÄúProject Contributors‚Äù and then please state the student ids and matching GitHub usernames, so we know who-is-who\nSince we‚Äôre not putting data on GitHub, you need to let us know how to get the data\n\n\n\n‚ÄúDoes our presentation follow the IMRAD structure?‚Äù\n\nIs the presentation created as one qmd-file and output to a HTML?\nAre the 10 presentation slides clear and concise?\nAre we doing good data communication via good visualisations?\nDo we present a clear overview of the data process incl.¬†any decisions made, e.g.¬†using a flow chart?\nAre we clearly communicating a biological insight?\nAre we following standard guidelines? Sources, references, etc.\n\n\n\n\n\n\n\n\n‚ÄúDoes our project include all components of the data science cycle?‚Äù\n\n\n\n\n\n\n\n‚ÄúAre we aware of and have included learning objectives as appropriate in the project?‚Äù\nCheck your project against the course learning objectives, as defined on the DTU course base\n\n\n‚ÄúIs our project-GitHub organised as instructed and can it run end-to-end via a‚Äùdoit‚Äù?‚Äù\n\nA ‚Äúdoit‚Äù is a script, which acts as a wrapper executing other scripts. In the project organisation, what would be the 00_all.qmd-file\n\n(Note, this is a generic representation, e.g.¬†you do not need exactly 3 key plots nor exactly 2 analyses)\n\n\n\n\n\n\n\n‚ÄúDoes ALL our code in ALL our files follow the Course Style Guide?‚Äù\n\nRecall what we discussed in the course on styling\nSee the Course Style Guide\nE.g.:\n\n\n\n\n\n\n\n\n‚ÄúAre we using base-R, where we should use tidyverse-R?‚Äù\n\nE.g. think about the following:\n\n\n\n\n\n\n\n\n‚ÄúCan we explain and justify the data decisions in the project?‚Äù\n\nIMPORTANT: In essence it does not matter which decision you took, what really matters is your ability to explain and justify why you took that decision, e.g.¬†decided on a particular path in your analysis"
  },
  {
    "objectID": "exam.html",
    "href": "exam.html",
    "title": "Exam",
    "section": "",
    "text": "The exam consists of 3 components:\n\nA group project, where all members are responsible for all parts of the project. This is handed in as a code base on the course GitHub repository\nAn oral group presentation of the project, where all group members, as per DTU rules on oral exams, must be physically present\nA two hour multiple choice quiz (MCQ) exam, where general course learning objectives are examined\n\nSee course description at DTU course base for further description"
  },
  {
    "objectID": "exam.html#deadlines-and-dates",
    "href": "exam.html#deadlines-and-dates",
    "title": "Exam",
    "section": "Deadlines and Dates",
    "text": "Deadlines and Dates\n\nProject code base and presentation must be completed at the latest 23:59 on the day before lab 13. Note, you cannot edit anything on the GitHub repository after this deadline!\nThe oral group presentation will be on lab 13 of the course, see DTU Learn calender for dates\nThe MCQ is placed according to the DTU exam schedule"
  },
  {
    "objectID": "exam.html#content",
    "href": "exam.html#content",
    "title": "Exam",
    "section": "Content",
    "text": "Content\n\nThe Group Project\n\nBe sure that everyone understands ALL code in the project as all group members are responsible for ALL code\nThe aim of the project is to cover the entire course cycle, so be sure to align your project\nBe sure to read Project Description\nQuestions? Make sure to consult the Project FAQ\n\n\n\nThe Presentation\n\nThe format is a 10-slides-in-10-mins\nEveryone in the group must present a part of the project\nExpect a few overall questions regarding the decisions you have made throughout your project\nNOTE: The final presentation in HTML-format must be zipped and uploaded to DTU Learn before deadline, so we can check the GitHub version is identical\n\n\n\nThe MCQ\n\nA 2-hour individual multiple choice exam\nIn 2022 the exam contained 60 questions, expect a similar number of questions\nAll aids are allowed, but no open internet\nEach question will have 4 possible answers and only 1 is the right one and you can only choose one answer per question\n\nIf any aspects of the above is not clear, please reach out to the teaching team!"
  }
]